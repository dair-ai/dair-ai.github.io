<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">dair.ai</title>
<generator uri="https://github.com/jekyll/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="https://dair.ai/feed.xml" />
<link rel="alternate" type="text/html" href="https://dair.ai" />
<updated>2020-03-17T18:43:50-05:00</updated>
<id>https://dair.ai/</id>
<author>
  <name>dair.ai</name>
  <uri>https://dair.ai/</uri>
  <email>ellfae@gmail.com</email>
</author>


<entry>
  <title type="html"><![CDATA[NLP ç®€æŠ¥ï¼ˆIssue#7ï¼‰: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,â€¦]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_NLP_7-ZH-.md/" />
  <id>https://dair.ai/NLP_Newsletter_NLP_7[ZH].md</id>
  <published>2020-03-16T00:00:00-05:00</published>
  <updated>2020-03-16T00:00:00-05:00</updated>
  <author>
    <name>kaiyuan</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*9gNslKwKiRaffDt2RSOgoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
æ¬¢è¿æ¥åˆ°NLPç®€æŠ¥ç¬¬ä¸ƒæœŸã€‚
Welcome to the 7th issue of the NLP Newsletter.å¸Œæœ›æ‚¨ä»Šå¤©è¿‡å¾—æ„‰å¿«ï¼Œå¹¶å¸Œæœ›æ‚¨å’Œæ‚¨çš„äº²äººåœ¨è¿™ä¸ªå›°éš¾çš„æ—¥å­é‡Œå¹³å®‰æ— äº‹ã€‚æˆ‘ä»¬å†³å®šå‘å¸ƒæ­¤æ–°é—»é€šè®¯ï¼Œä»¥ä½¿æˆ‘ä»¬çš„è¯»è€…æ„Ÿåˆ°é«˜å…´ï¼Œå› æ­¤è¯·åœ¨æœ‰ç©ºçš„æ—¶å€™é˜…è¯»ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç»§ç»­å…³æ³¨æœ€é‡è¦çš„äº‹æƒ…-æˆ‘ä»¬çš„å®¶äººå’Œæœ‹å‹ã€‚ â¤ï¸ ğŸ’› ğŸ’š&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;ä¸€äº›å…³äºNLPç®€æŠ¥ä»¥åŠdair.aiçš„æ›´æ–°&lt;/em&gt;&lt;/strong&gt;
ä¹‹å‰æ¯ä¸€æœŸNLPç®€æŠ¥çš„æ³•è¯­å’Œä¸­æ–‡ç¿»è¯‘éƒ½å·²ç»å®Œæˆï¼Œå¯ä»¥åœ¨ &lt;a href=&quot;https://github.com/dair-ai/nlp_newsletter&quot;&gt;è¿™é‡Œ&lt;/a&gt;æŸ¥çœ‹. &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;åœ¨æ­¤&lt;/a&gt;äº†è§£å¦‚ä½•ä¸ºNLPæ–°é—»é€šè®¯çš„å‰æœŸå’Œå³å°†å‘è¡Œçš„ç¿»è¯‘åšå‡ºè´¡çŒ®ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
æˆ‘ä»¬æœ€è¿‘å»ºç«‹äº†ä¸¤ä¸ªGithubåº“ï¼š &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;NLP paper summaries&lt;/a&gt; ä»¥åŠ&lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;PyTorch notebooks&lt;/a&gt; ã€‚&lt;/p&gt;

&lt;h1 id=&quot;1research-and-publications-&quot;&gt;1ã€Research and Publications ğŸ“™&lt;/h1&gt;

&lt;h4 id=&quot;11-measuring-compositional-generalization&quot;&gt;1.1 Measuring Compositional Generalization&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
åœ¨æœºå™¨å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼Œåˆæˆæ³›åŒ–ï¼ˆcompositional generalizationï¼‰æ˜¯æŒ‡æœºå™¨å­¦ä¹ ä»ä¸€ç»„è®­ç»ƒç¤ºä¾‹å­¦ä¹ ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚ è¿„ä»Šä¸ºæ­¢ï¼Œå°šä¸æ¸…æ¥šå¦‚ä½•æ­£ç¡®åœ°æµ‹é‡ç¥ç»ç½‘ç»œä¸­çš„compositionalityã€‚Google AIç ”ç©¶è€…åœ¨ ICLR 2020 ä¸Šçš„è®ºæ–‡ã€Š&lt;a href=&quot;https://ai.googleblog.com/2020/03/measuring-compositional-generalization.html&quot;&gt;Measuring Compositonal Generalization: A Comprehensive Method on Realistic Data&lt;/a&gt;ã€‹ï¼Œæå‡ºäº†ä½¿ç”¨é—®é¢˜è§£ç­”å’Œè¯­ä¹‰è§£æç­‰ä»»åŠ¡è¿›è¡Œcompositional generalizationçš„æœ€å¤§åŸºå‡†ä¹‹ä¸€ã€‚ ä¸‹å›¾æ˜¾ç¤ºäº†è¯¥ç§æ–°æ¨¡å‹ï¼Œä½¿ç”¨åŸå­ï¼ˆprodeceï¼Œdirectç­‰ï¼‰æ¥äº§ç”Ÿæ–°åŒ–åˆç‰©ï¼ˆå³åŸå­çš„ç»„åˆï¼‰çš„ç¤ºä¾‹ã€‚ è¿™é¡¹å·¥ä½œçš„æƒ³æ³•æ˜¯äº§ç”Ÿä¸€ä¸ªè®­ç»ƒæµ‹è¯•æ‹†åˆ†ï¼Œå…¶ä¸­åŒ…å«å…±äº«ç›¸ä¼¼åŸå­ï¼ˆç”Ÿæˆç¤ºä¾‹çš„æ„é€ å—ï¼‰ä½†å…·æœ‰ä¸åŒåŒ–åˆç‰©åˆ†å¸ƒï¼ˆåŸå­ç»„æˆï¼‰çš„ç¤ºä¾‹ã€‚ ä½œè€…å£°ç§°è¿™æ˜¯æµ‹è¯•compositional generalizationçš„ä¸€ç§æ›´å¯é çš„æ–¹æ³•ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*lXmUWOY8HJL7YVn1.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Credit: Google AI Blog&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;12-å¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡&quot;&gt;1.2 å¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
ç ”ç©¶äººå‘˜è¿›è¡Œäº†ä¸€ç³»åˆ—å…¨é¢çš„å¾®è°ƒè¯•éªŒï¼Œä»¥æ›´å¥½åœ°äº†è§£æƒé‡åˆå§‹åŒ–å’Œæ—©åœå¯¹è¯­è¨€æ¨¡å‹çš„æ•ˆæœï¼Œå‘è¡¨åœ¨è®ºæ–‡ã€Š&lt;a href=&quot;https://arxiv.org/abs/2002.06305&quot;&gt;Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping&lt;/a&gt;ã€‹ä¸­ã€‚ é€šè¿‡æ¶‰åŠå¯¹BERTè¿›è¡Œæ•°ç™¾æ¬¡å¾®è°ƒçš„å„ç§å®éªŒï¼Œå‘ç°ä¸åŒçš„éšæœºç§å­ä¼šäº§ç”Ÿæˆªç„¶ä¸åŒçš„ç»“æœã€‚ç‰¹åˆ«æ˜¯ï¼Œè¯¥ç ”ç©¶æŠ¥å‘Šç§°ï¼Œä¸€äº›æƒé‡åˆå§‹åŒ–åœ¨ä¸€ç»„ä»»åŠ¡ä¸­ç¡®å®è¡¨ç°è‰¯å¥½ã€‚æ‰€æœ‰å®éªŒæ•°æ®å’Œè¯•éªŒå‡å·²å…¬å¼€å‘å¸ƒï¼Œä¾›æœ‰å…´è¶£è¿›ä¸€æ­¥äº†è§£å¾®è°ƒè¿‡ç¨‹ä¸­ä¸åŒåŠ¨æ€çš„å…¶ä»–ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚&lt;/p&gt;

&lt;h4 id=&quot;13-zoom-in-an-introduction-to-circuits&quot;&gt;1.3 Zoom In: An Introduction to Circuits&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
OpenAIç ”ç©¶äººå‘˜å‘è¡¨äº†ä¸€ç¯‡æ–‡ç« ï¼Œ&lt;a href=&quot;https://distill.pub/2020/circuits/zoom-in/&quot;&gt;Zoom In: An Introduction to Circuits&lt;/a&gt;ï¼Œè®¨è®ºäº†ç¥ç»ç½‘ç»œçš„å¯è§£é‡Šæ€§çŠ¶æ€ï¼Œå¹¶æå‡ºäº†ä¸€ç§è§£é‡Šç¥ç»ç½‘ç»œçš„æ–°æ–¹æ³•çš„å»ºè®®ã€‚å—ç»†èƒç”Ÿç‰©å­¦çš„å¯å‘ï¼Œä½œè€…é€šè¿‡æ£€æŸ¥ç¥ç»ç½‘ç»œçš„æƒé‡æ·±å…¥äº†è§£äº†è§†è§‰æ¨¡å‹ä»¥åŠä»–ä»¬å­¦åˆ°äº†ä»€ä¹ˆã€‚æœ¬è´¨ä¸Šï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä»–ä»¬è®¤ä¸ºå¯ä»¥ä¸ºæ›´å¥½åœ°è§£é‡Šç¥ç»ç½‘ç»œé“ºå¹³é“è·¯çš„ä¸€äº›ä¸»å¼ ä»¥åŠè¯æ®ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*i0c-qpiire6dD4IqJVKlYg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;14-nlp-research-highlightsissue-1&quot;&gt;1.4 NLP Research Highlightsâ€Šâ€”â€ŠIssue #1&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
åœ¨dair.aiçš„æ–°ç³»åˆ—&lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-bertology-primer-fastpages-t5-data-science-education-pytorch-notebooks-slow-8ae5d499e040&quot;&gt;NLP Research Highlights&lt;/a&gt;ä¸­ï¼Œè¯¦ç»†ä»‹ç»äº†å½“å‰æœ‰è¶£ä¸”é‡è¦çš„NLPç ”ç©¶ã€‚é€šè¿‡å¯¹è¿™äº›å·¥ä½œçš„æ€»æ€§ï¼Œè¿™å°†æˆä¸ºè·Ÿè¸ªNLPè¿›å±•çš„ä¸€ç§æ–¹å¼ã€‚åœ¨ç¬¬ä¸€å­£åº¦ä¸­ï¼Œä¸»é¢˜æ¶‰åŠä»æ”¹è¿›è¯­è¨€æ¨¡å‹åˆ°æ”¹è¿›å¯¹è¯ä»£ç†åˆ°æœ€æ–°çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿã€‚ è¿™äº›æ‘˜è¦ä¹Ÿå°†ä¿ç•™åœ¨&lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;nlp_paper_summaries&lt;/a&gt;ä¸­ã€‚&lt;/p&gt;

&lt;h4 id=&quot;15ç”¨å›¾ç½‘ç»œæ¨¡æ‹Ÿå¤æ‚ç‰©ç†&quot;&gt;1.5ç”¨å›¾ç½‘ç»œæ¨¡æ‹Ÿå¤æ‚ç‰©ç†&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
åœ¨è¿‡å»çš„å‡ ä¸ªæœˆä¸­ï¼Œç”±äºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ä¸ä»…åœ¨NLPä¸­æœ‰æ•ˆï¼Œè€Œä¸”åœ¨åŸºå› ç»„å­¦å’Œææ–™ç­‰å…¶ä»–é¢†åŸŸä¹Ÿéå¸¸æœ‰æ•ˆï¼Œå› æ­¤æˆ‘ä»¬ä¸€ç›´åœ¨å…³æ³¨å®ƒä»¬ã€‚åœ¨æœ€è¿‘çš„ä¸€ç¯‡è®ºæ–‡ä¸­ï¼Œã€Š&lt;a href=&quot;https://arxiv.org/abs/2002.09405&quot;&gt;Learning to Simulate Complex Physics with Graph Networks&lt;/a&gt;ã€‹ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§åŸºäºå›¾ç½‘ç»œçš„é€šç”¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå­¦ä¹ æµä½“å’Œå¯å˜å½¢ææ–™ç­‰ä¸åŒé¢†åŸŸçš„æ¨¡æ‹Ÿã€‚ ä½œè€…å£°ç§°ä»–ä»¬åœ¨ä¸åŒé¢†åŸŸéƒ½å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»–ä»¬çš„é€šç”¨æ–¹æ³•å¯èƒ½æ˜¯è¿„ä»Šä¸ºæ­¢å­¦å¾—æœ€å¥½çš„ç‰©ç†æ¨¡æ‹Ÿå™¨ã€‚ å®éªŒåŒ…æ‹¬å¯¹ææ–™çš„æ¨¡æ‹Ÿï¼Œä¾‹å¦‚åœ¨æ°´ä¸Šæ»‘è¡Œä»¥åŠå…¶ä»–ä¸åˆšæ€§éšœç¢ç‰©çš„ç›¸äº’ä½œç”¨ã€‚ ä»–ä»¬è¿˜æµ‹è¯•äº†å…³äºåˆ†å‘ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶æ‰¾åˆ°äº†å¯å–œçš„ç»“æœï¼Œè¡¨æ˜è¯¥æ¡†æ¶å·²æ¨å¹¿åˆ°æ›´å¤§çš„é¢†åŸŸã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*48EolUDJoHpYRCTZxgn_qg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.09405.pdf&quot;&gt;&lt;em&gt;(Sanchez-Gonzalez et al., 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;16-ç‰¹å®šè¯­è¨€bertæ¨¡å‹&quot;&gt;1.6 ç‰¹å®šè¯­è¨€BERTæ¨¡å‹&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
Hugging Face Transformeråº“ä¸­ç°åœ¨æä¾›é˜¿æ‹‰ä¼¯è¯­BERTï¼ˆAraBERTï¼‰ã€‚ ä½ å¯ä»¥è®¿é—®&lt;a href=&quot;https://huggingface.co/aubmindlab/bert-base-arabert&quot;&gt;AraBERTæ¨¡å‹&lt;/a&gt;ä»¥åŠå¯¹åº”çš„[AraBERTè®ºæ–‡(https://arxiv.org/abs/2003.00104);&lt;/p&gt;

&lt;p&gt;æœ€è¿‘è¿˜å‘å¸ƒäº†&lt;a href=&quot;https://github.com/akirakubo/bert-japanese-aozora&quot;&gt;æ—¥è¯­BERT&lt;/a&gt;ä»¥åŠæ³¢å…°è¯­BERT&lt;a href=&quot;https://github.com/kldarek/polbert&quot;&gt;Polbert&lt;/a&gt;ã€‚&lt;/p&gt;

&lt;h1 id=&quot;2creativity-ethics-and-society-&quot;&gt;2ã€Creativity, Ethics, and Society ğŸŒ&lt;/h1&gt;

&lt;h4 id=&quot;21-covid-19ç›¸å…³çš„è›‹ç™½è´¨ç»“æ„çš„è®¡ç®—é¢„æµ‹&quot;&gt;2.1 COVID-19ç›¸å…³çš„è›‹ç™½è´¨ç»“æ„çš„è®¡ç®—é¢„æµ‹&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
DeepMindå…¬å¼€ä¸COVID-19ç›¸å…³ç—…æ¯’ç›¸å…³çš„è›‹ç™½è´¨çš„è®¡ç®—é¢„æµ‹ç»“æ„ï¼Œ&lt;a href=&quot;https://deepmind.com/research/open-source/computational-predictions-of-protein-structures-associated-with-COVID-19&quot;&gt;computational-predictions-of-protein-structures-associated-with-COVID-19&lt;/a&gt;ã€‚è¿™äº›é¢„æµ‹æ˜¯ç›´æ¥ä»AlphaFoldç³»ç»Ÿè·å¾—çš„ï¼Œä½†å°šæœªç»è¿‡å®éªŒéªŒè¯ã€‚è¯¥å¼€æºçš„åˆè¡·æ˜¯é¼“åŠ±ä¸ºæ›´å¥½åœ°äº†è§£è¯¥ç—…æ¯’åŠå…¶åŠŸèƒ½åšå‡ºè´¡çŒ®ã€‚&lt;/p&gt;

&lt;h4 id=&quot;22-court-cases-that-sound-like-the-weirdest-fights&quot;&gt;2.2 Court cases that sound like the weirdest fights&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
Janelle Shaneåˆ†äº«äº†ä¸€ä¸ªæœ‰è¶£å®éªŒçš„ç»“æœï¼Œ&lt;a href=&quot;https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights&quot;&gt;court-cases-that-sound-like-the-weirdest-fights&lt;/a&gt;ï¼Œå…¶ä¸­å¯¹GPT-2æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒä»¥ç”Ÿæˆé’ˆå¯¹æ— ç”Ÿå‘½ç‰©ä½“çš„æ¡ˆä¾‹ã€‚è¯¥æ¨¡å‹å–‚å…¥äº†ä¸€ç³»åˆ—æ”¿åºœæ‰£æŠ¼è¿ç¦å“æˆ–å±é™©å“çš„æ¡ˆä¾‹ï¼Œå¹¶ç”Ÿæˆäº†å¦‚ä¸‹å›¾æ‰€ç¤ºçš„æ¡ˆä¾‹ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*E5mHmkm1h4VQJ2Ni.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights&quot;&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;23-ä»¥äººä¸ºä¸­å¿ƒçš„mlæ¡†æ¶è®¾è®¡&quot;&gt;2.3 ä»¥äººä¸ºä¸­å¿ƒçš„MLæ¡†æ¶è®¾è®¡&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
Google AIå…¬å¸ƒäº†å¯¹ä½¿ç”¨TensorFlow.jsçš„645äººçš„å¤§è§„æ¨¡è°ƒæŸ¥ç»“æœï¼Œ&lt;a href=&quot;https://ai.googleblog.com/2020/03/toward-human-centered-design-for-ml.html&quot;&gt;toward-human-centered-design-for-ml&lt;/a&gt;ã€‚ ä»–ä»¬æ—¨åœ¨ä»éMLè½¯ä»¶å¼€å‘äººå‘˜é‚£é‡Œäº†è§£æœ€é‡è¦çš„åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Œä»¥åŠä»–ä»¬åœ¨ä½¿ç”¨å½“å‰MLæ¡†æ¶æ—¶çš„æ€»ä½“ç»éªŒã€‚ç ”ç©¶å‘ç°åŒ…æ‹¬â€œç¼ºä¹å¯¹MLçš„æ¦‚å¿µæ€§ç†è§£â€é˜»ç¢äº†MLæ¡†æ¶é’ˆå¯¹æ­¤ç‰¹å®šç”¨æˆ·é›†çš„ä½¿ç”¨ã€‚è¯¥ç ”ç©¶çš„å‚ä¸è€…è¿˜æŠ¥å‘Šäº†å…³äºå¦‚ä½•å°†MLæ¨¡å‹åº”ç”¨äºä¸åŒé—®é¢˜çš„éœ€æ±‚&lt;/p&gt;

&lt;h4 id=&quot;24-åœ¨æµè§ˆå™¨ä¸­è¿›è¡Œé¢éƒ¨å’Œæ‰‹éƒ¨è·Ÿè¸ª&quot;&gt;2.4 åœ¨æµè§ˆå™¨ä¸­è¿›è¡Œé¢éƒ¨å’Œæ‰‹éƒ¨è·Ÿè¸ª&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
è¿™ç¯‡å¾ˆæ£’çš„TensorFlowæ–‡ç« ï¼Œ&lt;a href=&quot;https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html?linkId=83996111&quot;&gt;Toward Human-Centered Design for ML Frameworks&lt;/a&gt;ï¼Œæä¾›äº†å¦‚ä½•ä½¿ç”¨TensorFlow.jså’ŒMediaPipeåœ¨æµè§ˆå™¨ä¸Šå¯ç”¨å®æ—¶é¢éƒ¨å’Œæ‰‹éƒ¨è·Ÿè¸ªçš„æ¼”ç»ƒã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*XsRsB-tSOZo9yWOc.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Credit: TensorFlow Blog&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;tools-and-datasets-ï¸&quot;&gt;Tools and Datasets âš™ï¸&lt;/h1&gt;

&lt;h4 id=&quot;31-nlp-paper-summaries&quot;&gt;3.1 NLP Paper Summaries&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
æˆ‘ä»¬æœ€è¿‘åˆ›å»ºäº†ä¸€ä¸ª&lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;nlp_paper_summariesåº“&lt;/a&gt;ï¼Œå…¶ä¸­åŒ…å«ç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„NLPè®ºæ–‡æ‘˜è¦åˆ—è¡¨ï¼Œè¿™äº›æ‘˜è¦æ˜¯è¿‡å»å‡ å¹´ä¸­ä¸€äº›æœ€æœ‰è¶£å’Œæœ€é‡è¦çš„NLPè®ºæ–‡ã€‚ç€é‡äºç²¾é€‰é‡è¦è®ºæ–‡çš„è®ºæ–‡æ‘˜è¦å’Œåšå®¢æ–‡ç« ï¼Œä»¥å¸®åŠ©æé«˜NLPä¸»é¢˜å’Œç ”ç©¶çš„å¯åŠæ€§ã€‚&lt;/p&gt;

&lt;h4 id=&quot;32-pytorchçš„è®¡ç®—æœºè§†è§‰åº“&quot;&gt;3.2 PyTorchçš„è®¡ç®—æœºè§†è§‰åº“&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/kornia/kornia&quot;&gt;Kornia&lt;/a&gt; æ˜¯å»ºç«‹åœ¨PyTorchä¹‹ä¸Šçš„å¼€æºåº“ï¼Œå®ƒä½¿ç ”ç©¶äººå‘˜å¯ä»¥ä½¿ç”¨ä¸€ç»„è¿ç®—ç¬¦æ¥ä½¿ç”¨PyTorchæ‰§è¡Œä¸åŒçš„è®¡ç®—æœºè§†è§‰ã€‚æŸäº›åŠŸèƒ½åŒ…æ‹¬å›¾åƒè½¬æ¢ï¼Œæ·±åº¦ä¼°è®¡å’Œä½çº§å›¾åƒå¤„ç†ç­‰ã€‚å®ƒåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå—åˆ°OpenCVçš„å¯å‘ï¼Œä½†ä¸åŒä¹‹å¤„åœ¨äºï¼Œå®ƒæ—¨åœ¨ç”¨äºç ”ç©¶ï¼Œè€Œä¸æ˜¯æ„å»ºå¯æŠ•å…¥ç”Ÿäº§çš„åº”ç”¨ç¨‹åºã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*gN_-llcA4_3lIHYE.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;33-dietç®€ä»‹&quot;&gt;3.3 DIETç®€ä»‹&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/&quot;&gt;DIETï¼ˆDual Intent and Entity Transformerï¼‰&lt;/a&gt;æ˜¯Rasaæå‡ºçš„è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰å¤šä»»åŠ¡ä½“ç³»ç»“æ„ã€‚ è¯¥æ¡†æ¶ç€é‡äºå¤šä»»åŠ¡è®­ç»ƒï¼Œä»¥æ”¹å–„æ„å›¾åˆ†ç±»å’Œå®ä½“è¯†åˆ«æ–¹é¢çš„ç»“æœã€‚ DIETçš„å…¶ä»–å¥½å¤„åŒ…æ‹¬èƒ½å¤Ÿä½¿ç”¨ä»»ä½•å½“å‰çš„é¢„è®­ç»ƒåµŒå…¥ï¼Œä¾‹å¦‚BERTå’ŒGloVeã€‚é‡ç‚¹æ˜¯è¦æä¾›ä¸€ä¸ªæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥æé«˜è¿™äº›ä»»åŠ¡çš„å½“å‰æœ€æ–°æ€§èƒ½ï¼Œå¹¶ä¸”è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼ˆæ®æŠ¥é“ï¼Œé€Ÿåº¦æé«˜äº†6å€ï¼‰ã€‚ è¯¥æ¨¡å‹åœ¨&lt;a href=&quot;https://rasa.com/docs/rasa/1.8.0/nlu/components/#dietclassifier&quot;&gt;Rasaå¼€æºpythonåº“&lt;/a&gt;ä¸­å¯ç”¨.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*R_8FOU-CVZabv7hJ.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/?utm_source=twitter&quot;&gt;&lt;em&gt;DIET framework&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;34-è¿·å¤±åœ¨ä¼—å¤šbertæ¨¡å‹ä¸­&quot;&gt;3.4 è¿·å¤±åœ¨ä¼—å¤šBERTæ¨¡å‹ä¸­ï¼Ÿ&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://bertlang.unibocconi.it/&quot;&gt;BERT Lang Street&lt;/a&gt;æ˜¯ä¸€ä¸ªç®€æ´çš„ç½‘ç«™ï¼Œå®ƒèƒ½å¤Ÿæœç´¢30ç§åŸºäºBERTçš„æ¨¡å‹ï¼Œå…¶ä¸­åŒ…å«18ç§è¯­è¨€å’Œ28ä¸ªä»»åŠ¡ï¼Œå…±177ä¸ªæ¡ç›®ã€‚ ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³ä½¿ç”¨BERTæ¨¡å‹æ‰¾å‡ºæœ€æ–°çš„æƒ…æ„Ÿåˆ†ç±»ç»“æœï¼Œåˆ™å¯ä»¥åœ¨æœç´¢æ ä¸­æœç´¢â€œæƒ…æ„Ÿâ€ï¼ˆå¦‚ä¸‹é¢çš„å›¾ç‰‡æ‰€ç¤ºï¼‰ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*UuVno2eOAzYb_wlSSfukPA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;35-med7&quot;&gt;3.5 Med7&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
Andrey Kormilitzinå‘å¸ƒäº†&lt;a href=&quot;https://github.com/kormilitzin/med7&quot;&gt;Med7&lt;/a&gt; ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåœ¨ç”µå­å¥åº·è®°å½•ä¸Šæ‰§è¡Œä¸´åºŠNLPï¼ˆç‰¹åˆ«æ˜¯å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ï¼‰çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹æœ€å¤šå¯ä»¥è¯†åˆ«ä¸ƒä¸ªç±»åˆ«ï¼Œå¹¶ä¸”å¯ä»¥ä¸spaCyåº“ä¸€èµ·ä½¿ç”¨ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yOMqhvTwYnxB4LYXv2Mgjg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;36-é‡å­æœºå™¨å­¦ä¹ å¼€æºåº“&quot;&gt;3.6 é‡å­æœºå™¨å­¦ä¹ å¼€æºåº“&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html&quot;&gt;TensorFlow Quantum&lt;/a&gt;æ˜¯ä¸€ä¸ªå¼€æ”¾æºä»£ç åº“ï¼Œæä¾›äº†ç”¨äºå¿«é€Ÿè¿›è¡Œé‡å­MLç ”ç©¶åŸå‹çš„å·¥å…·ç®±ï¼Œè¯¥å·¥å…·ç®±åº”ç”¨MLæ¨¡å‹æ¥è§£å†³ä»åŒ»å­¦åˆ°ææ–™çš„å„ç§é—®é¢˜ã€‚&lt;/p&gt;

&lt;h4 id=&quot;37-å¿«é€Ÿç®€ä¾¿çš„æ— é™å®½ç½‘ç»œ&quot;&gt;3.7 å¿«é€Ÿç®€ä¾¿çš„æ— é™å®½ç½‘ç»œ&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/google/neural-tangents&quot;&gt;Neural Tangents&lt;/a&gt;æ˜¯ä¸€ä¸ªå¼€æ”¾æºä»£ç åº“ï¼Œå…è®¸ç ”ç©¶äººå‘˜ä½¿ç”¨JAXå»ºç«‹å’Œè®­ç»ƒæ— é™å®½æ¨¡å‹å’Œæœ‰é™ç¥ç»ç½‘ç»œã€‚å¯ä»¥é˜…è¯»ç›¸åº”åœ°åšå®¢è·å–æ›´å¤šä¿¡æ¯ï¼Œ&lt;a href=&quot;https://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html&quot;&gt;fast-and-easy-infinitely-wide-networks&lt;/a&gt;ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*CojgKJwB_n_7-j0DJZ0y7g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;articles-and-blog-posts-ï¸&quot;&gt;Articles and Blog posts âœï¸&lt;/h1&gt;

&lt;h4 id=&quot;41-ä»-pytorch-åˆ°jax&quot;&gt;4.1 ä» PyTorch åˆ°JAX&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
Sabrina J. Mielkeå‘è¡¨äº†ä¸€ç¯‡æ–‡ç« ï¼Œ&lt;a href=&quot;https://sjmielke.com/jax-purify.htm&quot;&gt;From PyTorch to JAX: towards neural net frameworks that purify stateful code&lt;/a&gt;ï¼Œå…¶ä¸­æä¾›äº†æœ‰å…³å¦‚ä½•ä½¿ç”¨JAXæ„å»ºå’Œè®­ç»ƒç¥ç»ç½‘ç»œçš„æ¼”ç»ƒã€‚ æœ¬æ–‡ç€é‡äºåœ¨æ„å»ºç¥ç»ç½‘ç»œæ—¶æ¯”è¾ƒPyTorchå’ŒJAXçš„å†…éƒ¨å·¥ä½œåŸç†ï¼Œè¿™æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£JAXçš„ä¸€äº›ä¼˜ç‚¹å’ŒåŒºåˆ«ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*Nrw4UnmnIZ__elHu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://sjmielke.com/jax-purify.htm&quot;&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/a&gt; **&lt;/p&gt;

&lt;h4 id=&quot;42-why-do-we-still-use-18-year-old-bleu&quot;&gt;4.2 Why do we still use 18-year old BLEU?***&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
åœ¨åšå®¢&lt;a href=&quot;https://ehudreiter.com/2020/03/02/why-use-18-year-old-bleu/&quot;&gt; Why do we still use 18-year old BLEU?&lt;/a&gt;ä¸­ï¼ŒEhud Reiterè°ˆåˆ°äº†ä¸ºä»€ä¹ˆæˆ‘ä»¬ä»ç„¶ä½¿ç”¨BLUEç­‰æ—§çš„è¯„ä¼°æŠ€æœ¯è¿›è¡Œè¯„ä¼°è¯¸å¦‚æœºå™¨ç¿»è¯‘ä¹‹ç±»çš„ä»»åŠ¡çš„NLPæ¨¡å‹ã€‚ä½œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶äººå‘˜ï¼Œä»–è¿˜è¡¨è¾¾äº†å¯¹å¯¹è¾ƒæ–°ä»»åŠ¡è¿›è¡Œè¯„ä¼°çš„æŠ€æœ¯çš„å«ä¹‰ã€‚&lt;/p&gt;

&lt;h4 id=&quot;43-bartç®€ä»‹&quot;&gt;4.3 BARTç®€ä»‹&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1910.13461&quot;&gt;BART&lt;/a&gt;æ˜¯Facebookæå‡ºçš„ä¸€ç§æ–°æ¨¡å‹ï¼Œå…¶ä¸­æ¶‰åŠä¸€ç§ç”¨äºå¯¹seq2seqæ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒçš„é™å™ªè‡ªåŠ¨ç¼–ç å™¨ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ”¹å–„ä¸‹æ¸¸æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æŠ½è±¡æ‘˜è¦ï¼‰çš„æ€§èƒ½ã€‚ Sam Shleiferæä¾›äº†&lt;a href=&quot;https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html&quot;&gt;BARTçš„æ‘˜è¦ç®€ä»‹&lt;/a&gt;ï¼Œä»¥åŠä»–å¦‚ä½•å°†å…¶é›†æˆåˆ°Hugging Face Transformersä»£ç åº“ä¸­ã€‚&lt;/p&gt;

&lt;h4 id=&quot;44-transformeré•¿ç¨‹ä¸Šä¸‹æ–‡ç»¼è¿°&quot;&gt;4.4 Transformeré•¿ç¨‹ä¸Šä¸‹æ–‡ç»¼è¿°&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
Madison Mayæœ€è¿‘å†™äº†ä¸€ç¯‡æœ‰è¶£çš„ç»¼è¿°ï¼Œ&lt;a href=&quot;https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/&quot;&gt;A Survey of Long-Term Context in Transformers&lt;/a&gt;ï¼Œæè¿°äº†æ”¹è¿›åŸºäºTransformerçš„æ–¹æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬Sparse Transformers, Adaptive Span Transformers, Transformer-XL, compressive Transformers, Reformerä»¥åŠrouting transformerã€‚&lt;/p&gt;

&lt;h4 id=&quot;45-å¦‚ä½•åœ¨è‡ªåŠ¨æ–‡æœ¬ç¼–å†™ä¸­æ§åˆ¶æ ·å¼å’Œå†…å®¹&quot;&gt;4.5 å¦‚ä½•åœ¨è‡ªåŠ¨æ–‡æœ¬ç¼–å†™ä¸­æ§åˆ¶æ ·å¼å’Œå†…å®¹&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
å°½ç®¡è‡ªåŠ¨æ–‡æœ¬ä¹¦å†™åœ¨è¿‡å»çš„ä¸€å¹´ä¸­å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œä½†æ˜¯æ§åˆ¶è¯¸å¦‚æœºå™¨ä¹¦å†™æ–‡æœ¬çš„ç»“æ„æˆ–å†…å®¹ä¹‹ç±»çš„å±æ€§ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ åœ¨æœ€è¿‘çš„åšå®¢æ–‡ç« ï¼Œ&lt;a href=&quot;https://creatext.ai/blog-posts/controllable-text-generation&quot;&gt;â€œMind your language, GPT-2â€: how to control style and content in automatic text writing&lt;/a&gt;ä¸­ï¼ŒManuel Tonneauä»Hugging Faceçš„GPT-2è®¨è®ºäº†å¯æ§æ–‡æœ¬ç”Ÿæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•å’Œè§‚ç‚¹ã€‚ è¯¥æ¨¡å‹åœ¨arXivä¸Šä¸Googleçš„T5è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶æåˆ°äº†Salesforceçš„CTRLå’ŒUber AIçš„PPLMã€‚&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education ğŸ“&lt;/h1&gt;

&lt;h4 id=&quot;51-pythonä¸­nlpçš„æœªæ¥å‘å±•&quot;&gt;5.1 Pythonä¸­NLPçš„æœªæ¥å‘å±•&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
åœ¨æˆ‘ä»¬ä»¥å‰çš„NLPç®€æŠ¥ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†&lt;a href=&quot;https://thinc.ai/&quot;&gt;THiNC&lt;/a&gt;ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ·±å±‚å­¦ä¹ åº“ï¼Œè‡´åŠ›äºä¸å…¶ä»–ç°æœ‰åº“çš„å…¼å®¹æ€§ã€‚ Ines Montaniåœ¨PyConå“¥ä¼¦æ¯”äºšçš„æ¼”è®²ä½¿ç”¨çš„PPT&lt;a href=&quot;https://speakerdeck.com/inesmontani/the-future-of-nlp-in-python-keynote-pycon-colombia-2020?slide=9&quot;&gt;The Future of NLP in Python&lt;/a&gt;å¼•å…¥äº†æ›´å¤šçš„åº“ã€‚&lt;/p&gt;

&lt;h4 id=&quot;52-transformers-notebooks&quot;&gt;5.2 Transformers Notebooks&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
HuggingFaceå‘å¸ƒäº†ä¸€ç»„&lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/notebooks&quot;&gt;Colab notebooks&lt;/a&gt;ï¼Œå¯å¸®åŠ©ä»–ä»¬å¼€å§‹ä½¿ç”¨æµè¡Œçš„Transformersåº“ã€‚ ä¸€äº›notebookåŒ…æ‹¬ä½¿ç”¨ä»¤ç‰ŒåŒ–ï¼Œè®¾ç½®NLPç®¡é“ä»¥åŠåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šè®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*0AYHYUsHbaqV2vqN2zCzLQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;53-tensorflow-20å…è´¹è¯¾ç¨‹&quot;&gt;5.3 TensorFlow 2.0å…è´¹è¯¾ç¨‹&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
åœ¨TensorFlow 2.0ä¸ŠæŸ¥çœ‹æ­¤&lt;a href=&quot;https://www.freecodecamp.org/news/massive-tensorflow-2-0-free-course/&quot;&gt;ã€œ7å°æ—¶å…è´¹è¯¾ç¨‹&lt;/a&gt; ï¼Œå…¶ä¸­åŒ…å«ä»åŸºæœ¬ç¥ç»ç½‘ç»œåˆ°NLPåˆ°å¼ºåŒ–å­¦ä¹ çš„ä»‹ç»ã€‚&lt;/p&gt;

&lt;h4 id=&quot;54-deepmindæ’­å®¢&quot;&gt;5.4 DeepMindæ’­å®¢&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
DeepMindå·²ä¸ºå…¶æ’­å®¢å‘å¸ƒäº†æ‰€æœ‰å‰§é›†ï¼Œ&lt;a href=&quot;https://www.youtube.com/playlist?list=PLqYmG7hTraZBiUr6_Qf8YTS2Oqy3OGZEj&quot;&gt;DeepMind: The Podcast&lt;/a&gt;ï¼Œå…¶ä¸­æœ‰ç§‘å­¦å®¶ï¼Œç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆè®¨è®ºä¸»é¢˜æ¶µç›–äº†AGI åˆ° ç¥ç»ç§‘å­¦ åˆ° æœºå™¨äººæŠ€æœ¯ã€‚&lt;/p&gt;

&lt;h4 id=&quot;55-mldlè¯¾ç¨‹&quot;&gt;5.5 ML&amp;amp;DLè¯¾ç¨‹&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://sites.google.com/view/berkeley-cs294-158-sp20/home&quot;&gt;Berkeleyçš„â€œæ·±åº¦æ— ç›‘ç£å­¦ä¹ â€è¯¾ç¨‹&lt;/a&gt;å·²ç»å…¬å¼€å‘å¸ƒæ•´ä¸ªæ•™å­¦å¤§çº²ï¼Œä¸»è¦ä¾§é‡äºè‡ªæˆ‘å­¦ä¹ çš„ç†è®ºæ–¹é¢ ç›‘ç£å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹ã€‚ä¸€äº›ä¸»é¢˜åŒ…æ‹¬æ½œåœ¨å˜é‡æ¨¡å‹ï¼Œè‡ªå›å½’æ¨¡å‹ï¼Œæµæ¨¡å‹å’Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ ç­‰ç­‰ï¼Œå·²ç»æœ‰æä¾›YouTubeè§†é¢‘å’Œå¹»ç¯ç‰‡ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
æˆ‘ä»¬è¿˜å‘ç°äº†æœ‰å…³æœºå™¨å­¦ä¹ ï¼ŒNLPå’Œæ·±åº¦å­¦ä¹ çš„é«˜çº§åœ¨çº¿è¯¾ç¨‹çš„ä»¤äººå°è±¡æ·±åˆ»çš„åˆ—è¡¨ï¼Œ&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/&quot;&gt;d_advanced_courses_update&lt;/a&gt; ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
è¿™æ˜¯å¦ä¸€é—¨åä¸º&lt;a href=&quot;https://compstat-lmu.github.io/lecture_i2ml/index.html&quot;&gt;â€œæœºå™¨å­¦ä¹ å…¥é—¨â€&lt;/a&gt;çš„è¯¾ç¨‹ï¼Œå…¶ä¸­åŒ…æ‹¬è¯¸å¦‚ç›‘ç£å›å½’ï¼Œæ€§èƒ½è¯„ä¼°ï¼Œéšæœºæ£®æ—ï¼Œå‚æ•°è°ƒæ•´ï¼Œ å®ç”¨å»ºè®®ç­‰ã€‚&lt;/p&gt;

&lt;h1 id=&quot;noteworthy-mentions-ï¸&quot;&gt;Noteworthy Mentions â­ï¸&lt;/h1&gt;

&lt;p&gt;ä¸Šä¸€æœŸçš„NLPç®€æŠ¥(Issue #6) å¯ä»¥åœ¨&lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-bertology-primer-fastpages-t5-data-science-education-pytorch-notebooks-slow-8ae5d499e040&quot;&gt;è¿™é‡Œ&lt;/a&gt;æŸ¥çœ‹ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Connon Shortenå‘è¡¨äº†è§£é‡Š&lt;a href=&quot;https://www.youtube.com/watch?v=QWu7j1nb_jI&amp;amp;feature=emb_logo&quot;&gt;ELECTRAæ¨¡å‹çš„è§†é¢‘&lt;/a&gt;ï¼Œè¯¥æ¨¡å‹æå‡ºäº†ä¸€ç§ç§°ä¸º &lt;code class=&quot;highlighter-rouge&quot;&gt;replaced token detection&lt;/code&gt;çš„æŠ€æœ¯ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°å¯¹Transformersè¿›è¡Œé¢„è®­ç»ƒã€‚ å¦‚æœæ‚¨æœ‰å…´è¶£ï¼Œæˆ‘ä»¬ä¹Ÿåœ¨&lt;a href=&quot;https://medium.com/dair-ai/nlp-research-highlights-cd522b21b01a&quot;&gt;æ­¤å¤„&lt;/a&gt;å†™äº†è¯¥æ¨¡å‹çš„ç®€çŸ­æ‘˜è¦ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Rachael Tatmanæ­£åœ¨ç ”ç©¶ä¸€ä¸ªåä¸º&lt;a href=&quot;https://www.youtube.com/watch?v=-G36q8_cYsc&amp;amp;feature=emb_logo&quot;&gt;é¢å‘å¼€å‘äººå‘˜çš„NLP&lt;/a&gt; çš„æ–°ç³»åˆ—ï¼Œå…¶ç›®çš„æ˜¯åœ¨ä½•æ—¶ä½¿ç”¨NLPçš„æ–¹æ³•è¿›è¡Œæ›´æ·±å…¥çš„è®¨è®º,ä½¿ç”¨å®ƒä»¬å¹¶è§£é‡Šä½ å¯èƒ½é‡åˆ°çš„å¸¸è§é—®é¢˜ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMindåœ¨YouTubeä¸Šå‘å¸ƒäº†&lt;a href=&quot;https://youtu.be/WXuK6gekU1Y&quot;&gt;AlphaGo-ç”µå½±&lt;/a&gt;ï¼Œä»¥åº†ç¥AlphaGoåœ¨Goæ¸¸æˆä¸­å‡»è´¥Lee Sedolå››å‘¨å¹´ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
OpenMinedä¸ºç ”ç©¶å·¥ç¨‹å¸ˆå’Œç ”ç©¶ç§‘å­¦å®¶&lt;a href=&quot;https://blog.openmined.org/introducing-openmined-research/&quot;&gt;å¼€æ”¾èŒä½&lt;/a&gt;ï¼Œè¿™æ˜¯å‚ä¸ä¿æŠ¤éšç§çš„AIçš„å¥½æœºä¼šã€‚&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;å¦‚æœæ‚¨å¸Œæœ›åœ¨ä¸‹ä¸€æœŸNLPæ–°é—»é€šè®¯ä¸­å…±äº«ä»»ä½•æ•°æ®é›†ï¼Œé¡¹ç›®ï¼Œåšå®¢æ–‡ç« ï¼Œæ•™ç¨‹æˆ–è®ºæ–‡ï¼Œè¯·éšæ—¶é€šè¿‡ellfae@gmail.comæˆ–&lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;&lt;strong&gt;DM on Twitter&lt;/strong&gt;&lt;/a&gt;ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Subscribe&lt;/em&gt;&lt;/a&gt;&lt;em&gt;ğŸ”–è‡³NLPæ–°é—»é€šè®¯ï¼Œä»¥åœ¨æ”¶ä»¶ç®±ä¸­æ¥æ”¶ä»¥åçš„æ–°é—»ã€‚&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_NLP_7-ZH-.md/&quot;&gt;NLP ç®€æŠ¥ï¼ˆIssue#7ï¼‰: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,â€¦&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 16, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,â€¦]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_NLP_7/" />
  <id>https://dair.ai/NLP_Newsletter_NLP_7</id>
  <published>2020-03-16T00:00:00-05:00</published>
  <updated>2020-03-16T00:00:00-05:00</updated>
  <author>
    <name>Elvis Saravia</name>
    <uri>https://dair.ai</uri>
    <email>ellfae@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*9gNslKwKiRaffDt2RSOgoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Welcome to the 7th issue of the NLP Newsletter. I hope you are having a wonderful day and that you and your loved ones are safe in these difficult times. We decided to publish this newsletter to bring some joy to our readers so please read when you have free time. For now, letâ€™s keep focused on the things that are of top priorityâ€” our families and friends. â¤ï¸ ğŸ’› ğŸ’š&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;A few updates about the NLP Newsletter and dair.ai&lt;/em&gt;&lt;/strong&gt;
All French and Chinese translations for the previous issues of the NLP Newsletter are now &lt;a href=&quot;https://github.com/dair-ai/nlp_newsletter&quot;&gt;available&lt;/a&gt;. Find out how you can contribute to the translation of previous and upcoming issues of the NLP Newsletter at this &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
We recently created two GitHub repositories that contain &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;NLP paper summaries&lt;/a&gt; and &lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;PyTorch notebooks&lt;/a&gt; to get you started with neural networks.&lt;/p&gt;

&lt;h1 id=&quot;research-and-publications-&quot;&gt;Research and Publications ğŸ“™&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Measuring Compositional Generalization&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In the context of machine learning, compositional generalization is the ability to learn to represent meaning and in turn sequences (novel combinations) from whatâ€™s learned in the training set. To this date, it is not clear how to properly measure compositionality in neural networks. A Google AI team &lt;a href=&quot;https://ai.googleblog.com/2020/03/measuring-compositional-generalization.html&quot;&gt;proposes&lt;/a&gt; one of the largest benchmarks for compositional generalization using tasks such as question answering and semantic parsing. The picture below shows an example of the proposed model using atoms (produce, direct, etc.) to produce novel compounds, i.e., combinations of atoms. The idea of this work is to produce a train-test split that contains examples that share similar atoms (building blocks to generate examples) distribution but different compound distribution (the composition of atoms). The authors claim that is a more reliable way to test for compositional generalization.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*lXmUWOY8HJL7YVn1.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Credit: Google AI Blog&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Researchers ran a comprehensive &lt;a href=&quot;https://arxiv.org/abs/2002.06305&quot;&gt;set of fine-tuning trials&lt;/a&gt; to better understand the effect of weight initialization and early stopping in the performance of language models. Through various experiments that involved fine-tuning BERT hundreds of times, it was found that distinct random seeds produce very different results. In particular, the study reports that some weight initialization does perform well across a set of tasks. All the experimental data and trials were publicly released for other researchers that are interested in further understanding different dynamics during fine-tuning.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Zoom In: An Introduction to Circuits&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
OpenAI researchers published a &lt;a href=&quot;https://distill.pub/2020/circuits/zoom-in/&quot;&gt;piece&lt;/a&gt; discussing the state of interpretability of neural networks and the proposal of a new approach to interpreting them. Inspired by cellular biology, the authors delve deep into understanding vision models and what they learn by inspecting the weights of neural networks. Essentially, the study presented a few claims along with collected evidence that they believe could pave the way to better interpret neural networks.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*i0c-qpiire6dD4IqJVKlYg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;NLP Research Highlightsâ€Šâ€”â€ŠIssue #1&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In a new dair.ai series called &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-bertology-primer-fastpages-t5-data-science-education-pytorch-notebooks-slow-8ae5d499e040&quot;&gt;NLP Research Highlights&lt;/a&gt;, we provide detailed descriptions of current interesting and important NLP research. This will serve as a way to keep track of NLP progress via approachable summaries of these works. In the first quarterly issue, topics range from improving language models to improving conversational agents to state-of-the-art speech recognition systems. These summaries will also be maintained &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Learning to Simulate Complex Physics with Graph Networks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In the past few months, we have been featuring a lot about Graph Neural Networks (GNNs) due to their effectiveness not only in NLP but in other areas such as genomics and materials. In a recent &lt;a href=&quot;https://arxiv.org/abs/2002.09405&quot;&gt;paper&lt;/a&gt;, researchers propose a general framework based on graph networks that is able to learn simulations in different domains such as fluids and deformable materials. The authors claim that they achieve state-of-the-art performance across different domains and that their general-purpose approach is potentially the best-learned physics simulator to date. Experiments include the simulation of materials such as goop over water and other interactions with rigid obstacles. They also tested a pre-trained model on out-of-distribution tasks and found promising results that show the generalization of the framework to larger domains.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*48EolUDJoHpYRCTZxgn_qg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.09405.pdf&quot;&gt;&lt;em&gt;(Sanchez-Gonzalez et al., 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Language-specific BERT models&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Arabic BERT (AraBERT) is now available in the Hugging Face Transformer library. You can access the model &lt;a href=&quot;https://huggingface.co/aubmindlab/bert-base-arabert&quot;&gt;here&lt;/a&gt; and the paper &lt;a href=&quot;https://arxiv.org/abs/2003.00104&quot;&gt;here&lt;/a&gt;. Recently, a Japanese version of BERT was also &lt;a href=&quot;https://github.com/akirakubo/bert-japanese-aozora&quot;&gt;released&lt;/a&gt;. And there is also a Polish version of BERT called &lt;a href=&quot;https://github.com/kldarek/polbert&quot;&gt;Polbert&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;creativity-ethics-and-society-&quot;&gt;Creativity, Ethics, and Society ğŸŒ&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Computational predictions of protein structures associated with COVID-19&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind releases &lt;a href=&quot;https://deepmind.com/research/open-source/computational-predictions-of-protein-structures-associated-with-COVID-19&quot;&gt;computationally-predicted structures&lt;/a&gt; for proteins linked with the virus related to COVID-19. The predictions are directly obtained from the AlphaFold systems but havenâ€™t been experimentally verified. The idea with this release is to encourage contributions that aim to better understand the virus and how it functions.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Court cases that sound like the weirdest fights&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Janelle Shane shares the &lt;a href=&quot;https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights&quot;&gt;results&lt;/a&gt; of a fun experiment where a GPT-2 model is fine-tuned to generate cases against inanimate objects. The model was fed a list of cases where the government was seizing contraband or dangerous goods and it generated cases like the ones shown in the picture below.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*E5mHmkm1h4VQJ2Ni.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights&quot;&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Toward Human-Centered Design for ML Frameworks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI &lt;a href=&quot;https://ai.googleblog.com/2020/03/toward-human-centered-design-for-ml.html&quot;&gt;published&lt;/a&gt; the results of a large-scale survey of 645 people who used TensorFlow.js. They aimed to find out from non-ML software developers what are the most important features and their overall experience with using current ML frameworks. Findings include that the â€œlack of conceptual understanding of MLâ€ hinders the use of ML frameworks for this particular set of users. Participants in the study also reported the need for better instructions on how to apply the ML models to different problems and more explicit support for modification.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Face and hand tracking in the browser with MediaPipe and TensorFlow.js&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
This awesome &lt;a href=&quot;https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html?linkId=83996111&quot;&gt;TensorFlow article&lt;/a&gt; provides a walkthrough of how to enable real-time face and hand tracking on the browser using TensorFlow.js and MediaPipe.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*XsRsB-tSOZo9yWOc.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Credit: TensorFlow Blog&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;tools-and-datasets-ï¸&quot;&gt;Tools and Datasets âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;NLP Paper Summaries&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
We recently created a &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;repository&lt;/a&gt; containing a list of carefully curated NLP paper summaries for some of the most interesting and important NLP papers in the past few years. The focus is to feature paper summaries and blog posts of important papers to help improve the approachability and accessibility of NLP topics and research.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;A differentiable computer vision library for PyTorch.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/kornia/kornia&quot;&gt;Kornia&lt;/a&gt; is an open-source library built on top of PyTorch that allows researchers to use a set of operators for performing differentiable computer vision using PyTorch. Some capabilities include image transformations, depth estimation, and low-level image processing, to name a few. It is heavily inspired by OpenCV but the difference is that it is meant to be used for research as opposed to building production-ready applications.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*gN_-llcA4_3lIHYE.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introducing DIET: state-of-the-art architecture that outperforms fine-tuning BERT and is 6X faster to train&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DIET (Dual Intent and Entity Transformer) is a natural language understanding (NLU) multitask architecture &lt;a href=&quot;https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/&quot;&gt;proposed&lt;/a&gt; by Rasa. The framework focuses on multitask training to improve results on both intent classification and entity recognition. Other benefits of DIET include the ability to use any of the current pre-trained embeddings such as BERT and GloVe. However, the focus was to provide a model that improves the current state-of-the-art performance on those tasks and is faster to train (6X speedup reported). The model is available in the &lt;a href=&quot;https://rasa.com/docs/rasa/1.8.0/nlu/components/#dietclassifier&quot;&gt;Rasa Open Source python library&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*R_8FOU-CVZabv7hJ.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/?utm_source=twitter&quot;&gt;&lt;em&gt;DIET framework&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Lost in (language-specific) BERT models?&lt;/em&gt;&lt;/strong&gt;
&lt;a href=&quot;https://bertlang.unibocconi.it/&quot;&gt;BERT Lang Street&lt;/a&gt; is a neat website that provides the ability to search over 30 BERT-based models with 18 languages and 28 tasks with a total of 177 entries. For instance, if you wanted to find out the state-of-the-art results for sentiment classification using BERT models, you can just search for â€œsentimentâ€ in the search bar (example shown in the screenshot below).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*UuVno2eOAzYb_wlSSfukPA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Med7&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrey Kormilitzin releases &lt;a href=&quot;https://github.com/kormilitzin/med7&quot;&gt;Med7&lt;/a&gt; which is a model for performing clinical NLP (in particular named entity recognition (NER) tasks) on electronic health records. The model can identify up to seven categories and is available for use with the spaCy library.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yOMqhvTwYnxB4LYXv2Mgjg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;An Open Source Library for Quantum Machine Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html&quot;&gt;TensorFlow Quantum&lt;/a&gt; is an open-source library that provides a toolbox for rapid prototyping of quantum ML research that allows the application of ML models to approach problems ranging from medicine to materials.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Fast and Easy Infinitely Wide Networks with Neural Tangents&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Neural Tangents is an open-source library that allows researchers to build and train infinite-width models and finite neural networks using JAX. Read the blog post of the release &lt;a href=&quot;https://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html&quot;&gt;here&lt;/a&gt; and get access to the library &lt;a href=&quot;https://github.com/google/neural-tangents&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*CojgKJwB_n_7-j0DJZ0y7g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;articles-and-blog-posts-ï¸&quot;&gt;Articles and Blog posts âœï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;From PyTorch to JAX: towards neural net frameworks that purify stateful code&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sabrina J. Mielke published an &lt;a href=&quot;https://sjmielke.com/jax-purify.htm&quot;&gt;article&lt;/a&gt; that provides a walkthrough of how to build and train neural networks using JAX. The article focuses on comparing the inner workings of PyTorch and JAX when building neural networks, which helps to better understand some of the benefits and differences of JAX.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*Nrw4UnmnIZ__elHu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://sjmielke.com/jax-purify.htm&quot;&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/a&gt; **&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Why do we still use 18-year old BLEU?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In this &lt;a href=&quot;https://ehudreiter.com/2020/03/02/why-use-18-year-old-bleu/&quot;&gt;blog post&lt;/a&gt;, Ehud Reiter talks about why we still use old evaluation techniques like BLUE for evaluating NLP models for tasks like machine translation. As a researcher in the space, he also expresses the implications for techniques that perform the evaluation on more recent tasks.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introducing BART&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1910.13461&quot;&gt;BART&lt;/a&gt; is a new model proposed by Facebook that involves a denoising autoencoder for pretraining seq2seq models that improve performance on downstream text generation tasks such as abstractive summarization. Sam Shleifer provides a &lt;a href=&quot;https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html&quot;&gt;nice summary&lt;/a&gt; of BART and how he integrated it into the Hugging Face Transformers repo.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;A Survey of Long-Term Context in Transformers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Madison May recently wrote an interesting &lt;a href=&quot;https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/&quot;&gt;survey&lt;/a&gt; describing ways to improve Transformer based approaches, which include Sparse Transformers, Adaptive Span Transformers, Transformer-XL, compressive Transformers, Reformer, and routing transformer. We also touched on some of these topics in the dair.ai &lt;a href=&quot;https://medium.com/dair-ai&quot;&gt;publication&lt;/a&gt; and in this list of &lt;a href=&quot;https://medium.com/dair-ai/nlp-research-highlights-cd522b21b01a&quot;&gt;paper summaries&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;â€œMind your language, GPT-2â€: how to control style and content in automatic text writing&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Despite the impressive fluency automatic text writing has exhibited in the past year, it is still challenging to control attributes like structure or content of the machine-written text. In a &lt;a href=&quot;https://creatext.ai/blog-posts/controllable-text-generation&quot;&gt;recent blog post&lt;/a&gt;, Manuel Tonneau discusses the recent progress and the perspectives in the field of controllable text generation, from Hugging Faceâ€™s GPT-2 model fine-tuned on arXiv to Googleâ€™s T5, with mentions of Salesforceâ€™s CTRL and Uber AIâ€™s PPLM.&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education ğŸ“&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Talk: The Future of NLP in Python&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In one of our previous newsletters, we featured &lt;a href=&quot;https://thinc.ai/&quot;&gt;THiNC&lt;/a&gt; which is a functional deep learning library focused on compatibility with other existing libraries. This &lt;a href=&quot;https://speakerdeck.com/inesmontani/the-future-of-nlp-in-python-keynote-pycon-colombia-2020?slide=9&quot;&gt;set of slides&lt;/a&gt; introduces a bit more of the library which was used in the talk by Ines Montani for PyCon Colombia.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Transformers Notebooks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
HuggingFace published a set of &lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/notebooks&quot;&gt;Colab notebooks&lt;/a&gt; that help to get started with their popular Transformers library. Some notebooks include using tokenization, setting up NLP pipelines, and training a language model on custom data.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*0AYHYUsHbaqV2vqN2zCzLQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;TensorFlow 2.0 in 7 hours&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Check out this &lt;a href=&quot;https://www.freecodecamp.org/news/massive-tensorflow-2-0-free-course/&quot;&gt;~7-hour free course&lt;/a&gt; on TensorFlow 2.0 containing topics that range from basic neural networks to NLP with RNNs to an introduction to reinforcement learning.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;DeepMind: The Podcast&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind has released all episodes (in the form of a &lt;a href=&quot;https://www.youtube.com/playlist?list=PLqYmG7hTraZBiUr6_Qf8YTS2Oqy3OGZEj&quot;&gt;YouTube playlist&lt;/a&gt;) for their podcast which features scientists, researchers, and engineers discussing topics that range from AGI to neuroscience to robotics.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Machine Learning and Deep Learning Courses&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Berkeley is publicly releasing the &lt;a href=&quot;https://sites.google.com/view/berkeley-cs294-158-sp20/home&quot;&gt;entire syllabus&lt;/a&gt; for its course on â€œDeep Unsupervised Learningâ€ mainly focusing on the theoretical aspects of self-supervised learning and generative models. Some topics include latent variable models, autoregressive models, flow models, and self-supervised learning, to name a few. Youtube videos and slides are available.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
We also found this &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/&quot;&gt;impressive list&lt;/a&gt; of advanced online courses on machine learning, NLP and deep learning.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
And here is another course called &lt;a href=&quot;https://compstat-lmu.github.io/lecture_i2ml/index.html&quot;&gt;â€œIntroduction to Machine Learning&lt;/a&gt;â€ which includes topics such as supervised regression, performance evaluation, random forests, parameter tuning, practical advice, and much more.&lt;/p&gt;

&lt;h1 id=&quot;noteworthy-mentions-ï¸&quot;&gt;Noteworthy Mentions â­ï¸&lt;/h1&gt;

&lt;p&gt;The previous NLP Newsletter (Issue #6) is available &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-bertology-primer-fastpages-t5-data-science-education-pytorch-notebooks-slow-8ae5d499e040&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Connon Shorten published a &lt;a href=&quot;https://www.youtube.com/watch?v=QWu7j1nb_jI&amp;amp;feature=emb_logo&quot;&gt;video&lt;/a&gt; explaining the ELECTRA model which proposes a technique called replaced token detection to pre-train Transformers more efficiently. If you are interested, we also wrote a short summary of the model &lt;a href=&quot;https://medium.com/dair-ai/nlp-research-highlights-cd522b21b01a&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Rachael Tatman is working on a new series called &lt;a href=&quot;https://www.youtube.com/watch?v=-G36q8_cYsc&amp;amp;feature=emb_logo&quot;&gt;NLP for Developers&lt;/a&gt; where the idea is to talk more in-depth about different NLP methods, when to use them and explaining common issues that you may run into.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind releases &lt;a href=&quot;https://youtu.be/WXuK6gekU1Y&quot;&gt;AlphaGoâ€Šâ€”â€ŠThe Movie&lt;/a&gt; on YouTube to celebrate the 4th anniversary of AlphaGo beating Lee Sedol at the game of Go.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
OpenMined has &lt;a href=&quot;https://blog.openmined.org/introducing-openmined-research/&quot;&gt;open positions&lt;/a&gt; for Research Engineer and Research Scientist roles which is a good opportunity to get involved with privacy-preserving AI.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;If you have any datasets, projects, blog posts, tutorials, or papers that you wish to share in the next iteration of the NLP Newsletter, please free to reach out to me at ellfae@gmail.com or &lt;em&gt;**&lt;/em&gt;&lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;&lt;strong&gt;DM on Twitter&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Subscribe&lt;/em&gt;&lt;/a&gt; &lt;em&gt;ğŸ”– to the NLP Newsletter to receive future issues in your inbox.&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_NLP_7/&quot;&gt;NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,â€¦&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 16, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,â€¦]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_-7_-FR/" />
  <id>https://dair.ai/NLP_Newsletter_#7_[FR]</id>
  <published>2020-03-16T00:00:00-05:00</published>
  <updated>2020-03-16T00:00:00-05:00</updated>
  <author>
    <name>LoÃ¯ck BOURDOIS</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*9gNslKwKiRaffDt2RSOgoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;avant-propos-delvis&quot;&gt;Avant-propos dâ€™Elvis&lt;/h1&gt;
&lt;p&gt;&lt;br /&gt;
Bienvenue au 7e numÃ©ro de la lettre dâ€™information consacrÃ©e au NLP. Jâ€™espÃ¨re que vous passez une merveilleuse journÃ©e et que vous et vos proches Ãªtes en sÃ©curitÃ© en ces temps difficiles. Nous avons dÃ©cidÃ© de publier ce bulletin pour apporter un peu de joie Ã  nos lecteurs, alors nâ€™hÃ©sitez pas Ã  le lire quand vous aurez du temps libre. Pour lâ€™instant, concentrons-nous sur les choses qui sont de la plus haute prioritÃ© : nos familles et nos amis. â¤ï¸ ğŸ’› ğŸ’š&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Quelques mises Ã  jour sur la lettre dâ€™information sur le NLP et sur dair.ai.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les traductions franÃ§aises et chinoises de tous les numÃ©ros prÃ©cÃ©dents de la newsletter sont dÃ©sormais &lt;a href=&quot;https://github.com/dair-ai/nlp_newsletter&quot;&gt;disponibles&lt;/a&gt;. DÃ©couvrez comment vous pouvez contribuer Ã  la traduction des numÃ©ros prÃ©cÃ©dents et Ã  venir en cliquant sur ce &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;lien&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Nous avons rÃ©cemment crÃ©Ã© deux nouveaux dÃ©pÃ´ts GitHub qui contiennent des &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;rÃ©sumÃ©s de publications sur le NLP&lt;/a&gt; et des &lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;notebooks PyTorch&lt;/a&gt; pour vous aider Ã  dÃ©marrer avec les rÃ©seaux de neurones.&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications ğŸ“™&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Mesure de la gÃ©nÃ©ralisation de la composition&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans le contexte de lâ€™apprentissage machine, la gÃ©nÃ©ralisation de la composition est la capacitÃ© dâ€™apprendre Ã  reprÃ©senter le sens et des sÃ©quences (combinaisons inÃ©dites) Ã  partir de ce qui est appris dans le jeu dâ€™entraÃ®nement. Ã€ ce jour, la maniÃ¨re de mesurer correctement la composition dans les rÃ©seaux neuronaux nâ€™est pas claire. Une Ã©quipe de Google AI &lt;a href=&quot;https://ai.googleblog.com/2020/03/measuring-compositional-generalization.html&quot;&gt;propose&lt;/a&gt; lâ€™un des plus grands benchmarks pour la gÃ©nÃ©ralisation de la composition en utilisant des tÃ¢ches telles que le question/ answering et lâ€™analyse sÃ©mantique. Lâ€™image ci-dessous montre un exemple du modÃ¨le proposÃ© utilisant des atomes (produire, diriger, etc.) pour produire de nouveaux composÃ©s, câ€™est-Ã -dire des combinaisons dâ€™atomes. Lâ€™idÃ©e de ce travail est de produire des Ã©chantillons entraÃ®nement/test qui contiennent des exemples qui partagent des atomes similaires (blocs de construction pour gÃ©nÃ©rer des exemples) de distribution mais une distribution de composÃ©s diffÃ©rente (la composition des atomes). Les auteurs affirment que câ€™est une faÃ§on plus fiable de tester la gÃ©nÃ©ralisation de la composition.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*lXmUWOY8HJL7YVn1.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Credit: Google AI Blog&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Fine-Tuning de modÃ¨les linguistiques prÃ©-entraÃ®nÃ©s : Initialisation des poids, ordonnancement des donnÃ©es et arrÃªt anticipÃ©&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Des chercheurs ont menÃ© une &lt;a href=&quot;https://arxiv.org/abs/2002.06305&quot;&gt;sÃ©rie dâ€™essais de fine-tuning&lt;/a&gt; pour mieux comprendre lâ€™effet de lâ€™initialisation du poids et de lâ€™arrÃªt prÃ©coce dans la performance des modÃ¨les. Au cours de diverses expÃ©riences qui ont nÃ©cessitÃ© des centaines de tunage de BERT, il a Ã©tÃ© constatÃ© que des graines alÃ©atoires distinctes donnent des rÃ©sultats trÃ¨s diffÃ©rents. En particulier, lâ€™Ã©tude indique quâ€™une certaine initialisation des poids donne de bons rÃ©sultats pour un ensemble de tÃ¢ches. Toutes les donnÃ©es expÃ©rimentales et les essais ont Ã©tÃ© rendus publics pour les autres chercheurs qui souhaitent mieux comprendre les diffÃ©rentes dynamiques lors de la mise au point.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Une introduction aux circuits&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les chercheurs dâ€™OpenAI ont publiÃ© un &lt;a href=&quot;https://distill.pub/2020/circuits/zoom-in/&quot;&gt;article&lt;/a&gt; sur lâ€™interprÃ©tabilitÃ© des rÃ©seaux de neurones et proposent une nouvelle approche pour les interprÃ©ter. InspirÃ©s par la biologie cellulaire, les auteurs approfondissent la comprÃ©hension des modÃ¨les de vision et de ce quâ€™ils apprennent en inspectant le poids des rÃ©seaux neuronaux. Essentiellement, lâ€™Ã©tude prÃ©sente quelques affirmations ainsi que des preuves recueillies qui, selon eux, pourraient ouvrir la voie Ã  une meilleure interprÃ©tation des rÃ©seaux neuronaux.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*i0c-qpiire6dD4IqJVKlYg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;NLP Research Highlightsâ€Šâ€”â€ŠIssue #1&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans une nouvelle sÃ©rie de dair.ai intitulÃ©e &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-bertology-primer-fastpages-t5-data-science-education-pytorch-notebooks-slow-8ae5d499e040&quot;&gt;NLP Research Highlights&lt;/a&gt;, nous fournissons des descriptions dÃ©taillÃ©es des recherches actuelles intÃ©ressantes et importantes sur le NLP. Dans le premier numÃ©ro trimestriel, les sujets vont de lâ€™amÃ©lioration des modÃ¨les de langage Ã  lâ€™amÃ©lioration des agents conversationnels en passant par les systÃ¨mes de reconnaissance vocale de pointe. Ces rÃ©sumÃ©s seront Ã©galement mis Ã  jour &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Apprendre Ã  simuler la physique complexe avec les rÃ©seaux de graphes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ces derniers mois, nous avons beaucoup parlÃ© des rÃ©seaux de neurones de graphes (GNN) en raison de leur efficacitÃ© non seulement en NLP mais aussi dans dâ€™autres domaines tels que la gÃ©nomique et les matÃ©riaux. Dans un rÃ©cent &lt;a href=&quot;https://arxiv.org/abs/2002.09405&quot;&gt;article&lt;/a&gt;, des chercheurs proposent un cadre gÃ©nÃ©ral basÃ© sur les rÃ©seaux de graphes qui est capable dâ€™apprendre des simulations dans diffÃ©rents domaines tels que les fluides et les matÃ©riaux dÃ©formables. Les auteurs affirment quâ€™ils obtiennent des performances de pointe dans diffÃ©rents domaines et que leur approche gÃ©nÃ©rale est potentiellement le simulateur de physique le mieux appris Ã  ce jour. Les expÃ©riences comprennent la simulation de matÃ©riaux tels que le Â« goop over water Â» (je ne sais pas comment cela se traduit en franÃ§ais) et dâ€™autres interactions avec des obstacles rigides. Ils ont Ã©galement testÃ© un modÃ¨le prÃ©-entraÃ®nÃ© sur des tÃ¢ches hors distribution et ont trouvÃ© des rÃ©sultats prometteurs qui montrent la gÃ©nÃ©ralisation du framework Ã  des domaines plus vastes.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*48EolUDJoHpYRCTZxgn_qg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.09405.pdf&quot;&gt;&lt;em&gt;(Sanchez-Gonzalez et al., 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;ModÃ¨les BERT spÃ©cifiques Ã  chaque langue&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le BERT en arabe (AraBERT) est maintenant disponible dans la librairie Transformer. Vous pouvez accÃ©der au modÃ¨le &lt;a href=&quot;https://huggingface.co/aubmindlab/bert-base-arabert&quot;&gt;ici&lt;/a&gt; et au document &lt;a href=&quot;https://arxiv.org/abs/2003.00104&quot;&gt;ici&lt;/a&gt;. RÃ©cemment, une version japonaise de BERT a Ã©galement Ã©tÃ© &lt;a href=&quot;https://github.com/akirakubo/bert-japanese-aozora&quot;&gt;publiÃ©e&lt;/a&gt;. Il existe Ã©galement une version polonaise de BERT appelÃ©e &lt;a href=&quot;https://github.com/kldarek/polbert&quot;&gt;Polbert&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;crÃ©ativitÃ©-Ã©thique-et-sociÃ©tÃ©-&quot;&gt;CrÃ©ativitÃ©, Ã©thique et sociÃ©tÃ© ğŸŒ&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;PrÃ©visions des structures protÃ©iques associÃ©es au COVID-19&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind dÃ©voile &lt;a href=&quot;https://deepmind.com/research/open-source/computational-predictions-of-protein-structures-associated-with-COVID-19&quot;&gt;des structures prÃ©dites par calcul&lt;/a&gt; pour les protÃ©ines liÃ©es au COVID-19. Les prÃ©dictions sont directement obtenues Ã  partir des systÃ¨mes AlphaFold mais nâ€™ont pas Ã©tÃ© vÃ©rifiÃ©es expÃ©rimentalement. Lâ€™idÃ©e de cette publication est dâ€™encourager les contributions qui visent Ã  mieux comprendre le virus et son fonctionnement.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Utilisation du GPT2 pour une expÃ©rience sur des cas judiciaires&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Janelle Shane partage les &lt;a href=&quot;https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights&quot;&gt;rÃ©sultats&lt;/a&gt; dâ€™une expÃ©rience amusante oÃ¹ un modÃ¨le GPT-2 est fine-tunÃ© pour gÃ©nÃ©rer des cas contre des objets inanimÃ©s. Le modÃ¨le a Ã©tÃ© alimentÃ© par une liste de cas oÃ¹ le gouvernement saisissait des marchandises de contrebande ou dangereuses. Cela a gÃ©nÃ©rÃ© des cas comme ceux prÃ©sentÃ©s dans lâ€™image ci-dessous.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*E5mHmkm1h4VQJ2Ni.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights&quot;&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Vers une conception centrÃ©e sur lâ€™homme des frameworks de ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI &lt;a href=&quot;https://ai.googleblog.com/2020/03/toward-human-centered-design-for-ml.html&quot;&gt;a publiÃ©&lt;/a&gt; les rÃ©sultats dâ€™une enquÃªte Ã  grande Ã©chelle menÃ©e auprÃ¨s de 645 personnes ayant utilisÃ© TensorFlow.js. Lâ€™objectif Ã©tait de connaÃ®tre les caractÃ©ristiques les plus importantes ainsi que lâ€™expÃ©rience gÃ©nÃ©rale des dÃ©veloppeurs de logiciels non ML testant des frameworks de ML actuels.
Les rÃ©sultats montrent notamment que le â€œmanque de comprÃ©hension conceptuelle du MLâ€ entrave lâ€™utilisation des frameworks de ML pour cet ensemble dâ€™utilisateurs. Les participants Ã  lâ€™Ã©tude ont Ã©galement signalÃ© le besoin de meilleures instructions sur la faÃ§on dâ€™appliquer les modÃ¨les de ML Ã  diffÃ©rents problÃ¨mes et dâ€™un soutien plus explicite pour les modifications.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Tracking du visage et de la main avec MediaPipe et TensorFlow.js&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cet &lt;a href=&quot;https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html?linkId=83996111&quot;&gt;article sur TensorFlow&lt;/a&gt; explique comment activer le suivi des visages et des mains en temps rÃ©el Ã  lâ€™aide de TensorFlow.js et de MediaPipe.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*XsRsB-tSOZo9yWOc.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Credit: TensorFlow Blog&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;outils-et-jeux-de-donnÃ©es-ï¸&quot;&gt;Outils et jeux de donnÃ©es âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;NLP Paper Summaries&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Nous avons rÃ©cemment crÃ©Ã© un &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;dÃ©pÃ´t&lt;/a&gt; contenant des rÃ©sumÃ©s des publications de NLP les intÃ©ressantes et les plus importantes de ces derniÃ¨res annÃ©es. Lâ€™objectif est dâ€™amÃ©liorer lâ€™accessibilitÃ© de ces recherches et sujets.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Une librairie de vision par ordinateur pour PyTorch&lt;/em&gt;&lt;/strong&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/kornia/kornia&quot;&gt;Kornia&lt;/a&gt; est une librairie open-source construite sur PyTorch qui permet aux chercheurs dâ€™utiliser un ensemble dâ€™opÃ©rateurs pour rÃ©aliser une vision informatique diffÃ©renciÃ©e en utilisant PyTorch. Parmi les fonctionnalitÃ©s, on trouve les transformations dâ€™images, lâ€™estimation de la profondeur et le traitement dâ€™images de bas niveau, pour nâ€™en citer que quelques-unes. Elle est fortement inspirÃ©e dâ€™OpenCV, mais la diffÃ©rence est quâ€™elle est destinÃ©e Ã  la recherche plutÃ´t quâ€™Ã  la crÃ©ation dâ€™applications prÃªtes Ã  la production.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*gN_-llcA4_3lIHYE.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PrÃ©sentation de DIET : une architecture qui surpasse le fine-tuning de BERT et qui est 6X plus rapide Ã  entraÃ®ner&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DIET (Dual Intent and Entity Transformer) est une architecture multitÃ¢che de comprÃ©hension du langage naturel (NLU) &lt;a href=&quot;https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/&quot;&gt;proposÃ©e&lt;/a&gt; par Rasa. Le framework se concentre sur lâ€™entraÃ®nement multitÃ¢che afin dâ€™amÃ©liorer les rÃ©sultats en matiÃ¨re de classification des intentions et de reconnaissance des entitÃ©s. Un autre avantage de DIET est que lâ€™on peut utiliser nâ€™importe quel Ã©lÃ©ment prÃ© entrainÃ© tels que BERT et GloVe. Lâ€™objectif principale de cette librairie est de fournir un modÃ¨le qui amÃ©liore les performances actuelles de ces tÃ¢ches et qui est plus rapide Ã  entraÃ®ner (accÃ©lÃ©ration de 6X). Le modÃ¨le est disponible dans la &lt;a href=&quot;https://rasa.com/docs/rasa/1.8.0/nlu/components/#dietclassifier&quot;&gt;librairie python Rasa Open Source&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*R_8FOU-CVZabv7hJ.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/?utm_source=twitter&quot;&gt;&lt;em&gt;DIET framework&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Perdu parmi toutes les langues disponibles pour BERT ?&lt;/em&gt;&lt;/strong&gt;
&lt;a href=&quot;https://bertlang.unibocconi.it/&quot;&gt;BERT Lang Street&lt;/a&gt; est un site web qui offre la possibilitÃ© de rechercher plus de 30 modÃ¨les basÃ©s sur le BERT, en 18 langues et 28 tÃ¢ches, soit un total de 177 entrÃ©es. Par exemple, si vous souhaitez connaÃ®tre les rÃ©sultats de la classification des sentiments Ã  lâ€™aide des modÃ¨les de BERT, il vous suffit de rechercher â€œsentimentâ€ dans la barre de recherche (exemple illustrÃ© dans la capture dâ€™Ã©cran ci-dessous).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*UuVno2eOAzYb_wlSSfukPA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Med7&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrey Kormilitzin publie &lt;a href=&quot;https://github.com/kormilitzin/med7&quot;&gt;Med7&lt;/a&gt; qui est un modÃ¨le pour du NLP Ã  usage clinique (en particulier les tÃ¢ches de reconnaissance dâ€™entitÃ©s nommÃ©es (NER)) sur les dossiers mÃ©dicaux Ã©lectroniques. Le modÃ¨le peut identifier jusquâ€™Ã  sept catÃ©gories et est disponible pour Ãªtre utilisÃ© avec la bibliothÃ¨que spaCy.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yOMqhvTwYnxB4LYXv2Mgjg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Une bibliothÃ¨que open source pour le ML quantique&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html&quot;&gt;TensorFlow Quantum&lt;/a&gt; est une bibliothÃ¨que open-source qui fournit une boÃ®te Ã  outils pour le prototypage rapide de la recherche en ML quantique qui permet lâ€™application de modÃ¨les de ML pour aborder des problÃ¨mes allant de la mÃ©decine aux matÃ©riaux.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Des rÃ©seaux infiniment larges, rapides et faciles, avec Neural Tangents&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Neural Tangents est une librairie open-source qui permet aux chercheurs de dâ€™entraÃ®ner des modÃ¨les de largeur finie ou infinie en utilisant JAX. Lisez lâ€™article du blog de la version &lt;a href=&quot;https://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html&quot;&gt;ici&lt;/a&gt; et accÃ©dez Ã  la librairie &lt;a href=&quot;https://github.com/google/neural-tangents&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*CojgKJwB_n_7-j0DJZ0y7g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;articles-et-blog-ï¸&quot;&gt;Articles et Blog âœï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;De PyTorch Ã  JAX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sabrina J. Mielke a publiÃ© un &lt;a href=&quot;https://sjmielke.com/jax-purify.htm&quot;&gt;article&lt;/a&gt; qui explique comment construire et entraÃ®ner des rÃ©seaux de neurones en utilisant JAX. Lâ€™article se concentre sur la comparaison du fonctionnement interne de PyTorch et de JAX lors de la construction de rÃ©seaux neuronaux, ce qui permet de mieux comprendre certains des avantages et des diffÃ©rences de JAX.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*Nrw4UnmnIZ__elHu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://sjmielke.com/jax-purify.htm&quot;&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Pourquoi utilisons-nous encore des jeux de donnÃ©es test crÃ©Ã©s il y a plus de 18 ans ?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans cet &lt;a href=&quot;https://ehudreiter.com/2020/03/02/why-use-18-year-old-bleu/&quot;&gt;article de blog&lt;/a&gt;, Ehud Reiter explique pourquoi nous utilisons encore de vieilles techniques dâ€™Ã©valuation comme BLUE pour Ã©valuer les modÃ¨les de NLP pour des tÃ¢ches comme la traduction automatique&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introduction Ã  BART&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1910.13461&quot;&gt;BART&lt;/a&gt; est un modÃ¨le proposÃ© par Facebook qui implique un autoencodeur de dÃ©bruitage pour le prÃ©-entraÃ®nement de modÃ¨les seq2seq, ce qui amÃ©liorent les performances sur les tÃ¢ches de gÃ©nÃ©ration de texte telles que le rÃ©sumÃ© abstrait. Sam Shleifer fournit un &lt;a href=&quot;https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html&quot;&gt;rÃ©sumÃ©&lt;/a&gt; de BART et explique comment il lâ€™a intÃ©grÃ© dans la librairie Transformers de Hugging Face.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;AmÃ©liorations des Transformers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Madison May a rÃ©cemment rÃ©digÃ© une &lt;a href=&quot;https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/&quot;&gt;enquÃªte&lt;/a&gt; dÃ©crivant les moyens dâ€™amÃ©liorer les approches basÃ©es sur les Transformers. Lâ€™article aborde ainsi les Sparse Transformers, les Adaptive Span Transformers, le Transformer-XL, les compressive Transformers, le Reformer, et les routing transformers.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;ContrÃ´ler le style et le contenu dans la rÃ©daction automatique de textes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
MalgrÃ© lâ€™impressionnante fluiditÃ© dont a fait preuve lâ€™Ã©criture automatique des textes lâ€™annÃ©e derniÃ¨re, il est toujours difficile de contrÃ´ler des attributs comme la structure ou le contenu du texte. Dans un &lt;a href=&quot;https://creatext.ai/blog-posts/controllable-text-generation&quot;&gt;rÃ©cent article de blog&lt;/a&gt;, Manuel Tonneau Ã©voque les progrÃ¨s rÃ©cents et les perspectives dans le domaine de la gÃ©nÃ©ration de texte contrÃ´lable, du modÃ¨le GPT-2 de Hugging Face, fine-tunÃ© sur arXiv, au T5 de Google, en passant par CTRL de Salesforce et PPLM de Uber AI.&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education ğŸ“&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Lâ€™avenir du NLP en Python&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans lâ€™un de nos numÃ©ros prÃ©cÃ©dents, nous avons prÃ©sentÃ© &lt;a href=&quot;https://thinc.ai/&quot;&gt;THiNC&lt;/a&gt;, qui est une librairie de DL fonctionnelle axÃ©e sur la compatibilitÃ© avec dâ€™autres librairies existantes. Ces &lt;a href=&quot;https://speakerdeck.com/inesmontani/the-future-of-nlp-in-python-keynote-pycon-colombia-2020?slide=9&quot;&gt;diapositives&lt;/a&gt; prÃ©sentent un peu plus cette livrairie qui a Ã©tÃ© utilisÃ©e par Ines Montani pour la confÃ©rence PyCon Colombia.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Les notebooks de Transformers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
HuggingFace a publiÃ© un ensemble de &lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/notebooks&quot;&gt;notebooks&lt;/a&gt; qui aident Ã  dÃ©marrer avec leur librairie Transformers. Certains notebooks comprennent lâ€™utilisation de la tokenisation, la mise en place de pipelines et lâ€™entraÃ®nement dâ€™un modÃ¨le sur des donnÃ©es personnalisÃ©es.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*0AYHYUsHbaqV2vqN2zCzLQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;TensorFlow 2.0 en 7 heures&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jetez un oeil Ã  ce &lt;a href=&quot;https://www.freecodecamp.org/news/massive-tensorflow-2-0-free-course/&quot;&gt;cours gratuity dâ€™environ 7h&lt;/a&gt; consacrÃ© Ã  TensorFlow 2.0 abordant des sujets qui vont des rÃ©seaux neuronaux de base au NLP avec les RNN, en passant par une introduction Ã  lâ€™apprentissage par renforcement.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;DeepMind: le podcast&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind a publiÃ© tous les Ã©pisodes (sous la forme dâ€™une &lt;a href=&quot;https://www.youtube.com/playlist?list=PLqYmG7hTraZBiUr6_Qf8YTS2Oqy3OGZEj&quot;&gt;playlist YouTube&lt;/a&gt;) de son podcast qui prÃ©sente des scientifiques, des chercheurs et des ingÃ©nieurs discutant de sujets allant de lâ€™IAG aux neurosciences en passant par la robotique.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Cours de Machine Learning et de Deep Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Berkeley rend public le &lt;a href=&quot;https://sites.google.com/view/berkeley-cs294-158-sp20/home&quot;&gt;programme complet&lt;/a&gt; de son cours sur â€œlâ€™apprentissage profond non supervisÃ©â€, principalement axÃ© sur les aspects thÃ©oriques de lâ€™apprentissage autosupervisÃ© ainsi que sur les modÃ¨les gÃ©nÃ©rateurs. Parmi les sujets abordÃ©s, citons les modÃ¨les de variables latentes, les modÃ¨les autorÃ©gressifs et les modÃ¨les de flux pour nâ€™en citer que quelques-uns. Des vidÃ©os et des diapositives sont disponibles sur Youtube.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Nous avons Ã©galement trouvÃ© cette importante &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/&quot;&gt;liste&lt;/a&gt; de cours en ligne sur lâ€™apprentissage machine, le NLP et lâ€™apprentissage approfondi.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Et voici un autre cours intitulÃ© &lt;a href=&quot;https://compstat-lmu.github.io/lecture_i2ml/index.html&quot;&gt;â€œIntroduction to Machine Learningâ€&lt;/a&gt; qui comprend des sujets tels que la rÃ©gression supervisÃ©e, lâ€™Ã©valuation des performances, les forÃªts alÃ©atoires, le rÃ©glage des paramÃ¨tres, des conseils pratiques, et bien plus encore.&lt;/p&gt;

&lt;h1 id=&quot;mentions-spÃ©ciales-ï¸&quot;&gt;Mentions spÃ©ciales â­ï¸&lt;/h1&gt;

&lt;p&gt;Connon Shorten a publiÃ© une &lt;a href=&quot;https://www.youtube.com/watch?v=QWu7j1nb_jI&amp;amp;feature=emb_logo&quot;&gt;vidÃ©o&lt;/a&gt; expliquant le modÃ¨le ELECTRA qui propose une technique appelÃ©e â€œremplacement de la dÃ©tection de jetonâ€ pour prÃ©traiter les Transformers plus efficacement. Si vous Ãªtes intÃ©ressÃ©, nous avons Ã©galement rÃ©digÃ© un bref rÃ©sumÃ© du modÃ¨le &lt;a href=&quot;https://medium.com/dair-ai/nlp-research-highlights-cd522b21b01a&quot;&gt;ici&lt;/a&gt; (en anglais).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Rachael Tatman travaille sur une nouvelle sÃ©rie intitulÃ©e &lt;a href=&quot;https://www.youtube.com/watch?v=-G36q8_cYsc&amp;amp;feature=emb_logo&quot;&gt;NLP for Developers&lt;/a&gt; oÃ¹ lâ€™idÃ©e est de parler plus en profondeur des diffÃ©rentes mÃ©thodes de NLP du moment et dâ€™expliquer les problÃ¨mes communs que vous pourriez rencontrer.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind diffuse &lt;a href=&quot;https://youtu.be/WXuK6gekU1Y&quot;&gt;AlphaGo - The Movie&lt;/a&gt; sur YouTube pour cÃ©lÃ©brer le 4Ã¨me anniversaire de la victoire dâ€™AlphaGo sur Lee Sedol au jeu de Go.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
OpenMined &lt;a href=&quot;https://blog.openmined.org/introducing-openmined-research/&quot;&gt;Ã©voque&lt;/a&gt; les rÃ´les dâ€™ingÃ©nieur de recherche et de chercheur scientifique, ce qui est une bonne occasion de sâ€™impliquer dans la prÃ©servation de la vie privÃ©e en matiÃ¨re dâ€™IA.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Vous pouvez retrouver la prÃ©cÃ©dente newsletter &lt;a href=&quot;https://dair.ai/NLP_Newsletter_-6_-FR/&quot;&gt;ici&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous avez des jeux de donnÃ©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine Ã©dition de la newletter, nâ€™hÃ©sitez pas Ã  contacter Elvis Ã  ellfae@gmail.com ou par message sur &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Abonnez-vous&lt;/a&gt; pour recevoir les prochains numÃ©ros dans votre boÃ®te mail.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_-7_-FR/&quot;&gt;NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,â€¦&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 16, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [FR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_-6_-FR/" />
  <id>https://dair.ai/NLP_Newsletter_#6_[FR]</id>
  <published>2020-03-09T00:00:00-05:00</published>
  <updated>2020-03-09T00:00:00-05:00</updated>
  <author>
    <name>LoÃ¯ck BOURDOIS</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*vWICxAehSy3xOnqGIXtpoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;avant-propos&quot;&gt;Avant-propos&lt;/h1&gt;
&lt;p&gt;Bienvenue au sixiÃ¨me numÃ©ro de la lettre dâ€™information consacrÃ©e au NLP. Merci pour votre soutien et pour avoir pris le temps de lire les derniÃ¨res nouvelles sur le ML et le NLP. Ce numÃ©ro traite de sujets allant de lâ€™extension du modÃ¨le Transformer au ralentissement de la publication en ML, en passant par une sÃ©rie de livres et de lancements de projets en ML et en NLP.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Quelques mises Ã  jour sur la lettre dâ€™information sur le NLP et sur dair.ai.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Nous avons traduit la lettre dâ€™information dans dâ€™autres langues telles que le portugais brÃ©silien, le chinois, lâ€™arabe, lâ€™espagnol, entre autres. Merci aux personnes qui ont aidÃ© Ã  la traduction. Vous pouvez Ã©galement contribuer &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Il y a un mois, nous avons officiellement lancÃ© notre nouveau &lt;a href=&quot;https://dair.ai/&quot;&gt;site web&lt;/a&gt;. Vous pouvez consulter notre &lt;a href=&quot;https://github.com/dair-ai&quot;&gt;organisation GitHub&lt;/a&gt; pour plus dâ€™informations sur dair.ai et ses projets. Si vous souhaitez voir comment dâ€™autres personnes contribuent dÃ©jÃ  Ã  dair.ai ou si vous souhaitez contribuer Ã  la dÃ©mocratisation de la recherche, de lâ€™Ã©ducation et des technologies en matiÃ¨re dâ€™intelligence artificielle, consultez notre &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues&quot;&gt;section&lt;/a&gt; sur les questions dâ€™actualitÃ©.&lt;/p&gt;

&lt;h1 id=&quot;publications--&quot;&gt;Publications  ğŸ“™&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Une introduction Ã  la BERTologie : Ce que nous savons sur le fonctionnement de BERT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les modÃ¨les basÃ©s sur le Transformer se sont avÃ©rÃ©s efficaces pour aborder diffÃ©rents types de tÃ¢ches de NLP allant de la classification de sÃ©quences Ã  la rÃ©ponse aux questions. Lâ€™un de ces modÃ¨les, appelÃ© BERT (&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;Devlin et al. 2019&lt;/a&gt;), est largement utilisÃ© mais comme dâ€™autres modÃ¨les qui utilisent des rÃ©seaux de neurones profonds, nous savons trÃ¨s peu de choses sur leur fonctionnement interne. Un nouvel &lt;a href=&quot;https://arxiv.org/abs/2002.12327&quot;&gt;article&lt;/a&gt; intitulÃ© â€œ A Primer in BERTology: What we know about how BERT works â€œ vise Ã  rÃ©pondre Ã  certaines des interrogations portant sur les raisons pour lesquelles BERT est performant dans un si grand nombre de tÃ¢ches de NLP. Parmi les sujets abordÃ©s dans cet article, on trouve le type de connaissances acquises par BERT ainsi que leur reprÃ©sentation, la maniÃ¨re dont ces connaissances sont acquises et les autres mÃ©thodes utilisÃ©es par les chercheurs pour les amÃ©liorer.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Explorer les limites de lâ€™apprentissage par transfert avec un Transformer de texte Ã  texte&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI a rÃ©cemment publiÃ© une &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;mÃ©thode&lt;/a&gt; qui rassemble tous les enseignements et les amÃ©liorations tirÃ©s des modÃ¨les de NLP basÃ©s sur lâ€™apprentissage par transfert.  Les auteurs lâ€™ont appelÃ© Text-to-Text Transfer Transformer (T5). Ce travail propose que la plupart des tÃ¢ches de NLP puissent Ãªtre formulÃ©es dans un format texte-texte, suggÃ©rant que les entrÃ©es et les sorties sont des textes. Les auteurs affirment que ce â€œ cadre fournit un objectif dâ€™entraÃ®nement cohÃ©rent Ã  la fois pour le prÃ©-entraÃ®nement et le fine-tuningâ€. 			
Le T5 est essentiellement un Transformer encoder-decoder qui applique diverses amÃ©liorations, en particulier aux composantes dâ€™attention qui composent le modÃ¨le. Le modÃ¨le a Ã©tÃ© prÃ©-entraÃ®nÃ© sur un ensemble de donnÃ©es rÃ©cemment publiÃ©, le &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/c4&quot;&gt;Colossal Clean Crawled Corpus&lt;/a&gt; et a Ã©tÃ© appliquÃ© sur SOTA sur des tÃ¢ches de NLP telles que le rÃ©sumÃ©, la rÃ©ponse aux questions et la classification de textes.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*T9MXxcDOd2fX6xblbu7VdQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;&lt;em&gt;(Raffel et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;12 en 1 : Apprentissage multitÃ¢che de la reprÃ©sentation de la vision et des langues&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
La recherche actuelle utilise des tÃ¢ches et des ensembles de donnÃ©es indÃ©pendants pour effectuer des recherches sur la vision et le langage mÃªme lorsque les â€œcompÃ©tences de comprÃ©hension du langage fondÃ©es sur la visionâ€ requises pour effectuer ces tÃ¢ches se chevauchent. Une nouvelle &lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;publication&lt;/a&gt; (qui sera prÃ©sentÃ©e Ã  la CVPR) propose une approche multitÃ¢che Ã  grande Ã©chelle pour mieux modÃ©liser et entraÃ®ner conjointement les tÃ¢ches de vision et du langage afin de gÃ©nÃ©rer un modÃ¨le de vision et de langue plus gÃ©nÃ©rique. Le modÃ¨le rÃ©duit la taille des paramÃ¨tres et fonctionne bien pour des tÃ¢ches telles que la recherche dâ€™images basÃ©e sur des lÃ©gendes et la rÃ©ponse visuelle Ã  des questions.
&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yyvN4bK0K2iykyJ2-QVBjw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;&lt;em&gt;(Lu et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;BERT peut voir Ã  lâ€™extÃ©rieur de la boÃ®te : Sur la transfÃ©rabilitÃ© intermodale des reprÃ©sentations textuelles&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les chercheurs et collaborateurs de reciTAL ont publiÃ© un &lt;a href=&quot;https://arxiv.org/abs/2002.10832&quot;&gt;article&lt;/a&gt; qui vise Ã  rÃ©pondre Ã  la question de savoir si un modÃ¨le BERT peut produire des reprÃ©sentations qui se gÃ©nÃ©ralisent Ã  dâ€™autres modalitÃ©s que le texte, comme par exemple la vision. Ils proposent un modÃ¨le appelÃ© BERT-gen qui exploite des reprÃ©sentations mono ou multimodales et qui obtient de meilleurs rÃ©sultats sur les ensembles de donnÃ©es de gÃ©nÃ©ration de questions visuelles.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2NgR7yBuVLDcEza9UT41dw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.10832&quot;&gt;&lt;em&gt;(Scialom et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;crÃ©ativitÃ©-et-sociÃ©tÃ©-&quot;&gt;CrÃ©ativitÃ© et sociÃ©tÃ© ğŸ¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;La prochaine dÃ©cennie en IA : quatre Ã©tapes vers une intelligence artificielle robuste&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Gary Marcus a rÃ©cemment publiÃ© un &lt;a href=&quot;https://arxiv.org/abs/2002.06177&quot;&gt;article&lt;/a&gt; dans lequel il explique une sÃ©rie de mesures que nous devrions prendre selon lui afin de construire des systÃ¨mes dâ€™IA plus robustes. Lâ€™idÃ©e centrale dans ce papier est de se concentrer sur la construction de systÃ¨mes hybrides et axÃ©s sur la connaissance, guidÃ©s par des modÃ¨les cognitifs, plutÃ´t que de se concentrer sur la construction de systÃ¨mes plus importants qui nÃ©cessitent plus de donnÃ©es et de puissance de calcul.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;10 technologies de pointe pour 2020&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
La revue technologique du MIT a publiÃ© une liste des &lt;a href=&quot;https://www.technologyreview.com/lists/technologies/2020/&quot;&gt;10 percÃ©es&lt;/a&gt; quâ€™elle a identifiÃ©es et qui feront la diffÃ©rence dans la rÃ©solution de problÃ¨mes susceptibles de changer notre faÃ§on de vivre et de travailler. La liste - sans ordre particulier - comprend lâ€™internet non piratable, la mÃ©decine hyper-personnalisÃ©e, lâ€™argent numÃ©rique, les mÃ©dicaments anti-Ã¢ge, les molÃ©cules dÃ©couvertes par lâ€™IA, les mÃ©ga-constellations de satellites, la suprÃ©matie quantique, lâ€™IA minuscule, la confidentialitÃ© diffÃ©rentielle et lâ€™attribution du climat.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Il est temps de repenser le processus de publication en ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Yoshua Bengio a rÃ©cemment fait part de ses &lt;a href=&quot;https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/&quot;&gt;prÃ©occupations&lt;/a&gt; concernant les cycles rapides des publications du ML. La principale est quâ€™en raison de la rapiditÃ© de la publication, beaucoup dâ€™articles publiÃ©s contiennent des erreurs et sont juste incrÃ©mentiels. A contrario, ceux sur lesquels plus de temps est consacrÃ© afin dâ€™en assurer la rigueur, semble disparaÃ®tre. De plus, ce sont les Ã©tudiants qui doivent faire face aux consÃ©quences nÃ©gatives de cette pression et de ce stress. Pour remÃ©dier Ã  cette situation, Bengio parle de ses actions pour aider Ã  ralentir le processus de publication des recherches pour le bien de la science.&lt;/p&gt;

&lt;h1 id=&quot;outils-et-jeux-de-donnÃ©es-ï¸&quot;&gt;Outils et jeux de donnÃ©es âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Mise en Å“uvre du rÃ©seau PointerGenerator dans AllenNLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les rÃ©seaux Â« Pointer-Generator Â» visent Ã  augmenter les modÃ¨les dâ€™attention utilisÃ©s pour amÃ©liorer &lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;la synthÃ¨se abstraite&lt;/a&gt;. Si vous souhaitez utiliser cette technique en utilisant AllenNLP, Kundan Krishna a dÃ©veloppÃ© une &lt;a href=&quot;https://github.com/kukrishna/pointer-generator-pytorch-allennlp&quot;&gt;librairie&lt;/a&gt; qui vous permet dâ€™exÃ©cuter un modÃ¨le prÃ©-entraÃ®nÃ© (fourni) ou dâ€™entraÃ®ner votre propre modÃ¨le.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Questions/rÃ©ponses pour diffÃ©rentes langues&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Avec la prolifÃ©ration des modÃ¨les de Transformer et leur efficacitÃ© pour les tÃ¢ches de NLP, des efforts impressionnants ont Ã©tÃ© dÃ©ployÃ©s pour publier diffÃ©rents types de jeux de donnÃ©es dans diffÃ©rentes langues. Par exemple, Sebastian Ruder a &lt;a href=&quot;https://twitter.com/seb_ruder/status/1231713840502657025?s=20&quot;&gt;partagÃ© une liste&lt;/a&gt; de jeux de donnÃ©es qui peuvent Ãªtre utilisÃ©s pour des tÃ¢ches de rÃ©ponses aux questions dans diffÃ©rentes langues : &lt;a href=&quot;https://www.aclweb.org/anthology/W18-2605/&quot;&gt;DuReader&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1909.07005&quot;&gt;KorQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1912.09723&quot;&gt;SberQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2002.06071&quot;&gt;FQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1906.05394&quot;&gt;Arabic-SQuAD&lt;/a&gt;, &lt;a href=&quot;https://github.com/crux82/squad-it&quot;&gt;SQuAD-it&lt;/a&gt; et &lt;a href=&quot;https://arxiv.org/abs/1912.05200v2&quot;&gt;Spanish SQuAD&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PyTorch Lightning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
PyTorch Lightning est un &lt;a href=&quot;https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09&quot;&gt;outil&lt;/a&gt; qui vous permet de rÃ©aliser un entraÃ®nement abstrait qui nÃ©cessiterait lâ€™utilisation de GPU/TPU dâ€™une prÃ©cision de 16 bits. PyTorch Lightning permet dâ€™entraÃ®ner des modÃ¨les sur des plusieurs GPU et TPU sans avoir besoin de changer votre code PyTorch actuel.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Graph Neural Networks dans TensorFlow 2&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Une Ã©quipe de recherche de Microsoft publie une &lt;a href=&quot;https://github.com/microsoft/tf2-gnn&quot;&gt;librairie&lt;/a&gt; qui donne accÃ¨s aux implÃ©mentations de nombreuses architectures de rÃ©seaux neuronaux en graphes (GNN). Cette librairie est basÃ©e sur TensorFlow 2 et fournit Ã©galement des modules de manipulation de donnÃ©es qui peuvent Ãªtre directement utilisÃ©s dans des boucles dâ€™entrainement/Ã©valuation.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PrÃ©-entraÃ®nement de SmallBERTa - Un petit modÃ¨le pour sâ€™entraÃ®ner sur un petit jeu de donnÃ©es&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Avez-vous dÃ©jÃ  voulu entraÃ®ner votre propre modÃ¨le linguistique Ã  partir de zÃ©ro mais nâ€™avez pas eu assez de ressources pour le faire ? Si câ€™est le cas, Aditya Malte vous propose un &lt;a href=&quot;https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb&quot;&gt;notebook&lt;/a&gt; qui vous apprend Ã  entrainer un modÃ¨le linguistique Ã  partir de zÃ©ro avec un ensemble de donnÃ©es plus restreint.&lt;/p&gt;

&lt;h1 id=&quot;ethique-en-ia-&quot;&gt;Ethique en IA ğŸš¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Pourquoi les visages ne disent pas toujours la vÃ©ritÃ© sur les sentiments&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Depuis un certain temps, de nombreux chercheurs et entreprises ont tentÃ© de construire des modÃ¨les dâ€™IA qui comprennent et peuvent reconnaÃ®tre les Ã©motions dans un contexte textuel ou visuel. Un nouvel &lt;a href=&quot;https://www.nature.com/articles/d41586-020-00507-5&quot;&gt;article&lt;/a&gt; relance le dÃ©bat sur le fait que les techniques dâ€™IA qui visent Ã  reconnaÃ®tre les Ã©motions Ã  partir des images de visages ne le font pas correctement. Lâ€™argument principal, soulevÃ© par des psychologues, est quâ€™il nâ€™existe aucune preuve dâ€™expressions universelles pouvant Ãªtre utilisÃ©es pour la dÃ©tection dâ€™Ã©motions basÃ©es uniquement sur des images de visages. Il faudrait quâ€™un modÃ¨le comprenne mieux par exemple les traits de personnalitÃ© ou encore les mouvements du corps, afin de se rapprocher rÃ©ellement dâ€™une dÃ©tection plus prÃ©cise des Ã©motions affichÃ©es par les humains.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Differential Privacy and Federated Learning Explained&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Lâ€™une des considÃ©rations Ã©thiques Ã  prendre en compte lors de la construction de systÃ¨mes dâ€™IA est la garantie du respect de la vie privÃ©e. Actuellement, cela peut Ãªtre rÃ©alisÃ© de deux maniÃ¨res, soit en utilisant une intimitÃ© diffÃ©rentielle, soit par un apprentissage fÃ©dÃ©rÃ©. Si vous voulez en savoir plus sur ces sujets, Jordan Harrod nous fournit une excellente introduction dans cette &lt;a href=&quot;https://www.youtube.com/watch?v=MOcTGM_UteM&quot;&gt;vidÃ©o&lt;/a&gt; qui comprend Ã©galement une session de pratique avec lâ€™utilisation dâ€™un notebook.&lt;/p&gt;

&lt;h1 id=&quot;articles-et-blog-ï¸&quot;&gt;Articles et Blog âœï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;PlongÃ©e dans le Reformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Madison May a Ã©crit un nouvel &lt;a href=&quot;https://www.pragmatic.ml/reformer-deep-dive/&quot;&gt;article sur son blog&lt;/a&gt; consacrÃ© au &lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;Reformer&lt;/a&gt;, proposÃ© par Google AI. Nous avons Ã©galement prÃ©sentÃ© le Reformer dans un &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-reformer-deepmath-electra-tinybert-for-search-vizseq-open-sourcing-ml-68d5b6eed057&quot;&gt;prÃ©cÃ©dent numÃ©ro&lt;/a&gt; de la newsletter. Pour une explication en franÃ§ais, vous pouvez consulter lâ€™illustration &lt;a href=&quot;https://lbourdois.github.io/blog/nlp/Reformer/&quot;&gt;suivante&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Une plateforme de blogging gratuite&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://fastpages.fast.ai/fastpages/jupyter/2020/02/21/introducing-fastpages.html&quot;&gt;fastpages&lt;/a&gt; vous permet de crÃ©er automatiquement et gratuitement un blog en utilisant les pages GitHub. Cette solution simplifie le processus de publication dâ€™un blog et prend Ã©galement en charge lâ€™utilisation de documents Word et de notebook Jupyter.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Conseils pour un entretien chez Google&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pablo Castro, de lâ€™Ã©quipe Google Brain, a publiÃ© un &lt;a href=&quot;https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html&quot;&gt;article sur son blog&lt;/a&gt; mettant en avant une liste de conseils pour les personnes intÃ©ressÃ©es par un entretien dâ€™embauche chez Google. Parmi les sujets abordÃ©s figurent des conseils sur la faÃ§on de se prÃ©parer Ã  lâ€™entretien, sur ce Ã  quoi il faut sâ€™attendre pendant lâ€™entretien et sur ce qui se passe aprÃ¨s lâ€™entretien.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Les Transformers sont des Graph Neural Networks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les rÃ©seaux neuronaux de graphes (GNN) et les Transformers se sont avÃ©rÃ©s efficaces pour diffÃ©rentes tÃ¢ches de NLP. Pour mieux comprendre le fonctionnement interne de ces approches et leurs relations, Chaitanya Joshi a Ã©crit un &lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;article&lt;/a&gt; expliquant la connexion entre les GNN et les Transformers, ainsi que les diffÃ©rentes faÃ§ons dont ces mÃ©thodes peuvent Ãªtre combinÃ©es dans une sorte de modÃ¨le hybride.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;CNNs et Equivariance&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Fabian Fuchs et Ed Wagstaff &lt;a href=&quot;https://fabianfuchsml.github.io/equivariance1of2/&quot;&gt;discutent&lt;/a&gt; de lâ€™importance de lâ€™Ã©quivariance et de la maniÃ¨re dont les CNN la font respecter. Le concept dâ€™Ã©quivariance est dâ€™abord dÃ©fini, puis discutÃ© dans le contexte de CNN appliquÃ©s Ã  la traduction.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Lâ€™auto-apprentissage avec les images&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Lâ€™auto-apprentissage a Ã©tÃ© beaucoup Ã©voquÃ© dans les prÃ©cÃ©dents numÃ©ros de la newsletter en raison du rÃ´le quâ€™il a jouÃ© dans les techniques modernes de modÃ©lisation des langues. Ce &lt;a href=&quot;https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%e7%bd%91/&quot;&gt;billet&lt;/a&gt; de Jonathan Whitaker fournit une explication de lâ€™auto-apprentissage dans le contexte des images. Si le sujet vous intÃ©resse, Amit Chaudhary a Ã©galement Ã©crit un &lt;a href=&quot;https://amitness.com/2020/02/illustrated-self-supervised-learning/&quot;&gt;article&lt;/a&gt; dÃ©crivant le concept de maniÃ¨re visuelle.&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education ğŸ“&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Stanford CS330: Deep Multi-Task et Meta-Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Stanford a rÃ©cemment publiÃ© une &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&quot;&gt;playlist vidÃ©o YouTube&lt;/a&gt; sur son nouveau cours consacrÃ© Ã  lâ€™apprentissage multi-tÃ¢ches et au mÃ©ta-apprentissage. Parmi les sujets abordÃ©s, citons le mÃ©ta-apprentissage bayÃ©sien, le lifelong apprentissage, une introduction Ã  lâ€™apprentissage par renforcement, etcâ€¦&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Notebooks PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
dair.ai publie une &lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;sÃ©rie de notebook&lt;/a&gt; qui visent Ã  vous faire dÃ©couvrir les rÃ©seaux neuronaux profonds Ã  lâ€™aide de PyTorch. Il sâ€™agit dâ€™un travail en cours et certains sujets dâ€™actualitÃ© comprennent la faÃ§on de mettre en Å“uvre un modÃ¨le de rÃ©gression logistique Ã  partir de zÃ©ro et la faÃ§on de programmer un NN ou un RNN Ã  partir de zÃ©ro. Les notebooks sont Ã©galement disponibles dans le dÃ©pÃ´t GitHub.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;**Le livre de fastai book (Ã©bauche) **&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jeremy Howard et Sylvain Gugger ont publiÃ© une &lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;liste complÃ¨te&lt;/a&gt; de notebooks (non terminÃ©s) en vue de leur prochain cours qui prÃ©sentera des concepts de deep learning et diffÃ©rentes mÃ©thodes dâ€™utilisation de PyTorch et la librairie fastai.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Cours gratuits sur la datascience&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Au cas oÃ¹ vous lâ€™auriez manquÃ©, Kaggle propose une sÃ©rie de &lt;a href=&quot;https://www.kaggle.com/learn/overview&quot;&gt;petits cours gratuits&lt;/a&gt; sur les outils de datascience. Certains de ces cours comprennent, entre autres, lâ€™explication de lâ€™apprentissage machine, une introduction Ã  lâ€™apprentissage machine et Ã  Python, la visualisation de donnÃ©es, lâ€™ingÃ©nierie des fonctionnalitÃ©s et lâ€™apprentissage approfondi.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Un autre &lt;a href=&quot;https://lewtun.github.io/dslectures/&quot;&gt;cours de datascience en ligne&lt;/a&gt; propose un programme, des diapositives et des notebooks sur lâ€™analyse exploratoire des donnÃ©es, lâ€™interprÃ©tation des modÃ¨les ou encore le NLP.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;8 crÃ©ateurs et contributeurs parlent de leurs librairies PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
nepture.ai a publiÃ© un vaste &lt;a href=&quot;https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&amp;amp;utm_medium=tweet&amp;amp;utm_campaign=blog-model-training-libraries-pytorch-ecosystem&quot;&gt;article&lt;/a&gt; qui contient des discussions dÃ©taillÃ©es avec les principaux crÃ©ateurs et contributeurs de PyTorch (parcours, philosophie du projet, outils qui lâ€™entourent, etcâ€¦).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Visualisation des modÃ¨les adaptatifs dâ€™attention rÃ©duite&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sasha Rush partage un &lt;a href=&quot;https://colab.research.google.com/drive/1EB7MI_3gzAR1gFwPPO27YU9uYzE_odSu&quot;&gt;notebook&lt;/a&gt; qui explique et montre les dÃ©tails techniques sur la maniÃ¨re de produire des sorties softmax sparses. Il aborde Ã©galement la faÃ§on dâ€™induire la sparcitÃ© dans la composante dâ€™attention dâ€™un modÃ¨le de Transformer, ce qui aide Ã  produire une probabilitÃ© nulle pour les mots non pertinents dans un contexte donnÃ©, amÃ©liorant ainsi la performance et lâ€™interprÃ©tabilitÃ©.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*7BB322LlVgt1zzk-cviSoA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;mentions-spÃ©ciales-ï¸&quot;&gt;Mentions spÃ©ciales â­ï¸&lt;/h1&gt;

&lt;p&gt;Conor Bell a codÃ© un &lt;a href=&quot;https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4&quot;&gt;script python&lt;/a&gt; qui vous permet de visualiser et de prÃ©parer facilement un jeu de donnÃ©es pouvant Ãªtre utilisÃ© pour un modÃ¨le StyleGAN.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manu Romero &lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos&quot;&gt;a contribuÃ©&lt;/a&gt; au fine-tuning dâ€™un modÃ¨le POS pour lâ€™espagnol. Le modÃ¨le est sur la librairie Transfomers dâ€™Hugging Face. Il sera intÃ©ressant de voir cet effort dans dâ€™autres langues.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ce &lt;a href=&quot;https://github.com/tomohideshibata/BERT-related-papers&quot;&gt;rÃ©pertoire Github&lt;/a&gt; contient une longue liste de documents rÃ©digÃ©s sur BERT qui abordent diffÃ©rents problÃ¨mes tels que la compression de modÃ¨les, les domaines spÃ©cifiques, le multi-modÃ¨le, la gÃ©nÃ©ration, les tÃ¢ches en aval, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Connor Shorten a publiÃ© une courte &lt;a href=&quot;https://www.youtube.com/watch?time_continue=79&amp;amp;v=-Bh_7tzyoR4&amp;amp;feature=emb_logo&quot;&gt;vidÃ©o&lt;/a&gt; de 15 minutes expliquant un framework Ã  aborder pour rÃ©duire lâ€™effet des â€œraccourcisâ€ dans lâ€™auto-apprentissage de la reprÃ©sentation. Câ€™est important car, sâ€™il nâ€™est pas bien fait, le modÃ¨le peut ne pas apprendre des reprÃ©sentations sÃ©mantiques utiles et se rÃ©vÃ©ler potentiellement inefficace dans un contexte dâ€™apprentissage par transfert.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Ruder a publiÃ© un nouveau numÃ©ro de sa newsletter qui met en lumiÃ¨re des sujets et des ressources allant dâ€™analyses dâ€™articles sur le NLP et le ML de 2019 Ã  des diapositives sur lâ€™apprentissage par transfert et des Ã©lÃ©ments essentiels sur le deep learning. Vous pouvez le consulter &lt;a href=&quot;http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Vous pouvez retrouver la prÃ©cÃ©dente newsletter &lt;a href=&quot;https://dair.ai/NLP_Newsletter_-5_-FR/&quot;&gt;ici&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous avez des jeux de donnÃ©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine Ã©dition de la newletter, nâ€™hÃ©sitez pas Ã  me contacter Ã  ellfae@gmail.com ou par message sur &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Abonnez-vous&lt;/a&gt; pour recevoir les prochains numÃ©ros dans votre boÃ®te mail.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_-6_-FR/&quot;&gt;NLP Newsletter [FR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 09, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [FR] #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,â€¦]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_-5_-FR/" />
  <id>https://dair.ai/NLP_Newsletter_#5_[FR]</id>
  <published>2020-03-09T00:00:00-05:00</published>
  <updated>2020-03-09T00:00:00-05:00</updated>
  <author>
    <name>LoÃ¯ck BOURDOIS</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;avant-propos&quot;&gt;Avant-propos&lt;/h1&gt;
&lt;p&gt;Tout dâ€™abord, je ne saurais trop tous vous remercier pour le soutien et les encouragements incroyables que vous avez apportÃ©s Ã  cette newsletter. Son Ã©laboration nÃ©cessite des recherches et une rÃ©daction fastidieuse que je trouve Ã  la fois enrichissantes et utiles, afin de vous fournir le meilleur contenu. Jâ€™espÃ¨re que vous les apprÃ©ciez, car câ€™est le cas pour moi. ğŸ˜‰&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications ğŸ“™&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Une comprÃ©hension thÃ©orique de lâ€™autodistillation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Lâ€™&lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;&gt;autodistillation&lt;/a&gt; est le processus de transfert de connaissances dâ€™une architecture Ã  une seconde qui est identique. Les prÃ©dictions du modÃ¨le original sont transmises comme valeurs cibles au second modÃ¨le pendant la phase dâ€™entraÃ®nement. Outre les propriÃ©tÃ©s souhaitables, comme la rÃ©duction de la taille du modÃ¨le, les rÃ©sultats empiriques montrent que cette approche fonctionne bien sur des ensembles de donnÃ©es maintenus. Un groupe de chercheurs a rÃ©cemment publiÃ© un article qui fournit une analyse thÃ©orique visant Ã  mieux comprendre ce qui se passe dans ce processus et pourquoi il est efficace. Les rÃ©sultats montrent que quelques cycles dâ€™autodistillation amplifient la rÃ©gularisation (&lt;a href=&quot;https://twitter.com/TheGradient/status/1228132843630387201?s=20&quot;&gt;en limitant progressivement le nombre de fonctions de base qui reprÃ©sentent la solution&lt;/a&gt;), ce qui tend Ã  rÃ©duire le sur-apprentissage. (Lire lâ€™article complet &lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;ici&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Les annÃ©es 2010 : Une dÃ©cennie dâ€™apprentissage approfondi / Perspectives pour les annÃ©es 2020&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;http://people.idsia.ch/~juergen/&quot;&gt;JÃ¼rgen Schmidhuber&lt;/a&gt;, un pionnier de lâ€™intelligence artificielle, a rÃ©cemment publiÃ© un &lt;a href=&quot;http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html&quot;&gt;article sur son blog&lt;/a&gt; qui se concentre sur un aperÃ§u historique de lâ€™apprentissage profond depuis 2010. Parmi les sujets abordÃ©s, citons les LSTM, les feedforward NN, les GAN, lâ€™apprentissage par renforcement, le mÃ©ta-apprentissage, la distillation, lâ€™apprentissage par lâ€™attention, etc. Lâ€™article se termine par une perspective sur les annÃ©es 2020, encourageant lâ€™attention sur des questions urgentes telles que la vie privÃ©e et les marchÃ©s des donnÃ©es.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Utilisation des rÃ©seaux de neurones pour rÃ©soudre des Ã©quations mathÃ©matiques&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les chercheurs du FAIR de Facebook ont publiÃ© un &lt;a href=&quot;https://arxiv.org/abs/1912.01412&quot;&gt;article&lt;/a&gt; qui propose un modÃ¨le entraÃ®nÃ© sur des problÃ¨mes mathÃ©matiques ainsi que leurs solutions associÃ©es, afin dâ€™apprendre Ã  prÃ©dire les solutions possibles pour des tÃ¢ches telles que la rÃ©solution de problÃ¨mes dâ€™intÃ©gration. Lâ€™approche est basÃ©e sur une approche similaire Ã  celle utilisÃ©e dans la traduction automatique oÃ¹ les expressions mathÃ©matiques sont reprÃ©sentÃ©es comme une sorte de langage et les solutions sont traitÃ©es comme le problÃ¨me de traduction. Ainsi, au lieu que le modÃ¨le produise une traduction, le rÃ©sultat est la solution elle-mÃªme. Les chercheurs affirment ainsi que les rÃ©seaux neuronaux profonds ne sont pas seulement bons pour le raisonnement symbolique, mais aussi pour des tÃ¢ches plus diverses.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ã‰quations fournies en entrÃ©e avec la solution correspondante fournie par le modÃ¨le â€“&lt;/em&gt; &lt;a href=&quot;https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;crÃ©ativitÃ©-et-sociÃ©tÃ©-&quot;&gt;CrÃ©ativitÃ© et sociÃ©tÃ© ğŸ¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Lâ€™IA au service de la dÃ©couverte scientifique&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Mattew Hutson &lt;a href=&quot;https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times&quot;&gt;rapporte&lt;/a&gt; comment lâ€™IA peut Ãªtre utilisÃ©e pour produire des Ã©mulateurs qui ont une utilitÃ© importante dans la modÃ©lisation de phÃ©nomÃ¨nes naturels complexes qui pourraient, Ã  leur tour, conduire Ã  diffÃ©rents types de dÃ©couvertes scientifiques. Le dÃ©fi avec la construction de ces Ã©mulateurs est quâ€™ils nÃ©cessitent souvent dâ€™importantes donnÃ©es et une exploration approfondie des paramÃ¨tres. Un &lt;a href=&quot;https://arxiv.org/abs/2001.08055&quot;&gt;article&lt;/a&gt; rÃ©cent propose DENSE, une approche basÃ©e sur la &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_architecture_search&quot;&gt;recherche dâ€™architecture neurale&lt;/a&gt; pour construire des Ã©mulateurs prÃ©cis tout en ne sâ€™appuyant que sur une quantitÃ© limitÃ©e de donnÃ©es dâ€™entraÃ®nement. Ils lâ€™ont testÃ©e en effectuant des simulations pour des cas tels que lâ€™astrophysique, la climatologie et la fusion, entre autres.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;AmÃ©liorer la Â« traduction Â» de lâ€™image Ã  lâ€™illustration&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
GANILLA est une approche qui propose lâ€™utilisation de GAN pour amÃ©liorer le transfert Ã  la fois du style et du contenu dans la &lt;a href=&quot;https://paperswithcode.com/task/image-to-image-translation&quot;&gt;tÃ¢che de traduction dâ€™image Ã  image&lt;/a&gt; non appariÃ©e. En particulier, un modÃ¨le dâ€™illustration dâ€™image Ã  image est proposÃ© (avec un rÃ©seau gÃ©nÃ©rateur amÃ©liorÃ©) et Ã©valuÃ© sur la base dâ€™un nouveau cadre dâ€™Ã©valuation quantitative qui prend en compte Ã  la fois le contenu et le style. La nouveautÃ© de ce travail rÃ©side dans le rÃ©seau gÃ©nÃ©rateur proposÃ© qui tient compte dâ€™un Ã©quilibre entre le style et le contenu, ce que les modÃ¨les prÃ©cÃ©dents nâ€™ont pas rÃ©ussi Ã  atteindre. Des codes et modÃ¨les prÃ©-entrainÃ©s sont mis Ã  &lt;a href=&quot;https://github.com/giddyyupp/ganilla&quot;&gt;disposition&lt;/a&gt;. Lisez le document complet &lt;a href=&quot;https://arxiv.org/abs/2002.05638&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Andrew Ng parle de lâ€™intÃ©rÃªt pour lâ€™auto-apprentissage&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew Ng, le fondateur de deeplearning.ai, est intervenu dans un podcast sur lâ€™intelligence artificielle pour &lt;a href=&quot;https://www.youtube.com/watch?v=0jspaMLxBig&quot;&gt;parler&lt;/a&gt; de sujets tels que ses dÃ©buts dans le ML, lâ€™avenir de lâ€™IA, lâ€™enseignement de lâ€™IA, ses recommandations pour une bonne utilisation du ML, ses objectifs personnels et les techniques de ML auxquelles il faudra prÃªter attention dans les annÃ©es 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew a expliquÃ© pourquoi il est trÃ¨s enthousiaste Ã  lâ€™idÃ©e de sâ€™initier Ã  lâ€™auto-apprentissage. &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html&quot;&gt;Lâ€™auto-apprentissage supervisÃ©&lt;/a&gt; consiste Ã  formuler un problÃ¨me dâ€™apprentissage dont le but est dâ€™obtenir une supervision Ã  partir des donnÃ©es elles-mÃªmes. Lâ€™intÃ©rÃªt est dâ€™utiliser de grandes quantitÃ©s de donnÃ©es non labÃ©lisÃ©es, ce qui est plus disponibles en plus grande quantitÃ© que les donnÃ©es labÃ©lisÃ©es. Les reprÃ©sentations, par opposition Ã  lâ€™exÃ©cution de la tÃ¢che, sont importantes et peuvent Ãªtre utilisÃ©es pour traiter des tÃ¢ches en aval, comme câ€™est le cas dans les modÃ¨les linguistiques tels que le &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert&quot;&gt;BERT&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Il y a Ã©galement beaucoup dâ€™intÃ©rÃªt Ã  utiliser lâ€™auto-apprentissage supervisÃ©](pour apprendre des reprÃ©sentations visuelles gÃ©nÃ©ralisÃ©es qui rendent les modÃ¨les plus prÃ©cis dans des environnements Ã  faibles ressources. Par exemple, une mÃ©thode rÃ©cente appelÃ©e &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;SimCLR&lt;/a&gt; (dirigÃ©e par Geoffrey Hinton) propose un cadre pour amÃ©liorer les rÃ©sultats de la classification des images dans diffÃ©rents contextes tels que lâ€™apprentissage par transfert et lâ€™apprentissage semi-supervisÃ©.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;outils-et-jeux-de-donnÃ©es-ï¸&quot;&gt;Outils et jeux de donnÃ©es âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Les libraries liÃ©es Ã  JAX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/google/jax&quot;&gt;JAX&lt;/a&gt; est une nouvelle bibliothÃ¨que qui combine NumPy et la diffÃ©renciation automatique pour mener des recherches de ML de haut niveau. Afin de simplifier les pipelines utilisant JAX, DeepMind a publiÃ© &lt;a href=&quot;https://github.com/deepmind/dm-haiku&quot;&gt;Haiku&lt;/a&gt; et &lt;a href=&quot;https://github.com/deepmind/rlax&quot;&gt;RLax&lt;/a&gt;. RLax simplifie lâ€™implÃ©mentation de modÃ¨les basÃ©s sur lâ€™apprentissage par renforcement et Haiku simplifie la construction de rÃ©seaux neuronaux en utilisant des modÃ¨les de programmation orientÃ©s objet.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Un outil de traitement des donnÃ©es WikipÃ©dia&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/epfl-lts2/sparkwiki&quot;&gt;Sparkwiki&lt;/a&gt; est un outil permettant de traiter les donnÃ©es de WikipÃ©dia. Cette version sâ€™inscrit dans le cadre de nombreux efforts visant Ã  permettre des recherches intÃ©ressantes en matiÃ¨re dâ€™analyse comportementale, telles que la &lt;a href=&quot;https://arxiv.org/abs/2002.06885&quot;&gt;capture des tendances et des biais linguistiques dans les diffÃ©rentes Ã©ditions linguistiques de WikipÃ©dia&lt;/a&gt;. Les auteurs ont dÃ©couvert quâ€™indÃ©pendamment de la langue, le comportement de navigation des utilisateurs de WikipÃ©dia montre quâ€™ils ont tendance Ã  partager des intÃ©rÃªts communs comme par exemples les films, la musique et le sport, mais que les diffÃ©rences deviennent plus apparentes avec les Ã©vÃ©nements locaux et les particularitÃ©s culturelles.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Mise Ã  jour de la librairie Transformers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Une &lt;a href=&quot;https://github.com/huggingface/transformers/releases/tag/v2.5.0&quot;&gt;nouvelle version&lt;/a&gt; de la librairie Transformers dâ€™Hugging Face est disponible. Elle comprend lâ€™intÃ©gration de leur librairie Tokenizer qui vise Ã  accÃ©lÃ©rer des modÃ¨les comme BERT, RoBERTa, GPT2, et dâ€™autres modÃ¨les construits par la communautÃ©.&lt;/p&gt;

&lt;h1 id=&quot;ethique-en-ia-&quot;&gt;Ethique en IA ğŸš¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;ConsidÃ©rations Ã©thiques pour les modÃ¨les de NLP et de ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans un nouvel &lt;a href=&quot;https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender&quot;&gt;Ã©pisode&lt;/a&gt; des &lt;a href=&quot;https://soundcloud.com/nlp-highlights&quot;&gt;NLP Highlights&lt;/a&gt;, Emily Bender et les intervenants parlent de certaines considÃ©rations Ã©thiques qui peuvent se poser lors du dÃ©veloppement de modÃ¨les de NLP dans un contexte dâ€™utilisation universitaire et grand public. Parmi les sujets abordÃ©s, citons les considÃ©rations Ã©thiques lors de la conception des tÃ¢ches de NLP, les approches de collecte de donnÃ©es et, finalement, la publication des rÃ©sultats.
En plus de toutes les considÃ©rations ci-dessus, une prÃ©occupation qui est toujours discutÃ©e dans la communautÃ© de lâ€™IA est de se concentrer trop sur lâ€™optimisation dâ€™une mesure, ce qui va Ã  lâ€™encontre des fondements de ce que lâ€™IA vise Ã  atteindre (cÃ d une IA gÃ©nÃ©rale). Rachel Thomas et David Uminsky discutent des erreurs possibles en &lt;a href=&quot;https://arxiv.org/abs/2002.08512&quot;&gt;analysant de maniÃ¨re approfondie&lt;/a&gt; diffÃ©rents cas dâ€™utilisation. Ils proposent Ã©galement un cadre simple pour attÃ©nuer le problÃ¨me, qui implique lâ€™utilisation et la combinaison de plusieurs mesures, suivies par lâ€™implication des personnes directement concernÃ©es par la technologie.&lt;/p&gt;

&lt;h1 id=&quot;articles-et-blog-ï¸&quot;&gt;Articles et Blog âœï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Le GPT2 annotÃ©&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aman Arora a rÃ©cemment publiÃ© un article sur son blog, intitulÃ© le â€œ&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;The Annotated GPT-2&lt;/a&gt;â€, qui explique le fonctionnement interne du GPT-2. Son approche sâ€™inspire de â€œ&lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;The Annotated Transformer&lt;/a&gt;â€ qui a adoptÃ© une approche dâ€™annotation pour expliquer les parties importantes du modÃ¨le par le biais de code et dâ€™explications faciles Ã  suivre. Aman a fait de gros efforts pour rÃ©implÃ©menter le GPT-2 dâ€™OpenAI en utilisant PyTorch et la bibliothÃ¨que Transformers de Hugging Face.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Au-delÃ  de BERT ?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sergi Castella expose son &lt;a href=&quot;https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1&quot;&gt;point de vue&lt;/a&gt; sur ce qui se trouve au-delÃ  de BERT. Les principaux sujets abordÃ©s sont lâ€™amÃ©lioration des mesures, la faÃ§on dont la librairie Transformers dâ€™HuggingfFace permet de faire des recherches, les jeux de donnÃ©es intÃ©ressants Ã  consulter, etcâ€¦&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;OpÃ©rateur de compression matricielle&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
TensorFlow blog a publiÃ© un &lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;article&lt;/a&gt; expliquant les techniques et lâ€™importance de la compression des matrices dans un modÃ¨le de rÃ©seau neuronal profond. La compression des matrices peut aider Ã  construire des modÃ¨les petits plus efficaces qui peuvent Ãªtre incorporÃ©s dans des appareils tels que les tÃ©lÃ©phones et les assistants vocaux. En se concentrant sur la compression des modÃ¨les par des mÃ©thodes telles que la low-rank-approximation et la quantization, nous nâ€™avons pas besoin de compromettre la qualitÃ© du modÃ¨le.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education ğŸ“&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Les bases du NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Elvis a publiÃ© une Ã©bauche du chapitre 1 de sa nouvelle sÃ©rie intitulÃ©e â€œ&lt;a href=&quot;https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684&quot;&gt;Les bases du NLP&lt;/a&gt;â€. Il enseigne les concepts du NLP en partant des bases tout en partageant les meilleures pratiques, les rÃ©fÃ©rences importantes, et les erreurs courantes Ã  Ã©viter. Un &lt;a href=&quot;https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ&quot;&gt;Google Colab&lt;/a&gt; est disponible et le projet sera maintenu &lt;a href=&quot;https://github.com/dair-ai/nlp_fundamentals&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;[Online] Review/Discussion: Part I Mathematical Foundations Reading Session&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Machine Learning Tokyo organise une discussion en ligne sur les chapitres qui ont Ã©tÃ© couverts lors de leurs rÃ©centes sessions dâ€™Ã©tude en ligne. Le groupe avait auparavant Ã©tudiÃ© des chapitres basÃ©s sur le livre intitulÃ© &lt;a href=&quot;https://mml-book.github.io/&quot;&gt;Mathematics For Machine Learning&lt;/a&gt;, Ã©crit par Marc Peter Deisenroth, A Aldo Faisal et Cheng Soon Ong. Lâ€™Ã©vÃ©nement est prÃ©vu pour le 8 mars 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Recommandations de livres&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans une partie prÃ©cÃ©dente, nous avons discutÃ© de lâ€™importance de la compression matricielle pour la construction de petits modÃ¨les de ML. Si vous souhaitez en savoir plus sur la faÃ§on de construire des rÃ©seaux neuronaux profonds plus petits pour les systÃ¨mes embarquÃ©s, consultez cet excellent livre intitulÃ© &lt;a href=&quot;https://tinymlbook.com/?linkId=82595412&quot;&gt;TinyML&lt;/a&gt;, Ã©crit par Pete Warden et Daniel Situnayake.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Un autre livre intÃ©ressant Ã  surveiller et qui est Ã  paraÃ®tre est â€œ&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;Deep Learning for Coders with fastai and PyTorchâ€ : AI Applications Without a PhD&lt;/a&gt;â€ de Jeremy Howard et Sylvain Gugger. Ce livre vise Ã  fournir les bases mathÃ©matiques nÃ©cessaires pour construire et former des modÃ¨les permettant dâ€™aborder des tÃ¢ches dans les domaines de computer vision et du NLP.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;mentions-spÃ©ciales-ï¸&quot;&gt;Mentions spÃ©ciales â­ï¸&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.06576&quot;&gt;Torchmeta&lt;/a&gt; est une librairie qui permet dâ€™utiliser facilement des chargeurs de donnÃ©es connexes pour la recherche sur le mÃ©ta-apprentissage. Elle a Ã©tÃ© rÃ©digÃ©e par Tristan Deleu.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manuel Tonneau a Ã©crit un &lt;a href=&quot;https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter&quot;&gt;article&lt;/a&gt; offrant un regard plus approfondi sur certains des mÃ©canismes impliquÃ©s dans la modÃ©lisation du langage. Parmi les sujets abordÃ©s, citons la greedy recherche, la beam recherche et lâ€™Ã©chantillonnage de noyaux.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le MIT &lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;publie&lt;/a&gt; le programme complet et le calendrier du cours intitulÃ© â€œIntroduction to Deep Learningâ€. Lâ€™objectif est de publier chaque semaine des vidÃ©os et des diapositives.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Apprenez comment entraÃ®ner un modÃ¨le de reconnaissance dâ€™entitÃ©s nommÃ©es (NER) en utilisant une approche basÃ©e sur &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py&quot;&gt;Transformers&lt;/a&gt; en moins de 300 lignes de code. Vous pouvez trouver le programme Google Colab qui lâ€™accompagne &lt;a href=&quot;https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Vous pouvez retrouver la prÃ©cÃ©dente newsletter &lt;a href=&quot;https://dair.ai/NLP_Newsletter_-4_-FR/&quot;&gt;ici&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous avez des jeux de donnÃ©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine Ã©dition de la newletter, nâ€™hÃ©sitez pas Ã  me contacter Ã  ellfae@gmail.com ou par message sur &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Abonnez-vous&lt;/a&gt; pour recevoir les prochains numÃ©ros dans votre boÃ®te mail.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_-5_-FR/&quot;&gt;NLP Newsletter [FR] #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,â€¦&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 09, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [FR] #4: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,â€¦]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_-4_-FR/" />
  <id>https://dair.ai/NLP_Newsletter_#4_[FR]</id>
  <published>2020-03-09T00:00:00-05:00</published>
  <updated>2020-03-09T00:00:00-05:00</updated>
  <author>
    <name>LoÃ¯ck BOURDOIS</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*3vNKhz6K-oGQ8aLi3mo84Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications ğŸ“™&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Turing-NLG: Un modÃ¨le linguistique de 17 milliards de paramÃ¨tres par Microsoft&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Turing Natural Language Generation (T-NLG) est un modÃ¨le de 17 milliards de paramÃ¨tres &lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/&quot;&gt;proposÃ©&lt;/a&gt; par les chercheurs en IA de Microsoft. Il est Ã  ce jour, le plus grand modÃ¨le de langage connu (illustrÃ© dans la figure ci-dessous) et est basÃ© sur un Transformer Ã  78 couches qui surpasse les rÃ©sultats prÃ©cÃ©dents (dÃ©tenus par Megatron-LM de NVIDIA) sur la perplexitÃ© de WikiText-103. Il a Ã©tÃ© testÃ© sur des tÃ¢ches telles que la rÃ©ponse Ã  des questions et le rÃ©sumÃ© abstrait. Le modÃ¨le est rendu possible par une librairie dâ€™optimisation de lâ€™entraÃ®nement appelÃ©e DeepSpeed avec ZeRO, qui est Ã©galement prÃ©sentÃ©e plus loin dans cette newsletter.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*CAZm7uj8EaupnvnJ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Neural based Dependency Parsing&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Miryam de Lhoneux a publiÃ© sa thÃ¨se de doctorat intitulÃ©e â€œ&lt;a href=&quot;http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1357373&amp;amp;dswid=7905&quot;&gt;Linguistically Informed Neural Dependency Parsing for Typologically Diverse Languages&lt;/a&gt;â€. Ce travail porte sur lâ€™utilisation dâ€™approches neurales pour lâ€™&lt;a href=&quot;http://nlpprogress.com/english/dependency_parsing.html&quot;&gt;analyse des dÃ©pendances&lt;/a&gt; dans les langues typologiquement diverses (câ€™est-Ã -dire les langues qui construisent et expriment le sens de maniÃ¨re structurellement diffÃ©rente). Ce travail rapporte que les RNN et les couches rÃ©cursives pourraient Ãªtre utiles pour lâ€™incorporation dans les parsers car elles aident Ã  informer les modÃ¨les avec des connaissances linguistiques importantes nÃ©cessaires pour lâ€™analyse. Dâ€™autres idÃ©es comprennent lâ€™utilisation de lâ€™analyse syntaxique polyglotte et des stratÃ©gies de partage de paramÃ¨tres pour lâ€™analyse syntaxique dans des langues apparentÃ©es et non apparentÃ©es.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Extraction dâ€™informations de bout en bout dans le cloud avec BERT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Une Ã©quipe de chercheurs a publiÃ© un &lt;a href=&quot;https://arxiv.org/abs/2002.01861&quot;&gt;article&lt;/a&gt; dÃ©crivant comment des modÃ¨les de Transformers comme BERT peuvent aider Ã  lâ€™extraction dâ€™informations de bout en bout dans des documents commerciaux spÃ©cifiques Ã  un domaine, tels que les dÃ©pÃ´ts rÃ©glementaires et les contrats de location de propriÃ©tÃ©. Non seulement ce type de travail peut aider Ã  optimiser les opÃ©rations commerciales, mais il montre Ã©galement lâ€™applicabilitÃ© et lâ€™efficacitÃ© des modÃ¨les basÃ©s sur BERT sur des rÃ©gimes avec trÃ¨s peu de donnÃ©es annotÃ©es. Une application, et ses dÃ©tails de mise en Å“uvre, qui fonctionne sur le cloud est Ã©galement proposÃ©e (voir figure ci-dessous).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*KqViSLhP0otleDY-XFy3Bg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.01861&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Question Answering Benchmark&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/2001.11770v1&quot;&gt;Wolfson et al. (2020)&lt;/a&gt; ont publiÃ© un benchmark pour la comprÃ©hension des questions ainsi quâ€™une mÃ©thode pour dÃ©composer une question qui est nÃ©cessaire pour calculer une rÃ©ponse appropriÃ©e. Ils sâ€™appuient sur le crowdsourcing pour annoter les Ã©tapes nÃ©cessaires Ã  la dÃ©composition des questions. Pour montrer la faisabilitÃ© et lâ€™applicabilitÃ© de lâ€™approche, ils amÃ©liorent la rÃ©ponse aux questions du domaine ouvert en utilisant lâ€™ensemble de donnÃ©es HotPotQA.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*AztG-Inqt6LGQ87lSufRcw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;â€Š&lt;a href=&quot;https://arxiv.org/pdf/2001.11770v1.pdf&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;DonnÃ©es radioactives : le traÃ§age par lâ€™entraÃ®nement&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les chercheurs de Facebook AI ont rÃ©cemment publiÃ© un &lt;a href=&quot;https://ai.facebook.com/blog/using-radioactive-data-to-detect-if-a-data-set-was-used-for-training/&quot;&gt;travail&lt;/a&gt; qui vise Ã  marquer les images (appelÃ©es donnÃ©es radioactives) afin de vÃ©rifier si un jeu de donnÃ©es particulier a Ã©tÃ© utilisÃ© pour lâ€™entraÃ®nement dâ€™un modÃ¨le de ML. Ils ont dÃ©couvert quâ€™il est possible dâ€™utiliser un marqueur intelligent qui dÃ©place les caractÃ©ristiques dans une direction, que le modÃ¨le utilise pour aider Ã  dÃ©tecter lâ€™utilisation de donnÃ©es radioactives mÃªme si seulement 1 % des donnÃ©es dâ€™entraÃ®nement sont radioactives. Câ€™est un dÃ©fi car tout changement dans les donnÃ©es peut potentiellement dÃ©grader la prÃ©cision du modÃ¨le. Selon les auteurs, ce travail peut Â« aider les chercheurs et les ingÃ©nieurs Ã  savoir quel jeu de donnÃ©es a Ã©tÃ© utilisÃ© pour entraÃ®ner un modÃ¨le, afin de mieux comprendre comment les diffÃ©rents ensembles de donnÃ©es affectent les performances des diffÃ©rents rÃ©seaux neuronaux Â». Cette approche semble importante pour les applications critiques de ML. Consultez le document complet &lt;a href=&quot;https://arxiv.org/pdf/2002.00937.pdf&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;REALM: Retrieval-Augmented Language Model Pre-Training&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://kentonl.com/pub/gltpc.2020.pdf&quot;&gt;REALM&lt;/a&gt; est une approche dâ€™extraction Ã  grande Ã©chelle qui utilise un corpus de connaissances textuelles pour prÃ©-entraÃ®ner un modÃ¨le de langue de maniÃ¨re non supervisÃ©e. Les tÃ¢ches abordÃ©es et Ã©valuÃ©es Ã  lâ€™aide de REALM comprennent des questions ouvertes rÃ©pondant Ã  des critÃ¨res de rÃ©fÃ©rence. Outre lâ€™amÃ©lioration de la prÃ©cision du modÃ¨le, les autres avantages comprennent les composantes de modularitÃ© et dâ€™interprÃ©tabilitÃ©.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*MJO-yzCwsB5ydKGz7hKHVA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://kentonl.com/pub/gltpc.2020.pdf&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;crÃ©ativitÃ©-et-sociÃ©tÃ©-&quot;&gt;CrÃ©ativitÃ© et sociÃ©tÃ© ğŸ¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Permettre la prÃ©sentation Ã  distance de documents et dâ€™affiches lors de confÃ©rences scientifiques&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
La semaine derniÃ¨re, une &lt;a href=&quot;https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences&quot;&gt;pÃ©tition&lt;/a&gt; a Ã©tÃ© lancÃ©e pour permettre la prÃ©sentation Ã  distance de documents et dâ€™affiches lors de confÃ©rences scientifiques comme celles liÃ©es au ML. Il semble que Yoshua Bengio, plaide pour que les gens aillent signer la pÃ©tition. Il lâ€™a clairement indiquÃ© dans son nouveau &lt;a href=&quot;https://yoshuabengio.org/2020/02/10/fusce-risus/&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;DÃ©fi de lâ€™abstraction et du raisonnement&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
FranÃ§ois Chollet a rÃ©cemment mis en ligne un &lt;a href=&quot;https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview&quot;&gt;concours Kaggle&lt;/a&gt; oÃ¹ il a publiÃ© le Corpus dâ€™abstraction et de raisonnement (ARC). Il vise Ã  encourager les utilisateurs Ã  crÃ©er des systÃ¨mes dâ€™IA capables de rÃ©soudre des tÃ¢ches de raisonnement auxquelles ils nâ€™ont jamais Ã©tÃ© exposÃ©s. Lâ€™espoir est de commencer Ã  construire des systÃ¨mes dâ€™IA plus robustes, capables de rÃ©soudre mieux et rapidement de nouveaux problÃ¨mes par eux-mÃªmes, ce qui pourrait aider Ã  rÃ©soudre les applications du monde rÃ©el les plus difficiles, comme lâ€™amÃ©lioration des voitures autonomes.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Publications de ML et NLP en 2019&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Marek Rei publie son &lt;a href=&quot;https://www.marekrei.com/blog/ml-and-nlp-publications-in-2019/&quot;&gt;analyse annuelle&lt;/a&gt; sur les statistiques en lien avec lâ€™apprentissage machine et le NLP pour lâ€™annÃ©e 2019. Les confÃ©rences incluses dans lâ€™analyse sont ACL, EMNLP, NAACL, EACL, COLING, TACL, CL, CoNLL, NeurIPS, ICML, ICLR, et AAAI.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;La croissance dâ€™automates cellulaires neuronaux&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
La morphogenÃ¨se est un processus dâ€™auto-organisation par lequel certaines crÃ©atures comme les salamandres peuvent se rÃ©gÃ©nÃ©rer ou rÃ©parer des dommages corporels. Ce processus est robuste aux perturbations et de nature adaptative. InspirÃ©s par ce phÃ©nomÃ¨ne biologique et par le besoin de mieux comprendre le processus, les chercheurs ont publiÃ© un &lt;a href=&quot;https://distill.pub/2020/growing-ca/&quot;&gt;article&lt;/a&gt; intitulÃ© â€œGrowing Neural Cellular Automataâ€, qui adopte un modÃ¨le diffÃ©renciable pour la morphogenÃ¨se visant Ã  reproduire les comportements et les propriÃ©tÃ©s des systÃ¨mes dâ€™autorÃ©paration. Lâ€™espoir est de pouvoir construire des machines autorÃ©paratrices qui possÃ¨dent la mÃªme robustesse et plasticitÃ© que la vie biologique. En outre, cela permettrait de mieux comprendre le processus de rÃ©gÃ©nÃ©ration lui-mÃªme. Les applications qui peuvent en bÃ©nÃ©ficier comprennent la mÃ©decine rÃ©gÃ©nÃ©ratrice et la modÃ©lisation des systÃ¨mes sociaux et biologiques.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2p62h1RaHD6d11LX8olnTA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://distill.pub/2020/growing-ca/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Visualiser lâ€™attention des Transformers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hendrik Strobelt a partagÃ© ce &lt;a href=&quot;https://github.com/SIDN-IAP/attnvis&quot;&gt;repertoire&lt;/a&gt; qui montre comment construire rapidement une visualisation interactive simple de lâ€™attention dâ€™un Transformer Ã  travers une application web en utilisant la bibliothÃ¨que HuggingFace et d3.js.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;SketchTransfer : Une nouvelle tÃ¢che stimulante pour explorer lâ€™invariance des dÃ©tails et les abstractions apprises par les rÃ©seaux&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/pdf/1912.11570.pdf&quot;&gt;SketchTransfer&lt;/a&gt; propose une nouvelle tÃ¢che pour tester la capacitÃ© des rÃ©seaux neuronaux Ã  supporter lâ€™invariance en prÃ©sence/absence de dÃ©tails. On a longtemps dÃ©battu du fait que les rÃ©seaux ne peuvent pas se gÃ©nÃ©raliser Ã  des variations qui nâ€™ont pas encore Ã©tÃ© observÃ©es pendant lâ€™entraÃ®nement, comme par exemple traiter les dÃ©tails visuels manquants lorsquâ€™ils regardent des dessins animÃ©s. Le document examine et publie un ensemble de donnÃ©es pour aider les chercheurs Ã  Ã©tudier attentivement le problÃ¨me de Â« lâ€™invariance des dÃ©tails Â» en fournissant des croquis non Ã©tiquetÃ©s et des exemples Ã©tiquetÃ©s dâ€™images rÃ©elles.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*jdYuMoHiu2yya5rHzZyjwQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1912.11570.pdf&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;outils-et-jeux-de-donnÃ©es-ï¸&quot;&gt;Outils et jeux de donnÃ©es âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;DeepSpeed + ZeRO&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Microsoft a dÃ©voilÃ©e une librairie dâ€™optimisation pour lâ€™entrainement appelÃ©e DeepSpeed. Elle est compatible avec PyTorch et peut permettre lâ€™entrainement dâ€™un modÃ¨le de 100 milliards de paramÃ¨tres. La librairie se concentre sur quatre aspects importants de lâ€™entrainement dâ€™un modÃ¨le : lâ€™Ã©chelle, la vitesse, le coÃ»t et la convivialitÃ©. DeepSpeed a Ã©tÃ© &lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/&quot;&gt;lancÃ©&lt;/a&gt; en mÃªme temps que ZeRO. ZeRO est une technologie dâ€™optimisation de la mÃ©moire qui permet de faire du deep learning distribuÃ© Ã  grande Ã©chelle sur GPU tout en amÃ©liorant le dÃ©bit de trois Ã  cinq fois plus que le meilleur systÃ¨me actuel.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*MXDI1f3cSBrY5w2g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Une librairie pour mener des recherches rapides et efficaces sur le DL en 3D&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/&quot;&gt;PyTorch3D&lt;/a&gt; est une boÃ®te Ã  outils open-source pour la recherche sur le DL en 3D. La librairie consiste en des implÃ©mentations rapides et optimisÃ©es dâ€™opÃ©rateurs 3D et de fonctions de perte frÃ©quemment utilisÃ©s. Elle est Ã©galement dotÃ©e dâ€™un moteur de rendu modulaire et diffÃ©renciable qui permet de mener des recherches sur des entrÃ©es 3D complexes et de faire des prÃ©visions 3D de haute qualitÃ©.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*VbspKMmPBUsgpdnIkd5jYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Gestion de la configuration de projets de ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hydra est un outil pour gÃ©rer plus efficacement les projets de ML complexes. Il est destinÃ© Ã  aider les chercheurs de PyTorch en offrant une rÃ©utilisation fonctionnelle des configurations. Son principal avantage est quâ€™il permet au programmeur de gÃ©rer la configuration comme du code de composition, ce qui signifie que le fichier de configuration peut Ãªtre facilement Ã©crasÃ©. Hydra peut Ã©galement aider Ã  gÃ©rer automatiquement le rÃ©pertoire de travail des rÃ©sultats de votre projet de ML, ce qui est utile lorsque vous avez besoin de sauvegarder et dâ€™accÃ©der aux rÃ©sultats de plusieurs expÃ©riences pour des travaux multiples. Pour en savoir plus, cliquez &lt;a href=&quot;https://medium.com/pytorch/hydra-a-fresh-look-at-configuration-for-machine-learning-projects-50583186b710&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Une boÃ®te Ã  outils pour lâ€™infÃ©rence causale avec les rÃ©seaux bayÃ©siens&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html&quot;&gt;CausalNex&lt;/a&gt; est une boÃ®te Ã  outils pour â€œlâ€™infÃ©rence causale avec les rÃ©seaux bayÃ©siensâ€. Lâ€™outil vise Ã  combiner lâ€™apprentissage machine et le raisonnement causal pour dÃ©couvrir des relations structurelles dans les donnÃ©es. Les auteurs ont Ã©galement prÃ©parÃ© un guide dâ€™introduction sur le pourquoi et le comment de lâ€™infÃ©rence causale avec les rÃ©seaux bayÃ©siens en utilisant la librairie Python proposÃ©e.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*EYwKhdnscR7ZLuNkTqCS2Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Google Colab Pro est maintenant disponible&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google Colab propose dÃ©sormais une Ã©dition Pro, qui offre des avantages tels quâ€™un accÃ¨s exclusif Ã  des GPU et TPU plus rapides, des durÃ©es dâ€™exÃ©cution plus longues et plus de mÃ©moire.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;TyDi QA: Un benchmark pour le Question/Anwsering multilingues&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI publie &lt;a href=&quot;https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html&quot;&gt;TyDi QA&lt;/a&gt;, un ensemble de donnÃ©es multilingues qui peut encourager les chercheurs Ã  rÃ©pondre Ã  des questions dans des langues plus typologiquement diverses. Câ€™est Ã  dire qui construisent et expriment le sens de diffÃ©rentes maniÃ¨res. Lâ€™idÃ©e est de motiver les chercheurs Ã  construire des modÃ¨les plus robustes sur des langues typologiquement Ã©loignÃ©es, telles que lâ€™arabe, le bengali, le corÃ©en, le russe, le tÃ©lougou et le thaÃ¯, afin de gÃ©nÃ©raliser Ã  encore plus de langues.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*1dZv5you3jigdrQ2uAKzUw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Question Answering pour Node.js&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hugging Face publie une &lt;a href=&quot;https://github.com/huggingface/node-question-answering&quot;&gt;librairie&lt;/a&gt; de questions/rÃ©ponses basÃ©e sur DistilBERT. Ce modÃ¨le peut fonctionner en production en utilisant Node.js avec seulement 3 lignes de code. Le modÃ¨le tire parti de la mise en Å“uvre rapide de Tokenizers, et de TensorFlow.js (une bibliothÃ¨que populaire pour lâ€™utilisation de modÃ¨les dâ€™apprentissage machine avec Javascript).&lt;/p&gt;

&lt;h1 id=&quot;ethique-en-ia-&quot;&gt;Ethique en IA ğŸš¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Identifier les biais subjectifs dans les textes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ce &lt;a href=&quot;https://podcasts.apple.com/us/podcast/will-ai-help-identify-bias-or-perpetuate-it-with-diyi-yang/id1435564422?i=1000464141922&quot;&gt;podcast&lt;/a&gt; prÃ©sente Diyi Yang, chercheur en sciences sociales computationnelles, qui explique comment les systÃ¨mes dâ€™IA peuvent aider Ã  identifier les biais subjectifs dans les informations textuelles. Il sâ€™agit dâ€™un domaine de recherche important impliquant les systÃ¨mes dâ€™IA et de NLP. En particulier lorsque nous discutons de la consommation de mÃ©dias textuels tels que les news qui peuvent Ãªtre facilement encadrÃ©s pour biaiser les consommateurs alors quâ€™en rÃ©alitÃ© ils devraient viser Ã  Ãªtre plus objectifs. Du point de vue de lâ€™application, il devient essentiel dâ€™identifier automatiquement le biais subjectif prÃ©sent dans les mÃ©dias textuels afin dâ€™aider les consommateurs Ã  devenir plus conscients du contenu quâ€™ils consomment. Lâ€™Ã©pisode traite Ã©galement de la maniÃ¨re dont lâ€™IA peut Ã©galement perpÃ©tuer le biais.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Intelligence artificielle, valeurs et alignement&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Lâ€™essor des systÃ¨mes dâ€™IA et la maniÃ¨re dont ils sâ€™alignent sur les valeurs humaines est un domaine de recherche actif qui implique lâ€™Ã©thique dans les systÃ¨mes dâ€™IA. DeepMind a rÃ©cemment publiÃ© un &lt;a href=&quot;https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment&quot;&gt;papier&lt;/a&gt; qui examine plus en profondeur les questions philosophiques entourant lâ€™alignement de lâ€™IA. Le rapport se concentre sur deux parties :  technique (câ€™est-Ã -dire comment coder les valeurs qui rendent les rÃ©sultats des agents dâ€™IA fiables) et normative (quels principes seraient justes Ã  coder dans lâ€™IA). Le document prÃ©conise une approche fondÃ©e sur des principes visant Ã  prÃ©server Ã  prÃ©server un traitement Ã©quitable malgrÃ© la diffÃ©rence de croyances et dâ€™opinions.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Sur lâ€™audit des systÃ¨mes dâ€™IA&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
VentureBeat rapporte que Google Researchers, en collaboration avec dâ€™autres groupes, a crÃ©Ã© un framework appelÃ© SMACTR qui permet aux ingÃ©nieurs de vÃ©rifier les systÃ¨mes dâ€™IA. La raison de ce travail est de combler le fossÃ© de responsabilitÃ© qui existe avec les systÃ¨mes dâ€™IA actuels qui sont mis dans la nature pour Ãªtre utilisÃ©s par les consommateurs. Pour plus dâ€™informations, lire les deux documents suivants :  &lt;a href=&quot;https://venturebeat.com/2020/01/30/google-researchers-release-audit-framework-to-close-ai-accountability-gap/&quot;&gt;ici&lt;/a&gt; et &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3351095.3372873&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;articles-et-blog-ï¸&quot;&gt;Articles et Blog âœï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;La distillation de modÃ¨le en NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans un &lt;a href=&quot;https://soundcloud.com/nlp-highlights/104-model-distillation-with-victor-sanh-and-thomas-wolf&quot;&gt;podcast&lt;/a&gt; de NLP Highlights, Thomas Wolf et Victor Sanh parlent de la distillation de modÃ¨les et de la faÃ§on dont elle peut Ãªtre utilisÃ©e comme une approche rÃ©alisable pour comprimer de grands modÃ¨les comme BERT. Ce concept est discutÃ© plus en dÃ©tail dans la mÃ©thode quâ€™ils proposent, appelÃ©e &lt;a href=&quot;https://arxiv.org/abs/1910.01108&quot;&gt;DistilBERT&lt;/a&gt;, dans laquelle ils construisent des modÃ¨les plus petits (basÃ©s sur la mÃªme architecture quâ€™un modÃ¨le plus grand) pour essayer dâ€™imiter le comportement du modÃ¨le plus grand. En substance, le petit modÃ¨le (lâ€™Ã©tudiant) essaie de sâ€™adapter Ã  la distribution de probabilitÃ© de lâ€™enseignant.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;**BERT, ELMo, &amp;amp; GPT-2: Dans quelle mesure les reprÃ©sentations contextuelles des mots sont-elles contextualisÃ©es ? **&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
On a beaucoup parlÃ© du succÃ¨s des mÃ©thodes contextualisÃ©es comme BERT pour aborder une grande variÃ©tÃ© de tÃ¢ches complexes de NLP. Dans cet &lt;a href=&quot;https://kawine.github.io/blog/nlp/2020/02/03/contextual.html&quot;&gt;article&lt;/a&gt;, Kawin Ethayarajh tente de rÃ©pondre Ã  la question qui consiste Ã  savoir comment sont contextualisÃ©s les mots dans les modÃ¨les comme BERT, ELMo et le GPT-2. Les sujets abordÃ©s comprennent les mesures de la contextualitÃ©, la spÃ©cificitÃ© du contexte et les comparaisons entre les embeddings statiques et les reprÃ©sentations contextualisÃ©es.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*70aIv1Fkkz4rnHgQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://kawine.github.io/blog/nlp/2020/02/03/contextual.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Sparsity in Neural Networks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
FranÃ§ois Lagunas, a Ã©crit cet &lt;a href=&quot;https://medium.com/huggingface/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70&quot;&gt;article Medium&lt;/a&gt; pour discuter de son optimisme quant Ã  lâ€™adoption de tenseurs clairsemÃ©s dans les modÃ¨les de rÃ©seaux de neurones. Lâ€™espoir est dâ€™utiliser une forme de raretÃ© pour rÃ©duire la taille des modÃ¨les actuels qui, Ã  un moment donnÃ©, deviennent peu pratiques en raison de leur taille et de leur vitesse. Ce concept pourrait Ãªtre intÃ©ressant Ã  explorer en ML en raison de la taille mÃªme des modÃ¨les actuels comme les Transformers. Cependant, les dÃ©tails de mise en Å“uvre ne sont pas aussi clairs du point de vue des outils de dÃ©veloppement disponibles, et câ€™est quelque chose sur lequel la communautÃ© travaille dÃ©jÃ .&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;EntraÃ®ner votre propre modÃ¨le linguistique&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous souhaitez apprendre Ã  entrainer un modÃ¨le de zÃ©ro, consultez ce &lt;a href=&quot;https://huggingface.co/blog/how-to-train&quot;&gt;tutoriel&lt;/a&gt; dâ€™Hugging Face. Ils utilisent Ã©videmment leurs propres bibliothÃ¨ques Transformers et Tokenizers pour entraÃ®ner le modÃ¨le.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Tokenizers: Comment les machines lisent&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cathal Horan a publiÃ© un &lt;a href=&quot;https://blog.floydhub.com/tokenization-nlp/&quot;&gt;article&lt;/a&gt; sur la maniÃ¨re dont les modÃ¨les de NLP les plus rÃ©cents utilisent les tokenizers. Il explique Ã©galement pourquoi la tokenisation est un domaine de recherche actif, passionnant et important. Lâ€™article vous montre mÃªme comment entraÃ®ner vos propres tokenizers en utilisant des mÃ©thodes de tokenisation comme SentencePiece et WordPiece.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Vkjw5n9Sz0Was43haVNJMg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.floydhub.com/tokenization-nlp/%27&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education ğŸ“&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;ML Ã  lâ€™universitÃ© dâ€™Amsterdam&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Vous pouvez dÃ©sormais suivre en ligne le &lt;a href=&quot;https://mlvu.github.io/&quot;&gt;cours dâ€™apprentissage machine 2020 MLVU&lt;/a&gt;, qui comprend des diapositives, des &lt;a href=&quot;https://www.youtube.com/watch?v=excCZSTJEPs&amp;amp;feature=youtu.be&quot;&gt;vidÃ©os&lt;/a&gt; et le programme. Il sâ€™agit dâ€™une introduction au ML, mais il comporte Ã©galement dâ€™autres sujets liÃ©s Ã  lâ€™apprentissage approfondi, tels que les VAE et les GAN.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*zFpU2rQL5Fby7X3boJyQNg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mlvu.github.io/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Ressources mathÃ©matiques pour le ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Suzana IliÄ‡ et le Machine Learning Tokyo (MLT) ont fait un travail remarquable en termes de dÃ©mocratisation de lâ€™Ã©ducation au ML. Par exemple, consultez ce &lt;a href=&quot;https://github.com/Machine-Learning-Tokyo/Math_resources&quot;&gt;rÃ©pertoire&lt;/a&gt; qui prÃ©sente une collection de ressources en ligne gratuites pour apprendre les fondements des concepts mathÃ©matiques utilisÃ©s en ML.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introduction au Deep Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Suivez le cours â€œIntroduction to Deep Learningâ€ du MIT sur ce &lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;site&lt;/a&gt;. De nouveaux cours seront publiÃ©s chaque semaine et toutes les diapositives, vidÃ©os et codes seront publiÃ©s.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Deep Learning avec PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Alfredo Canziani a publiÃ© les diapositives et les notebooks pour son mini-cours sur lâ€™apprentissage profond avec PyTorch. Le dÃ©pÃ´t contient Ã©galement un &lt;a href=&quot;https://atcold.github.io/pytorch-Deep-Learning/&quot;&gt;site web complÃ©mentaire&lt;/a&gt; qui comprend des descriptions textuelles des concepts enseignÃ©s dans le cours.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Missing Semester of Your CS&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le â€œ&lt;a href=&quot;https://missing.csail.mit.edu/&quot;&gt;Missing Semester of Your CS&lt;/a&gt;â€ est un cours en ligne pouvant Ãªtre utile aux spÃ©cialistes des donnÃ©es ayant une formation autre que celle du dÃ©veloppement. Il comprend des sujets tels que les outils shell, les scripts et le contrÃ´le de version. Le cours a Ã©tÃ© publiÃ© par des membres du corps enseignant du MIT.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*weUnTXxmHxYf-B2DDaslvw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://missing.csail.mit.edu/2020/shell-tools/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Deep Learning avancÃ©&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
La CMU a publiÃ© les diapositives et le programme du cours â€œ&lt;a href=&quot;https://andrejristeski.github.io/10707-S20/syllabus.html&quot;&gt;Advanced Deep Learning&lt;/a&gt;â€ qui comprend des sujets tels que les modÃ¨les autorÃ©gressifs, les modÃ¨les gÃ©nÃ©rateurs et lâ€™apprentissage autosurveillÃ©/prÃ©dictif. Le cours sâ€™adresse aux Ã©tudiants de master ou de doctorat ayant une formation avancÃ©e en ML.&lt;/p&gt;

&lt;h1 id=&quot;mentions-spÃ©ciales-ï¸&quot;&gt;Mentions spÃ©ciales â­ï¸&lt;/h1&gt;

&lt;p&gt;Xu et ses collaborateurs (2020) ont proposÃ© une &lt;a href=&quot;https://arxiv.org/abs/2002.02925&quot;&gt;mÃ©thode&lt;/a&gt; pour remplacer et compresser progressivement un modÃ¨le BERT en le divisant en ses composantes dâ€™origine. Le modÃ¨le proposÃ© surpasse les autres approches de distillation sur le rÃ©fÃ©rentiel GLUE.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le cours â€œ&lt;a href=&quot;https://compstat-lmu.github.io/lecture_i2ml/index.html&quot;&gt;Introduction Ã  lâ€™apprentissage machine&lt;/a&gt;â€ couvre les bases du ML, la rÃ©gression supervisÃ©e, les forÃªts alÃ©atoires, le tuning des paramÃ¨tres et bien dâ€™autres sujets fondamentaux du ML.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le modÃ¨le grec de BERT (&lt;a href=&quot;https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1&quot;&gt;GreekBERT&lt;/a&gt;) est maintenant disponible sur Transformers.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jeremy Howard publie un &lt;a href=&quot;https://arxiv.org/abs/2002.04688&quot;&gt;article&lt;/a&gt; dÃ©crivant la librairie fastai. Il sâ€™agit dâ€™une lecture recommandÃ©e aux dÃ©veloppeurs de logiciels qui travaillent Ã  la crÃ©ation et Ã  lâ€™amÃ©lioration des librairies dâ€™apprentissage profond et de ML.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Deeplearning.ai complÃ¨te la publication des quatre cours de TensorFlow : &lt;a href=&quot;https://www.coursera.org/specializations/tensorflow-data-and-deployment&quot;&gt;Data and Deployment Specialization&lt;/a&gt;. Cette spÃ©cialisation vise principalement Ã  apprendre aux dÃ©veloppeurs comment dÃ©ployer efficacement des modÃ¨les dans diffÃ©rents scÃ©narios et utiliser les donnÃ©es de maniÃ¨re intÃ©ressante.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Raschka a rÃ©cemment publiÃ© un &lt;a href=&quot;https://arxiv.org/abs/2002.04803&quot;&gt;article&lt;/a&gt; intitulÃ© â€œMachine Learning in Pythonâ€ : Principaux dÃ©veloppements et tendances technologiques en matiÃ¨re de science des donnÃ©es, dâ€™apprentissage automatique et dâ€™intelligence artificielleâ€. Ce document est un examen complet du paysage des outils dâ€™apprentissage machine. Il permet de comprendre les avantages de certaines librairies et les concepts utilisÃ©s dans lâ€™ingÃ©nierie ML. En outre, un mot sur lâ€™avenir des bibliothÃ¨ques dâ€™apprentissage machine basÃ©es sur Python est fourni.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Vous pouvez retrouver la prÃ©cÃ©dente newsletter &lt;a href=&quot;https://dair.ai/NLP_Newsletter_-3_-FR/&quot;&gt;ici&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous avez des jeux de donnÃ©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine Ã©dition de la newletter, nâ€™hÃ©sitez pas Ã  me contacter Ã  ellfae@gmail.com ou par message sur &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Abonnez-vous&lt;/a&gt; pour recevoir les prochains numÃ©ros dans votre boÃ®te mail.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_-4_-FR/&quot;&gt;NLP Newsletter [FR] #4: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,â€¦&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 09, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [FR] #3: Flax, Thinc, Language-specific BERT models, Meena, Flyte, LaserTagger,â€¦]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_-3_-FR/" />
  <id>https://dair.ai/NLP_Newsletter_#3_[FR]</id>
  <published>2020-03-09T00:00:00-05:00</published>
  <updated>2020-03-09T00:00:00-05:00</updated>
  <author>
    <name>LoÃ¯ck BOURDOIS</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2400/1*qaOM0D2tfy3chvnWRdycGA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;avant-propos&quot;&gt;Avant-propos&lt;/h1&gt;
&lt;p&gt;Bienvenue Ã  la Newsletter sur le NLP ! Le troisiÃ¨me numÃ©ro traite de sujets tels que lâ€™amÃ©lioration des agents conversationnels, les versions de BERT spÃ©cifiques Ã  chaque langue, les ensembles de donnÃ©es gratuits, les versions des bibliothÃ¨ques dâ€™apprentissage approfondi, et bien plus encore.&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications ğŸ“™&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Versions de BERT spÃ©cifiques Ã  chaque langue&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jâ€™ai perdu le compte du nombre de modÃ¨les BERT spÃ©cifiques Ã  chaque langue, mais voici quelques-unes des versions rÃ©centes :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;BERT nÃ©erlandais (&lt;a href=&quot;https://arxiv.org/abs/2001.06286&quot;&gt;RobBERT&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1912.09582&quot;&gt;BERTje&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://deepset.ai/german-bert&quot;&gt;BERT allemand&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/neuralmind-ai/portuguese-bert&quot;&gt;BERT portugais&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;BERT franÃ§ais (&lt;a href=&quot;https://arxiv.org/abs/1911.03894&quot;&gt;CamemBERT&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1912.05372&quot;&gt;FlauBERT&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;BERT italien (&lt;a href=&quot;http://ceur-ws.org/Vol-2481/paper57.pdf&quot;&gt;AlBERTo&lt;/a&gt;, &lt;a href=&quot;https://github.com/musixmatchresearch/umberto&quot;&gt;UmBERTo&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;BERT espagnol (&lt;a href=&quot;https://github.com/dccuchile/beto&quot;&gt;BETO&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;BERT arabe (&lt;a href=&quot;https://colab.research.google.com/drive/1KSy89fAkWt6EGfnFQElDjXrBror9lIZh&quot;&gt;araBERT&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
Notez que la plupart de ces modÃ¨les sont Ã©galement disponibles par le biais de la librairie de Transformers dâ€™Hugging Face, qui a rÃ©cemment Ã©tÃ© mise Ã  jour avec la version &lt;a href=&quot;https://github.com/huggingface/transformers/releases&quot;&gt;2.4.1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;RÃ©sultats trop optimistes de prÃ©dictions sur des donnÃ©es dÃ©sÃ©quilibrÃ©es : dÃ©fauts et avantages de lâ€™application du sur-Ã©chantillonnage&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cette &lt;a href=&quot;https://arxiv.org/abs/2001.06296&quot;&gt;publication&lt;/a&gt; rÃ©vÃ¨le et examine en dÃ©tail certains des dÃ©fauts et des avantages de lâ€™application du sur-Ã©chantillonnage pour traiter les jeux de donnÃ©es dÃ©sÃ©quilibrÃ©s avant de les partitionner. En outre, le travail reproduit des Ã©tudes antÃ©rieures et identifie cette faille mÃ©thodologique qui produit des rÃ©sultats trop optimistes.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Encoder, Ã©tiqueter et rÃ©aliser : une approche contrÃ´lable et efficace pour la gÃ©nÃ©ration de texte&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Afin de rÃ©duire lâ€™effet dâ€™&lt;a href=&quot;https://arxiv.org/abs/1910.08684&quot;&gt;hallucination&lt;/a&gt; (production de sorties non supportÃ©es par le texte dâ€™entrÃ©e) commun aux mÃ©thodes de gÃ©nÃ©ration de texte basÃ©es sur seq2seq, un groupe dâ€™ingÃ©nieurs de Google a ouvert une mÃ©thode de gÃ©nÃ©ration de texte appelÃ©e &lt;a href=&quot;https://ai.googleblog.com/2020/01/encode-tag-and-realize-controllable-and.html&quot;&gt;LaserTagger&lt;/a&gt;.
Lâ€™idÃ©e principale de cette mÃ©thode est de produire des sorties en marquant des mots avec des opÃ©rations dâ€™Ã©dition prÃ©dites (par exemple, KEEP, DELETE-ADD,, etc.) et en les appliquant aux mots dâ€™entrÃ©e dans une Ã©tape dite de rÃ©alisation. Cette mÃ©thode remplace celle de gÃ©nÃ©ration de texte courante qui ne fait que gÃ©nÃ©rer des sorties Ã  partir de zÃ©ro, ce qui est gÃ©nÃ©ralement lent et sujet Ã  des erreurs. Le modÃ¨le offre dâ€™autres avantages en plus de gÃ©nÃ©rer moins dâ€™erreurs, tels que la possibilitÃ© de prÃ©voir en parallÃ¨le les opÃ©rations dâ€™Ã©dition tout en conservant une bonne prÃ©cision et en surpassant une base de rÃ©fÃ©rence BERT dans des scÃ©narios avec un nombre rÃ©duit dâ€™exemples dâ€™entraÃ®nement.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/0*OJN4pNgrQoS2STAX.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;â€Š&lt;a href=&quot;https://ai.googleblog.com/2020/01/encode-tag-and-realize-controllable-and.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Les rÃ©seaux neuronaux convolutifs comme modÃ¨le du systÃ¨me visuel : passÃ©, prÃ©sent et futur&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Grace Lindsay a publiÃ© ce &lt;a href=&quot;https://arxiv.org/abs/2001.07092&quot;&gt;rapport&lt;/a&gt; sur lâ€™histoire des CNN et sur la maniÃ¨re dont ils sont Ã©valuÃ©s en tant que modÃ¨les de vision biologique, câ€™est-Ã -dire comment les reprÃ©sentations des CNN se comparent Ã  celles du cerveau ? La discussion sur les nouvelles possibilitÃ©s dâ€™utilisation des CNN pour la recherche sur la vision est vivement recommandÃ©e aux lecteurs.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*SngMqzPQJigR5A3AzeJGDQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.07092&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Multilingual Denoising Pre-training for Neural Machine Translation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Facebook AI a publiÃ© &lt;a href=&quot;https://arxiv.org/pdf/2001.08210.pdf&quot;&gt;mBART&lt;/a&gt;, une mÃ©thode basÃ©e sur un auto-encodeur de dÃ©bruitage multilingue seq2seq prÃ©-entrainÃ© sur des corpus monolingues Ã  grande Ã©chelle pour la traduction automatique dans 25 langues.
Le texte dâ€™entrÃ©e implique le masquage des phrases et la permutation des phrases (bruits). Un modÃ¨le basÃ© sur un Transformer est appris pour reconstruire le texte dans plusieurs langues. Le modÃ¨le autorÃ©gressif complet nâ€™est entraÃ®nÃ© quâ€™une seule fois et peut Ãªtre ajustÃ© sur nâ€™importe quelle paire de langues sans impliquer de modifications spÃ©cifiques Ã  la tÃ¢che ou Ã  la langue. Les problÃ¨mes de traduction au niveau du document et de la phrase sont abordÃ©s. En plus de montrer des gains de performance, les auteurs affirment que la mÃ©thode fonctionne bien sur la traduction automatique Ã  faible ressource.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*aigX70Om2rEaI7OoTcpyGA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2001.08210.pdf&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Sur lâ€™amÃ©lioration des agents conversationnels&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html&quot;&gt;Meena&lt;/a&gt; est un agent conversationnel qui vise Ã  mener des conversations plus sensibles et plus spÃ©cifiques ;  des mesures dÃ©finies pour saisir les attributs importants dâ€™une conversation humaine (par exemple, la fluiditÃ©). Le modÃ¨le apprend le contexte de la conversation via un encodeur et formule une rÃ©ponse sensible via le dÃ©codeur. Il est signalÃ© que lâ€™amÃ©lioration de la qualitÃ© des conversations a Ã©tÃ© possible en considÃ©rant des dÃ©codeurs plus puissants.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Vous pouvez Ã©galement prendre connaissance des &lt;a href=&quot;https://venturebeat.com/2020/01/31/with-googles-meena-are-ai-assistants-about-to-get-a-lot-smarter/&quot;&gt;rÃ©flexions&lt;/a&gt; dâ€™Alan Nichol (co-fondateur du siÃ¨ge de Rasa) sur ce travail.&lt;/p&gt;

&lt;h1 id=&quot;crÃ©ativitÃ©-et-sociÃ©tÃ©-&quot;&gt;CrÃ©ativitÃ© et sociÃ©tÃ© ğŸ¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Test de comprÃ©hension de la lecture et analyseur de sentiments&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ming Cheuk a conÃ§u cette &lt;a href=&quot;https://littlealbert.now.sh/#/&quot;&gt;application&lt;/a&gt; qui permet de tester les capacitÃ©s de comprÃ©hension de lecture dâ€™&lt;a href=&quot;https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html&quot;&gt;ALBERT&lt;/a&gt;. ALBERT est une version plus petite de BERT pour lâ€™apprentissage des reprÃ©sentations des langues. Lâ€™auteur explique plus en dÃ©tail le projet et les approches utilisÃ©es dans ce &lt;a href=&quot;https://www.spark64.com/post/machine-comprehension&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hendrik Strobelt a Ã©galement publiÃ© un petit &lt;a href=&quot;https://github.com/HendrikStrobelt/sentimenter_minimal_hai&quot;&gt;projet&lt;/a&gt; dans lequel il montre comment rÃ©aliser un prototype dâ€™un analyseur de sentiments interactif.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*kgKeL3svHqScr0Wjnfe0Cg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://littlealbert.now.sh/#/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Le parcours dâ€™un chercheur autodidacte sur lâ€™IA chez Google&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans cet &lt;a href=&quot;https://blog.floydhub.com/emils-story-as-a-self-taught-ai-researcher/&quot;&gt;entretien&lt;/a&gt; Emil, un chercheur de ML Ã  Google Art &amp;amp; Culture, parle de son parcours en tant que chercheur autodidacte.&lt;/p&gt;

&lt;h1 id=&quot;outils-et-jeux-de-donnÃ©es-ï¸&quot;&gt;Outils et jeux de donnÃ©es âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Jeux de donnÃ©es en libre accÃ¨s&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://blog.google/products/search/discovering-millions-datasets-web/&quot;&gt;Google Dataset Search&lt;/a&gt; fournit dÃ©sormais jusquâ€™Ã  25 millions de jeux de donnÃ©es. Il sâ€™agit essentiellement dâ€™un moteur de recherche pour les ensembles de donnÃ©es.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
La base de donnÃ©es &lt;a href=&quot;https://quantumstat.com/dataset/dataset.html&quot;&gt;Big Bad NLP&lt;/a&gt; est un site web oÃ¹ vous pouvez rechercher une base de donnÃ©es dÃ©diÃ©e de plus de 200 jeux de donnÃ©es de NLP de tous types pour des tÃ¢ches telles que le raisonnement, lâ€™analyse des sentiments, la rÃ©ponse aux questions, lâ€™infÃ©rence dâ€™implication, etcâ€¦&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*uYwA0snqOdKYyTJ56edtyA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Librairie dâ€™apprentissage par renforcement&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Chris Nota a dÃ©veloppÃ© et publiÃ© une &lt;a href=&quot;https://github.com/cpnota/autonomous-learning-library&quot;&gt;librairie PyTorch&lt;/a&gt; pour la construction dâ€™agents dâ€™apprentissage par renforcement basÃ©s sur des algorithmes populaires tels que DQN, PPO et DDPG. Lâ€™accent de la librairie est mis sur la conception orientÃ©e objet et sur la mise en Å“uvre et lâ€™Ã©valuation rapides de nouveaux agents dâ€™apprentissage par renforcement.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;ExplicabilitÃ© et interprÃ©tabilitÃ© du ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous travaillez actuellement avec des modÃ¨les linguistiques basÃ©s sur des textes et que vous souhaitez comprendre comment les interprÃ©ter plus facilement lorsquâ€™ils sont appliquÃ©s Ã  diffÃ©rentes tÃ¢ches linguistiques, alors vous pourriez Ãªtre intÃ©ressÃ© par &lt;a href=&quot;https://captum.ai/&quot;&gt;Captum&lt;/a&gt;. Captum est une librairie dâ€™interprÃ©tabilitÃ© qui peut Ãªtre utilisÃ©e pour analyser lâ€™importance des caractÃ©ristiques, interprÃ©ter des modÃ¨les de texte et de vision, interprÃ©ter des modÃ¨les multimodaux, et dâ€™autres modÃ¨les tels que BERT utilisÃ© pour la rÃ©ponse aux questions.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous vous intÃ©ressez Ã  lâ€™explicabilitÃ© des modÃ¨les, cet &lt;a href=&quot;https://www.kaggle.com/learn/machine-learning-explainability&quot;&gt;tutoriels&lt;/a&gt; peuvent Ã©galement vous intÃ©resser.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Libraries de ML et DL&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Lâ€™Ã©quipe de Google Research a publiÃ© &lt;a href=&quot;https://github.com/google-research/flax/tree/prerelease&quot;&gt;Flax&lt;/a&gt;, une libririe de rÃ©seaux neuronaux flexible et puissante basÃ©e sur &lt;a href=&quot;https://github.com/google/jax&quot;&gt;JAX&lt;/a&gt; qui fournit un framework pour le calcul rapide et lâ€™entraÃ®nement de modÃ¨les de ML en utilisant les API typiques de Numpy.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*LSWFZM-xMV-GnvGl_lC-sg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Flax syntax&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://thinc.ai/&quot;&gt;Thinc&lt;/a&gt; est une librairie lÃ©gÃ¨re de deep learning dÃ©veloppÃ©e par les crÃ©ateurs de spaCy. Elle offre des API de programmation fonctionnelle pour composer, configurer et dÃ©ployer des modÃ¨les personnalisÃ©s construits avec des librairies comme PyTorch et TensorFlow.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Lyft lance &lt;a href=&quot;https://eng.lyft.com/introducing-flyte-cloud-native-machine-learning-and-data-processing-platform-fb2bb3046a59&quot;&gt;Flyte&lt;/a&gt;, une plateforme, prÃªte pour la production et sans serveur, pour le dÃ©ploiement dde travail de traitement de donnÃ©es et de ML.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Un outil pour lâ€™IA conversationnelle&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/deepmipt/DeepPavlov&quot;&gt;DeepPavlov&lt;/a&gt; offre une solution gratuite et facile Ã  utiliser pour la construction de systÃ¨mes de dialogue et de systÃ¨mes conversationnels complexes. DeepPavlov est livrÃ© avec plusieurs composants prÃ©dÃ©finis pour rÃ©soudre les problÃ¨mes liÃ©s au NLP. Il intÃ¨gre BERT (y compris le BERT conversationnel) dans trois tÃ¢ches : la classification de textes, la reconnaissance dâ€™entitÃ©s nommÃ©es (et le marquage des sÃ©quences en gÃ©nÃ©ral) et la rÃ©ponse aux questions. En consÃ©quence, il a permis dâ€™amÃ©liorer considÃ©rablement toutes ces tÃ¢ches. (&lt;a href=&quot;https://colab.research.google.com/github/deepmipt/dp_notebooks/blob/master/DP_tf.ipynb&quot;&gt;Google Colab&lt;/a&gt; | &lt;a href=&quot;https://medium.com/tensorflow/deeppavlov-an-open-source-library-for-end-to-end-dialog-systems-and-chatbots-31cf26849e37&quot;&gt;Blog&lt;/a&gt; | &lt;a href=&quot;https://demo.deeppavlov.ai/#/en/textqa&quot;&gt;DÃ©mo&lt;/a&gt;).&lt;/p&gt;

&lt;h1 id=&quot;ethique-en-ia-&quot;&gt;Ethique en IA ğŸš¨&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Reconnaissance faciale et vie privÃ©e&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le New York Times a rÃ©digÃ© un article sur les diffÃ©rentes perspectives concernant la vie privÃ©e impliquant la technologie de reconnaissance faciale. Ce reportage porte sur une â€œsociÃ©tÃ© secrÃ¨teâ€ appelÃ©e Clearview qui utiliserait la technologie dâ€™IA pour construire une reconnaissance faciale universelle Ã  partir dâ€™images rÃ©cupÃ©rÃ©es sur les rÃ©seaux sociaux tels que Twitter, Facebook et YouTube, etcâ€¦ Cette technologie soulÃ¨ve des inquiÃ©tudes quant au respect de la vie privÃ©e, mais elle serait Ã©galement utilisÃ©e principalement pour lâ€™application de la loi. Pour en savoir plus, cliquez &lt;a href=&quot;https://www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.html&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;ProgrÃ¨s de lâ€™IA au niveau humain&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans cet &lt;a href=&quot;https://fortune.com/longform/ai-artificial-intelligence-big-tech-microsoft-alphabet-openai/&quot;&gt;article&lt;/a&gt;, Jeremy Kahn discute en dÃ©tail de la diffÃ©rence entre Â« lâ€™IA Ã©troite Â» et Â« lâ€™IA gÃ©nÃ©rale Â» dans le contexte des progrÃ¨s actuels de la technologie. Outre les nombreux sujets abordÃ©s, de nombreuses questions se posent sur les bÃ©nÃ©fices de la rÃ©alisation de lâ€™IA gÃ©nÃ©rale. Lâ€™article mentionne Ã©galement lâ€™intÃ©rÃªt rÃ©cent des grandes entreprises technologiques qui investissent dans ces efforts. Lâ€™article inclut plusieurs prÃ©occupations soulevÃ©es par des chercheurs respectÃ©s qui dÃ©noncent le comportement â€œirresponsable et contraire Ã  lâ€™Ã©thiqueâ€ de certains organismes de recherche qui tentent de manipuler les rÃ©cits sur lâ€™IA Ã  leur profit.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Technologies dâ€™IA prÃ©servant la vie privÃ©e&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Lâ€™un des efforts menÃ© afin de promouvoir une IA ethique, responsable et respectant la vie privÃ©e est celui de la communautÃ© [OpenMined] https://twitter.com/OpenMinedOrg). Si vous voulez en savoir plus, vous pouvez Ã©couter Andrew Trask, parler de cette initiative dans cette &lt;a href=&quot;https://www.youtube.com/watch?v=4zrU54VIK6k&quot;&gt;vidÃ©o&lt;/a&gt; faisant partie de la sÃ©rie de confÃ©rences du MIT sur lâ€™apprentissage profond.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Comprendre lâ€™Ã©thique et la sÃ©curitÃ©&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le Dr David Leslie a publiÃ© ce &lt;a href=&quot;https://www.turing.ac.uk/sites/default/files/2019-06/understanding_artificial_intelligence_ethics_and_safety.pdf&quot;&gt;rapport&lt;/a&gt; trÃ¨s dÃ©taillÃ© sur des sujets qui aident Ã  mieux comprendre lâ€™IA dans le contexte de lâ€™Ã©thique et de la sÃ©curitÃ©. Il vise Ã  aider les dÃ©veloppeurs et les chercheurs Ã  mieux concevoir et mettre en Å“uvre des systÃ¨mes dâ€™IA pour le secteur public.
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*Ye09aVDP93RKsLc12PXqNQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.turing.ac.uk/sites/default/files/2019-06/understanding_artificial_intelligence_ethics_and_safety.pdf&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;articles-et-blog-ï¸&quot;&gt;Articles et Blog âœï¸&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Tutoriel sur lâ€™accÃ©lÃ©ration de la tokenisation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Steven van de Graaf a Ã©crit cet [article] https://towardsdatascience.com/a-small-timing-experiment-on-the-new-tokenizers-library-a-write-up-7caab6f80ea6) Ã  propos des performances de la librairie &lt;a href=&quot;https://github.com/huggingface/tokenizers&quot;&gt;Tokenizers&lt;/a&gt; dâ€™Hugging Face par rapport au tokenizer standard intÃ©grÃ© utilisÃ© dans la librairie &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;Transformers&lt;/a&gt;. Steven constate quâ€™une implÃ©mentation prend 10,6 secondes pour tokenizer 1 million de phrases et que le temps dâ€™exÃ©cution est divisÃ© par 9.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Les modÃ¨les linguistiques peuvent-ils vraiment comprendre ?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
The Gradient a rÃ©cemment publiÃ© ce &lt;a href=&quot;https://thegradient.pub/gpt2-and-the-nature-of-intelligence/&quot;&gt;post&lt;/a&gt; de Gary Marcus oÃ¹ il discute de ce quâ€™il croit Ãªtre des dÃ©fauts fondamentaux derriÃ¨re des modÃ¨les de langage comme GPT-2. Lâ€™argument principal de Gary Marcus est quâ€™un modÃ¨le entraÃ®nÃ© pour pouvoir prÃ©dire le mot suivant nâ€™est pas nÃ©cessairement un modÃ¨le capable de comprendre ou de raisonner. Câ€™est-Ã -dire que â€œla prÃ©diction est une composante de la comprÃ©hension, pas lâ€™ensembleâ€.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Curriculum for Reinforcement Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Lillian Weng &lt;a href=&quot;https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html&quot;&gt;rÃ©sume&lt;/a&gt; plusieurs approches basÃ©es sur les curriculum et la faÃ§on dont cela peut Ãªtre utilisÃ© pour entraÃ®ner de faÃ§on efficace des agents dâ€™apprentissage par renforcement. Weng aborde les dÃ©fis de la conception dâ€™une telle approche, qui nÃ©cessite gÃ©nÃ©ralement de trier la complexitÃ© des tÃ¢ches et de fournir au modÃ¨le une sÃ©quence de tÃ¢ches dont le niveau de difficultÃ© augmente au cours de lâ€™entraÃ®nement.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/0*B-t_sNMjKiOb_Y3Z.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introduction Ã  NumPy&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Anne Bonner a rÃ©cemment publiÃ© ce tutoriel dÃ©taillÃ© prÃ©sentant les bases de &lt;a href=&quot;https://numpy.org/devdocs/user/absolute_beginners.html&quot;&gt;NumPy&lt;/a&gt;. Il est constitue une base solide pour les personnes souhaitant sâ€™initier Ã  cette librairie.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/0*FmUSU_dh-_cqGUk_.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://numpy.org/devdocs/user/absolute_beginners.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education ğŸ“&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Fondements de lâ€™apprentissage machine et de lâ€™infÃ©rence statistique&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Anima Anandkumar, du Caltech, a publiÃ© un cours intitulÃ© â€œFondements de lâ€™apprentissage machine et de lâ€™infÃ©rence statistiqueâ€. Le cours se concentre sur les concepts de ML tels que les matrices, les tenseurs, lâ€™optimisation, les modÃ¨les probabilistes, les rÃ©seaux de neurones et bien plus encore. Ce cours aborde les aspects thÃ©oriques du ML. (&lt;a href=&quot;https://www.youtube.com/playlist?list=PLVNifWxslHCDlbyitaLLYBOAEPbmF1AHg&quot;&gt;la playlist vidÃ©o&lt;/a&gt; | &lt;a href=&quot;http://tensorlab.cms.caltech.edu/users/anima/cms165-2020.html&quot;&gt;le programme du cours&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;SÃ©rie de confÃ©rences sur le DL&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind sâ€™est associÃ© Ã  lâ€™UCL pour lancer une &lt;a href=&quot;https://www.eventbrite.co.uk/o/ucl-x-deepmind-deep-learning-lecture-series-general-29078980901&quot;&gt;sÃ©rie de 12 confÃ©rences&lt;/a&gt; sur lâ€™apprentissage profond donnÃ©es par des chercheurs de DeepMind.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;~7 million de programmes dâ€™Ã©tudes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://opensyllabus.org/&quot;&gt;Open Syllabus&lt;/a&gt; est une organisation Ã  but non lucratif qui utilise le crowdsourcing pour mettre en place un programme dâ€™enseignement supÃ©rieur dans une base de donnÃ©es en ligne. Elle contient actuellement environ sept millions de programmes.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*fwQIhfb2VWuwQJM_LaLehg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://opensyllabus.org/results-list/titles?size=50&amp;amp;fields=Computer%20Science&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Discuter, partager et apprendre sur le ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.reddit.com/r/ResearchML/&quot;&gt;r/ResearchML&lt;/a&gt; est une nouvelle sous-rubrique de reddit consacrÃ© au ML. Celui-ci est davantage axÃ© sur la recherche et encourage des discussions plus approfondies.&lt;/p&gt;

&lt;h1 id=&quot;mentions-spÃ©ciales-ï¸&quot;&gt;Mentions spÃ©ciales â­ï¸&lt;/h1&gt;

&lt;p&gt;DÃ©couvrez comment &lt;a href=&quot;https://github.blog/2020-01-22-how-we-built-good-first-issues/&quot;&gt;GitHub&lt;/a&gt; exploite lâ€™apprentissage machine pour repÃ©rer les problÃ¨mes faciles et personnalisÃ©s des dÃ©veloppeurs afin quâ€™ils puissent sâ€™attaquer aux questions qui correspondent Ã  leurs intÃ©rÃªts. Cela encourage des contributions plus rapides et plus nombreuses de la part des contributeurs de logiciels libres.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Ruder a publiÃ© une nouvelle &lt;a href=&quot;http://newsletter.ruder.io/issues/nlp-progress-restrospectives-and-look-ahead-new-nlp-courses-independent-research-initiatives-interviews-lots-of-resources-217744&quot;&gt;newsletter&lt;/a&gt;. On y trouve une mise Ã  jour des progrÃ¨s du NLP, des rÃ©trospectives sur la derniÃ¨re dÃ©cennie, de nouveaux cours de NLP, et dâ€™autres sujets.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jetez un coup dâ€™Å“il Ã  &lt;a href=&quot;https://github.com/NERSC/dl4sci-tf-tutorials&quot;&gt;ces notebooks TensorFlow 2.0&lt;/a&gt; qui vont de CycleGAN Ã  Transformers en passant par les tÃ¢ches de sous-titrage dâ€™images. Ils ont Ã©tÃ© rendus publics par lâ€™Ã©cole dâ€™apprentissage profond pour la science du LBNL.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Un &lt;a href=&quot;https://engineering.papercup.com/posts/bayesian-neural-nets/&quot;&gt;blog&lt;/a&gt; pour sâ€™initier aux rÃ©seaux neuronaux bayÃ©siens.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
John Schulman &lt;a href=&quot;http://joschu.net/blog/opinionated-guide-ml-research.html&quot;&gt;partage&lt;/a&gt; quelques conseils pour les futurs chercheurs du LBNL sur la faÃ§on de mieux choisir les problÃ¨mes de recherche et dâ€™Ãªtre plus stratÃ©gique dans la rÃ©alisation des tÃ¢ches de recherche. John partage Ã©galement des conseils pour le dÃ©veloppement personnel et le progrÃ¨s continu.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Vous pouvez retrouver la prÃ©cÃ©dente newsletter &lt;a href=&quot;https://dair.ai/NLP_Newsletter_-2_-FR/&quot;&gt;ici&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous avez des jeux de donnÃ©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine Ã©dition de la newletter, nâ€™hÃ©sitez pas Ã  me contacter Ã  ellfae@gmail.com ou par message sur &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Abonnez-vous&lt;/a&gt; pour recevoir les prochains numÃ©ros dans votre boÃ®te mail.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_-3_-FR/&quot;&gt;NLP Newsletter [FR] #3: Flax, Thinc, Language-specific BERT models, Meena, Flyte, LaserTagger,â€¦&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 09, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [FR] #2: Reformer, DeepMath, ELECTRA, TinyBERT, VizSeq, Open-Sourcing ML,â€¦]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_-2_-FR/" />
  <id>https://dair.ai/NLP_Newsletter_#2_[FR]</id>
  <published>2020-03-09T00:00:00-05:00</published>
  <updated>2020-03-09T00:00:00-05:00</updated>
  <author>
    <name>LoÃ¯ck BOURDOIS</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*mgWc3FhHPRfCxdPir6wSeg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;avant-propos&quot;&gt;Avant-propos&lt;/h1&gt;
&lt;p&gt;Bienvenue Ã  cette nouvelle newsletter consacrÃ©e au NLP ! Ce deuxiÃ¨me numÃ©ro aborde des sujets qui vont de lâ€™interprÃ©tabilitÃ© des modÃ¨les au repliement des protÃ©ines en passant par lâ€™apprentissage par transfert actif.&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications ğŸ“™&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Sur lâ€™incertitude de la confiance dans un modÃ¨le&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Un article rÃ©cent de Google AI, publiÃ© au NeurIPS, examine si les probabilitÃ©s sorties par un modÃ¨le reflÃ¨tent sa capacitÃ© Ã  prÃ©voir les donnÃ©es dÃ©calÃ©es et hors distribution. Ils ont constatÃ© que les ensembles profonds ont de meilleures performances (câ€™est-Ã -dire une meilleure incertitude du modÃ¨le) sur le dÃ©calage de lâ€™ensemble de donnÃ©es, tandis que dâ€™autres modÃ¨les ne sont pas devenus de plus en plus incertains sur le dÃ©calage de lâ€™ensemble de donnÃ©es, mais se sont plutÃ´t trompÃ©s avec confiance. (Lire lâ€™article &lt;a href=&quot;https://arxiv.org/abs/1906.02530&quot;&gt;ici&lt;/a&gt; et le rÃ©sumÃ© &lt;a href=&quot;https://ai.googleblog.com/2020/01/can-you-trust-your-models-uncertainty.html&quot;&gt;ici&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*NrsUnHS1thKq3ChK.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;image corruptionâ€Šâ€”&lt;/em&gt;â€Š&lt;a href=&quot;https://ai.googleblog.com/2020/01/can-you-trust-your-models-uncertainty.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;GÃ©nÃ©ralisation systÃ©matique&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Un &lt;a href=&quot;https://www.semanticscholar.org/paper/Systematic-Generalization%3A-What-Is-Required-and-Can-Bahdanau-Murty/6c7494a47cc5421a7b636c244e13586dc2dab007&quot;&gt;travail&lt;/a&gt; intÃ©ressant publiÃ© dans ICLR prÃ©sente une comparaison entre les modÃ¨les modulaires et les modÃ¨les gÃ©nÃ©riques concernant leur efficacitÃ© pour la gÃ©nÃ©ralisation systÃ©matique dans la comprÃ©hension des langues. Sur la base dâ€™une Ã©valuation effectuÃ©e des questions/rÃ©ponses en lien avec une &lt;a href=&quot;https://arxiv.org/abs/1909.01860&quot;&gt;tÃ¢che visuelle&lt;/a&gt;, les auteurs concluent quâ€™il peut Ãªtre nÃ©cessaire dâ€™utiliser des rÃ©gularisateurs et des antÃ©cÃ©dents explicites pour parvenir Ã  une gÃ©nÃ©ralisation systÃ©matique.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Le Reformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Un Transformer est limitÃ© au niveau de la fenÃªtre de contexte quâ€™il peut couvrir en raison des calculs coÃ»teux effectuÃ©s dans la couche dâ€™attention. Ainsi, il est possible dâ€™appliquer le Transformer quâ€™Ã  des tailles de texte limitÃ©es ou de gÃ©nÃ©rer que de courtes phrases / morceaux de musique. GoogleAI a rÃ©cemment publiÃ© une variante efficace du modÃ¨le Transformer, appelÃ©e &lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;Reformer&lt;/a&gt;. Lâ€™objectif principal de cette mÃ©thode est de pouvoir traiter des sÃ©quences de contexte beaucoup plus grandes tout en rÃ©duisant les besoins de calcul et en amÃ©liorant lâ€™efficacitÃ© de la mÃ©moire. Reformer utilise le â€œlocality-sensitive-hashingâ€ (&lt;a href=&quot;https://fr.wikipedia.org/wiki/Locality_sensitive_hashing&quot;&gt;LSH&lt;/a&gt;) pour regrouper des vecteurs similaires et crÃ©er des segments Ã  partir de ceux-ci. Cela permet ainsi un traitement en parallÃ¨le. Lâ€™attention est ensuite portÃ©e sur ces segments plus petits et sur les parties voisines correspondantes, rÃ©duisant la charge de calcul. Lâ€™efficacitÃ© de la mÃ©moire est obtenue grÃ¢ce Ã  des couches rÃ©versibles qui permettent de recalculer Ã  la demande les informations dâ€™entrÃ©e de chaque couche tout en sâ€™entraÃ®nant par rÃ©tropropagation. Câ€™est une technique simple qui Ã©vite au modÃ¨le de devoir stocker en mÃ©moire les activations. Une description de ce modÃ¨le est disponible en langue franÃ§aise sur ce &lt;a href=&quot;https://lbourdois.github.io/blog/nlp/Reformer/&quot;&gt;site&lt;/a&gt;. Pour voir comment le Reformer peut Ãªtre appliquÃ© Ã  une tÃ¢che de gÃ©nÃ©ration dâ€™images, je vous invite Ã  consulter ce &lt;a href=&quot;https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb&quot;&gt;Google Colab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*Q6FHJ5bqZRCrBAp9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
 &lt;strong&gt;&lt;em&gt;Adaptation non supervisÃ©e de domaines pour la classification de textes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ce &lt;a href=&quot;https://arxiv.org/abs/2001.04362&quot;&gt;travail&lt;/a&gt; propose une combinaison de mesures de distance qui incorporÃ©es dans une fonction de perte lors de lâ€™entraÃ®nement dâ€™un modÃ¨le, permet dâ€™amÃ©liorer lâ€™adaptation du domaine non supervisÃ©. Le modÃ¨le est Ã©tendu Ã  un modÃ¨le Â« DistanceNet Bandit Â». Le problÃ¨me clÃ© abordÃ© par cette mÃ©thode est de comprendre comment traiter la dissimilitude entre les donnÃ©es de diffÃ©rents domaines.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;AmÃ©lioration des reprÃ©sentations contextualisÃ©es&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ce &lt;a href=&quot;https://openreview.net/forum?id=r1xMH1BtvB&quot;&gt;document&lt;/a&gt; propose une tÃ¢che de prÃ©-entraÃ®nement, appelÃ©e token detection, qui se rÃ©vÃ¨le plus efficace pour entraÃ®ner un modÃ¨le linguistique que les mÃ©thodes de basÃ©es sur un prÃ©-entaÃ®nement avec des masques telles que BERT par exemple. Le modÃ¨le est baptisÃ© ELECTRA et ses reprÃ©sentations contextualisÃ©es surpassent celles de BERT et XLNET Ã  donnÃ©es identiques et Ã  taille de modÃ¨le identique. La mÃ©thode fonctionne particuliÃ¨rement bien sur des machines Ã  faible capacitÃ© de calcul. Il sâ€™agit dâ€™un effort pour construire des modÃ¨les de langage plus petits et moins chers.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;InterprÃ©tabilitÃ© des modÃ¨les&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Distill a publiÃ© un document intitulÃ© â€œ&lt;a href=&quot;https://distill.pub/2020/attribution-baselines/&quot;&gt;Visualizing the Impact of Feature Attribution Baselines&lt;/a&gt;â€ qui traite des &lt;a href=&quot;https://medium.com/@kartikeyabhardwaj98/integrated-gradients-for-deep-neural-networks-c114e3968eae&quot;&gt;gradients intÃ©grÃ©s&lt;/a&gt; utilisÃ©s pour interprÃ©ter les rÃ©seaux neuronaux dans divers problÃ¨mes. Dans le contexte de lâ€™interprÃ©tabilitÃ© du modÃ¨le, le dÃ©fi consiste Ã  ce que la mÃ©thode puisse garantir que le modÃ¨le ne considÃ¨re pas les caractÃ©ristiques manquantes comme Ã©tant importantes mais aussi que le modÃ¨le Ã©vite de donner aux entrÃ©es de la baseline une importance nulle (ce qui peut facilement arriver). Lâ€™auteur propose dâ€™Ã©valuer quantitativement les diffÃ©rents effets de certains choix prÃ©cÃ©demment utilisÃ©s et propose des choix de baseline qui prÃ©servent mieux la notion de manque.&lt;/p&gt;

&lt;h1 id=&quot;crÃ©ativitÃ©-et-sociÃ©tÃ©-&quot;&gt;CrÃ©ativitÃ© et sociÃ©tÃ© ğŸ¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Lâ€™inadÃ©quation des sentiments&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cette &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8952437&quot;&gt;Ã©tude&lt;/a&gt; longitudinale rÃ©vÃ¨le que les Ã©motions extraites via lâ€™utilisation dâ€™algorithmes basÃ©s sur le texte ne sont souvent pas les mÃªmes que les Ã©motions autodÃ©clarÃ©es.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;ComprÃ©hension de la dopamine et repliement des protÃ©ines&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind a rÃ©cemment publiÃ© deux articles intÃ©ressants dans Nature. Le &lt;a href=&quot;https://deepmind.com/blog/article/Dopamine-and-temporal-difference-learning-A-fruitful-relationship-between-neuroscience-and-AI&quot;&gt;premier&lt;/a&gt; vise Ã  mieux comprendre le fonctionnement de la dopamine dans le cerveau grÃ¢ce Ã  lâ€™apprentissage par renforcement. Le &lt;a href=&quot;https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery&quot;&gt;second&lt;/a&gt; est liÃ© au repliement des protÃ©ines et tente de mieux comprendre ce fonctionnement afin de pouvoir Ã©ventuellement dÃ©couvrir des traitements pour un large Ã©ventail de maladies.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*0mfEtacqGLSrmaUlNjJa0g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;â€Š&lt;a href=&quot;https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Entretiens sur le ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans une &lt;a href=&quot;https://www.youtube.com/watch?v=I-EIVlHvHRM&amp;amp;feature=youtu.be&quot;&gt;vidÃ©o&lt;/a&gt; de Wired, Refik Anadol discute du potentiel des algorithmes dâ€™apprentissage automatique pour crÃ©er des Å“uvres dâ€™art.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Lâ€™un des secteurs oÃ¹ lâ€™IA pourrait avoir un impact majeur est celui de lâ€™Ã©ducation. Dans un nouvel &lt;a href=&quot;https://engineering.stanford.edu/magazine/article/emma-brunskill-amped-education-ai?sf115875862=1&quot;&gt;Ã©pisode&lt;/a&gt; de â€œThe Future of Everythingâ€, Russ Altman et Emma Brunskill ont une discussion approfondie sur lâ€™apprentissage assistÃ© par ordinateur.&lt;/p&gt;

&lt;h1 id=&quot;outils-et-jeux-de-donnÃ©es-ï¸&quot;&gt;Outils et jeux de donnÃ©es âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;ModÃ¨les PyTorch en production&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cortex est un outil permettant dâ€™automatiser lâ€™infrastructure et de dÃ©ployer les modÃ¨les PyTorch en tant quâ€™API en production avec AWS. Pour en savoir plus sur la faÃ§on dont cela se fait, cliquez &lt;a href=&quot;https://medium.com/pytorch/how-to-build-production-software-with-pytorch-9a8725382f2a&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Visualisation des sÃ©quences de gÃ©nÃ©ration de texte&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Facebook AI a lancÃ© &lt;a href=&quot;https://ai.facebook.com/blog/vizseq-a-visual-analysis-toolkit-for-accelerating-text-generation-research/&quot;&gt;VizSeq&lt;/a&gt;, un outil qui aide Ã  Ã©valuer visuellement les sÃ©quences de textes gÃ©nÃ©rÃ©es sous des mÃ©triques comme BLUE et METEOR. Lâ€™objectif principal de cet outil est de fournir une analyse plus intuitive des ensembles de donnÃ©es textuelles via des visualisations. Pour lire lâ€™article complet, cliquez &lt;a href=&quot;https://www.aclweb.org/anthology/D19-3043.pdf&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Ff7BTxmEjUXHtYu9JkfClg.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.facebook.com/blog/vizseq-a-visual-analysis-toolkit-for-accelerating-text-generation-research/&quot;&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Reconnaissance vocale en ligne&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Facebook AI a mis en open source son outil &lt;a href=&quot;https://ai.facebook.com/blog/online-speech-recognition-with-wav2letteranywhere/&quot;&gt;wav2letter@anywhere&lt;/a&gt;. Il sâ€™agit dâ€™un framework basÃ© sur un Transformer acoustique afin dâ€™Ã©tablir un Ã©tat de lâ€™art en ligne de la reconnaissance vocale. Les principales amÃ©liorations portent sur la taille du modÃ¨le et la rÃ©duction de la latence entre lâ€™audio et la transcription, deux Ã©lÃ©ments importants pour accÃ©lÃ©rer lâ€™infÃ©rence en temps rÃ©el.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*4_2Obuu8u8l2Vtp8UMHe7Q.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;â€Š&lt;a href=&quot;https://ai.facebook.com/blog/online-speech-recognition-with-wav2letteranywhere/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;ethique-en-ia-&quot;&gt;Ethique en IA ğŸš¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Implications de lâ€™IA&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans un objectif de prÃ©venir les abus et les actions contraires Ã  lâ€™Ã©thique des systÃ¨mes dâ€™IA sur le public, lâ€™Union europÃ©enne envisage dâ€™interdire la technologie de reconnaissance faciale au public pendant cinq ans. (&lt;a href=&quot;https://www.reuters.com/article/us-eu-ai/eu-mulls-five-year-ban-on-facial-recognition-tech-in-public-areas-idUSKBN1ZF2QL&quot;&gt;Article complet&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;CoÃ»ts environnementaux des modÃ¨les de NLP modernes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cet &lt;a href=&quot;https://arxiv.org/abs/1906.02243&quot;&gt;article&lt;/a&gt; aborde les considÃ©rations Ã©nergÃ©tiques et politiques des approches modernes en NLP. Les modÃ¨les actuels reposent sur des millions/milliards de paramÃ¨tres et par consÃ©quent sur dâ€™importantes ressources de calcul. En rÃ©sulte une consommation dâ€™Ã©nergie trÃ¨s importante. Les auteurs espÃ¨rent sensibiliser davantage les chercheurs aux coÃ»ts environnementaux liÃ©s Ã  lâ€™entraÃ®nement de ces modÃ¨les de NLP.
Zachary Lipton parle dâ€™Ã©quitÃ©, dâ€™interprÃ©tabilitÃ© et des dangers du solutionnisme dans cette &lt;a href=&quot;https://c4ejournal.net/2020/01/16/zack-lipton-fairness-interpretability-and-the-dangers-of-solutionism-ethics-of-ai-in-context2020-c4ej-2/&quot;&gt;confÃ©rence&lt;/a&gt; donnÃ©e Ã  lâ€™UniversitÃ© de Toronto. Les principaux sujets tournent autour des considÃ©rations et des implications des approches dâ€™Ã©quitÃ© en matiÃ¨re de blanchiment dâ€™argent.&lt;/p&gt;

&lt;h1 id=&quot;articles-et-blog-ï¸&quot;&gt;Articles et Blog âœï¸&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;ML open source&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Thomas Wolf, responsable scientifique de Hugging Face, donne des conseils Ã  ceux qui envisagent dâ€™utiliser du code open-source ou de faire des recherches. Trouvez le fil de discussion Twitter &lt;a href=&quot;https://twitter.com/Thom_Wolf/status/1216990543533821952?s=20&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introduction Ã  lâ€™apprentissage auto-supervisÃ© en computer vision&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jeremy Howard a Ã©crit &lt;a href=&quot;https://www.fast.ai/2020/01/13/self_supervised/&quot;&gt;cet article de blog&lt;/a&gt; qui prÃ©sente briÃ¨vement le concept dâ€™apprentissage auto-supervisÃ© dans le contexte de la vision par ordinateur.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;TinyBERT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Nous avons dÃ©jÃ  constatÃ© le succÃ¨s de nombreuses variantes des modÃ¨les BERT (par exemple, &lt;a href=&quot;https://medium.com/huggingface/distilbert-8cf3380435b5&quot;&gt;DistilBERT&lt;/a&gt;) qui utilisent une certaine forme de &lt;a href=&quot;https://nervanasystems.github.io/distiller/knowledge_distillation.html&quot;&gt;distillation des connaissances&lt;/a&gt; pour rÃ©duire considÃ©rablement la taille du modÃ¨le et amÃ©liorer la vitesse. &lt;a href=&quot;https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT&quot;&gt;TinyBERT&lt;/a&gt; est une variante de BERT que ses auteurs ont appliquÃ© Ã  une solution de &lt;a href=&quot;https://towardsdatascience.com/tinybert-for-search-10x-faster-and-20x-smaller-than-bert-74cd1b6b5aec&quot;&gt;recherche par mots-clÃ©s&lt;/a&gt;. Ce projet a Ã©tÃ© inspirÃ© par cette &lt;a href=&quot;https://www.blog.google/products/search/search-language-understanding-bert/&quot;&gt;publication&lt;/a&gt; de Google. Lâ€™intÃ©rÃªt de lâ€™architecture est quâ€™elle fonctionne sur un CPU standard et peut Ãªtre utilisÃ©e pour amÃ©liorer et comprendre les rÃ©sultats de recherche.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Transfert Learning actif&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Rober Monarch a Ã©crit un &lt;a href=&quot;https://medium.com/pytorch/https-medium-com-robert-munro-active-learning-with-pytorch-2f3ee8ebec&quot;&gt;article Medium&lt;/a&gt; sur lâ€™apprentissage actif par transfert, extrait de son prochain livre, &lt;a href=&quot;https://www.manning.com/books/human-in-the-loop-machine-learning&quot;&gt;Human-in-the-loop Machine Learning&lt;/a&gt;. Il Ã©crit aussi dâ€™autres articles sur les mÃ©thodes permettant de combiner lâ€™intelligence humaine et lâ€™intelligence machine pour rÃ©soudre des problÃ¨mes. Ses propos sont accompagnÃ©s de code Pytorch.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Les sombres secrets de BERT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Anna Roger a Ã©crit cet &lt;a href=&quot;https://text-machine-lab.github.io/blog/2020/bert-secrets/&quot;&gt;article&lt;/a&gt; de blog qui parle de ce qui se passe rÃ©ellement avec un BERT bien fine-tunÃ©. Les rÃ©sultats des analyses proposÃ©es suggÃ¨rent que BERT est sÃ©vÃ¨rement surparamÃ©trÃ© et que les avantages identifiÃ©s de lâ€™auto-attention ne sont pas nÃ©cessairement aussi affirmÃ©s, en particulier en ce qui concerne les informations linguistiques qui sont encodÃ©es et utilisÃ©es pour lâ€™infÃ©rence.&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education ğŸ“&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Neural Nets for NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Graham Neubig, professeur de NLP Ã  la CMU, a publiÃ© des &lt;a href=&quot;https://www.youtube.com/playlist?list=PL8PYTP1V4I8CJ7nMxMC8aXv8WqKYwj-aJ&quot;&gt;vidÃ©os&lt;/a&gt; pour le cours â€œNeural Nets for NLPâ€ dispensÃ© ce semestre.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;DeepMath&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Vous voulez vous plonger dans les mathÃ©matiques qui se cachent derriÃ¨re les mÃ©thodes dâ€™apprentissage approfondies ? Voici une sÃ©rie de &lt;a href=&quot;https://www.youtube.com/playlist?list=PLWQvhvMdDChzsThHFe4lYAff3pu2m0v2H&quot;&gt;confÃ©rences vidÃ©o&lt;/a&gt; accueillant un large Ã©ventail dâ€™intervenants.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Cours et tutoriels Python&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google a publiÃ© le â€œGoogle IT Automation with Python Professional Certificateâ€. Pour en savoir plus sur le moyen dâ€™obtention de ce certificat cliquez &lt;a href=&quot;https://blog.google/outreach-initiatives/grow-with-google/new-certificate-help-people-grow-careers&quot;&gt;ici&lt;/a&gt; et pours en savoir plus sur les cours, cliquez &lt;a href=&quot;https://www.coursera.org/professional-certificates/google-it-automation&quot;&gt;ici&lt;/a&gt;.
Bien que le cours ne soit pas directement liÃ© Ã  la ML ou Ã  lâ€™IA, cela peut consister en un cours de base pour maÃ®triser le langage Python. Des bourses dâ€™Ã©tudes sont Ã©galement disponibles.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Voici une autre &lt;a href=&quot;https://www.youtube.com/watch?v=fMqL5vckiU0&amp;amp;list=PL-wATfeyAMNrtbkCNsLcpoAyBBRJZVlnf&quot;&gt;sÃ©rie de vidÃ©os&lt;/a&gt; intitulÃ©e â€œDeep Learning (for Audio) with Pythonâ€, qui met lâ€™accent sur lâ€™utilisation de Tensorflow et de Python pour construire des applications liÃ©es Ã  lâ€™audio/musique en tirant parti de lâ€™apprentissage profond.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew Trask a publiÃ© une &lt;a href=&quot;https://github.com/OpenMined/PySyft/tree/master/examples/tutorials&quot;&gt;sÃ©rie de tutoriels&lt;/a&gt;, pour parvenir, Ã©tape par Ã©tape, Ã  un apprentissage approfondi dÃ©centralisÃ© et respectueux de la vie privÃ©e. Tous les notebooks contiennent des implÃ©mentations de PyTorch et sont destinÃ©s aux dÃ©butants.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Etats de lâ€™art du deep learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cette &lt;a href=&quot;https://www.youtube.com/watch?v=0VH1Lim8gL8&quot;&gt;confÃ©rence&lt;/a&gt; de Lex Fridman traite de la recherche et le dÃ©veloppement rÃ©cents dans le domaine de lâ€™apprentissage approfondi. Il parle des grandes avancÃ©es sur des sujets tels que les perceptrons, les rÃ©seaux de neurones, la rÃ©tropropagation, CNN, lâ€™apprentissage profond, ImageNet, les GAN, AlphaGo et les Transformers plus rÃ©cemment. Cette confÃ©rence fait partie de la sÃ©rie â€œDeep Learningâ€ du MIT.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Groupes dâ€™Ã©tudes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Deux groupes dâ€™Ã©tude / lectures dâ€™articles conseillÃ©s par Elvis : le &lt;a href=&quot;https://twitter.com/__MLT__&quot;&gt;MLT&lt;/a&gt; et le &lt;a href=&quot;https://www.nightai.co/&quot;&gt;nightai&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Le paysage de lâ€™apprentissage par renforcement&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DÃ©couvrez avec le Dr Katja Hofmann de Microsoft, les concepts et mÃ©thodes clÃ©s de lâ€™apprentissage par renforcement dans &lt;a href=&quot;https://note.microsoft.com/MSR-Webinar-RL-Algorithm-to-Adoption-Registration-Live.html?wt.mc_id=twitter_MSR-WBNR_post_v3&quot;&gt;sa sÃ©rie dâ€™articles&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;mentions-spÃ©ciales-ï¸&quot;&gt;Mentions spÃ©ciales â­ï¸&lt;/h1&gt;

&lt;p&gt;Jettez un Å“il Ã  cette &lt;a href=&quot;https://gist.github.com/y0ast/d91d09565462125a1eb75acc65da1469&quot;&gt;implÃ©mentation PyTorch&lt;/a&gt; utilisant de ResNet-18 appliquÃ©e Ã  CIFAR-10 permettant dâ€™atteindre une prÃ©cision de 94%.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
PyTorch 1.4 est sorti ! Consultez les notes de mise Ã  jour &lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v1.4.0&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Elona Shatri a rÃ©digÃ© un &lt;a href=&quot;https://medium.com/@e.shatri1/what-is-optical-music-recognition-6515d8a53e01&quot;&gt;rÃ©sumÃ©&lt;/a&gt; sur la faÃ§on dont elle entend aborder la reconnaissance optique de la musique par un apprentissage approfondi.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le titre de cet article de blog est explicite : â€œ&lt;a href=&quot;https://cims.nyu.edu/~andrewgw/caseforbdl/&quot;&gt;Les arguments en faveur de lâ€™apprentissage approfondi bayÃ©sien&lt;/a&gt;â€.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Chris Said partage son &lt;a href=&quot;https://chris-said.io/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/&quot;&gt;expÃ©rience&lt;/a&gt; dans lâ€™optimisation de la taille des Ã©chantillons pour les tests A/B, une partie importante de la science des donnÃ©es pratiques. Les sujets abordÃ©s comprennent les coÃ»ts et les avantages des grandes tailles dâ€™Ã©chantillon et les meilleures pratiques pour les praticiens.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Neural Data Server (NDS) est un moteur de recherche dÃ©diÃ© Ã  lâ€™obtention de donnÃ©es dâ€™apprentissage par transfert. Pour en savoir plus sur la mÃ©thode cliquez &lt;a href=&quot;https://arxiv.org/abs/2001.02799&quot;&gt;ici&lt;/a&gt;,  et sur le service cliquez &lt;a href=&quot;http://aidemos.cs.toronto.edu/nds/&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Vous pouvez retrouver la prÃ©cÃ©dente newsletter &lt;a href=&quot;https://dair.ai/NLP_Newsletter_-1_-FR/&quot;&gt;ici&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous avez des jeux de donnÃ©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine Ã©dition de la newletter, nâ€™hÃ©sitez pas Ã  me contacter Ã  ellfae@gmail.com ou par message sur &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Abonnez-vous&lt;/a&gt; pour recevoir les prochains numÃ©ros dans votre boÃ®te mail.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_-2_-FR/&quot;&gt;NLP Newsletter [FR] #2: Reformer, DeepMath, ELECTRA, TinyBERT, VizSeq, Open-Sourcing ML,â€¦&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 09, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [FR] #1: Tokenizers, TensorFlow 2.1, TextVectorization, TorchIO, NLP Shortfalls,â€¦]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_-1_-FR/" />
  <id>https://dair.ai/NLP_Newsletter_#1_[FR]</id>
  <published>2020-03-09T00:00:00-05:00</published>
  <updated>2020-03-09T00:00:00-05:00</updated>
  <author>
    <name>LoÃ¯ck BOURDOIS</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2400/1*gLVPodYjYd4YaF9sJbSpjg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;avant-propos&quot;&gt;Avant-propos&lt;/h1&gt;

&lt;p&gt;Bonjour et bonne annÃ©e ! Suite Ã  de nombreuses demandes, jâ€™ai dÃ©cidÃ© de recommencer la &lt;strong&gt;Newsletter consacrÃ© au NLP&lt;/strong&gt;. Cette fois-ci, je vais la garder courte et ciblÃ©e (Ã©galement maintenue dans ce &lt;a href=&quot;https://github.com/dair-ai/nlp_newsletter&quot;&gt;reportoire&lt;/a&gt;). Lâ€™objectif de ce bulletin est de vous tenir informÃ© de certaines des avancÃ©es intÃ©ressantes et rÃ©centes liÃ©es au NLP et au ML sans prendre trop de temps sur votre journÃ©e chargÃ©e.&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications ğŸ“™&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;SystÃ¨me dâ€™IA pour la dÃ©tection de cancers du sein&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind a publiÃ© un article dans Nature intitulÃ© â€œ&lt;a href=&quot;https://www.nature.com/articles/s41586-019-1799-6&quot;&gt;International evaluation of an AI system for breast cancer screening&lt;/a&gt;â€. Le travail porte sur lâ€™Ã©valuation dâ€™un systÃ¨me dâ€™IA qui surpasse les experts humains en matiÃ¨re de dÃ©pistage du cancer du sein. Ces systÃ¨mes font toujours lâ€™objet dâ€™un dÃ©bat notamment la maniÃ¨re dont ils sont Ã©valuÃ©s. Vous trouverez &lt;a href=&quot;https://www.nature.com/articles/d41586-019-03822-8&quot;&gt;ici&lt;/a&gt; un bref rÃ©sumÃ© de lâ€™article.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Extraction dâ€™informations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pankaj Gupta a rendu publique sa thÃ¨se de doctorat intitulÃ©e â€œ&lt;a href=&quot;https://www.researchgate.net/publication/336739252_PhD_Thesis_Neural_Information_Extraction_From_Natural_Language_Text&quot;&gt;Extraction dâ€™informations neurales Ã  partir dâ€™un texte en langage naturel&lt;/a&gt;â€. Le sujet principal porte sur la maniÃ¨re dâ€™extraire efficacement les relations sÃ©mantiques dâ€™un texte en langage naturel en utilisant des approches basÃ©es sur les neurones. Cette recherche vise Ã  contribuer Ã  la construction de bases de connaissances structurÃ©es, qui peuvent Ãªtre utilisÃ©es dans une sÃ©rie dâ€™applications de NLP, telles que la recherche sur le web, les questions-rÃ©ponses, entre autres tÃ¢ches.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;AmÃ©liorer les recommandations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Des chercheurs du MIT et dâ€™IBM ont mis au point une &lt;a href=&quot;news.mit.edu/2019/finding-good-read-among-billions-of-choices-1220&quot;&gt;mÃ©thode&lt;/a&gt; (publiÃ©e lâ€™annÃ©e derniÃ¨re au NeurIPS) de catÃ©gorisation, dâ€™affichage et de recherche de documents pertinents, basÃ©e sur une combinaison de trois outils dâ€™analyse de texte trÃ¨s utilisÃ©s : la modÃ©lisation de sujets, le word embedding et le transport optimal. La mÃ©thode donne Ã©galement des rÃ©sultats prometteurs pour le tri des documents. Ces mÃ©thodes sont applicables Ã  une grande variÃ©tÃ© de scÃ©narios nÃ©cessitant des suggestions telles que les systÃ¨mes de recherche et de recommandation.&lt;/p&gt;

&lt;h1 id=&quot;crÃ©ativitÃ©-et-sociÃ©tÃ©-&quot;&gt;CrÃ©ativitÃ© et sociÃ©tÃ© ğŸ¨&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;CarriÃ¨res&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le [rapport] https://hai.stanford.edu/sites/g/files/sbiybj10986/f/ai_index_2019_report.pdf) 2019 de lâ€™AI Index suggÃ¨re quâ€™il y a plus de demandes que dâ€™offres de diplÃ´mÃ©s en AI. Toutefois, certains aspects des emplois liÃ©s Ã  lâ€™IA, tels que les transitions de carriÃ¨re et les entretiens, ne sont pas encore bien dÃ©finis.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans ce &lt;a href=&quot;https://towardsdatascience.com/how-i-found-my-current-job-3fb22e511a1f&quot;&gt;post&lt;/a&gt;, Vladimir Iglovivok dÃ©crit en dÃ©tail sa carriÃ¨re et son aventure dans le domaine de lâ€™IA. Allant de la construction de systÃ¨mes de recommandation traditionnels Ã  la construction de modÃ¨les de vision par ordinateur qui ont remportÃ© des concours sur Kaggle. Il travaille maintenant sur des vÃ©hicules autonomes Ã  Lyft, mais le chemin pour y parvenir nâ€™a pas Ã©tÃ© si facile.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous Ãªtes intÃ©ressÃ© par une carriÃ¨re dans lâ€™IA, la sociÃ©tÃ© dâ€™Andrew Ng, deeplearning.ai, a fondÃ© Workera, qui vise Ã  aider les scientifiques spÃ©cialisÃ©s dans les donnÃ©es et les ingÃ©nieurs en apprentissage machine dans leur carriÃ¨re en IA. Obtenez leur rapport officiel &lt;a href=&quot;https://workera.ai/candidates/report/&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;outils-et-jeux-de-donnÃ©es-ï¸&quot;&gt;Outils et jeux de donnÃ©es âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Un tokenizer utra rapide&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hugging Face, la start-up de NLP derriÃ¨re la librairie Transformers, dispose de tokenizers open-source, une implÃ©mentation ultra-rapide de tokenisation qui peut Ãªtre utilisÃ©e dans les pipelines. Consultez la &lt;a href=&quot;https://github.com/huggingface/tokenizers&quot;&gt;documentation&lt;/a&gt; sur lâ€™utilisation des tokenizers sur le site de GitHub.
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*BGcXk6Yf9fXGZlEtxz1hcg.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;TensorFlow 2.1 intÃ¨gre une nouvelle couche&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization&quot;&gt;TextVectorization&lt;/a&gt; qui vous permet de traiter facilement les chaÃ®nes de caractÃ¨res brutes et dâ€™effectuer efficacement la normalisation du texte, la tokenisation, la gÃ©nÃ©ration de n-grammes et lâ€™indexation du vocabulaire. Vous pouvez aussi consulter le &lt;a href=&quot;https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3&quot;&gt;Google Colab&lt;/a&gt; de FranÃ§ois Chollet qui montre comment utiliser cette fonctionnalitÃ© pour la classification de texte.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Le NLP et le ML pour la recherche&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Lâ€™un des domaines qui a fait dâ€™Ã©normes progrÃ¨s lâ€™annÃ©e derniÃ¨re est le NLP. La recherche est lâ€™un des domaines qui pourrait potentiellement bÃ©nÃ©ficier de lâ€™apprentissage par transfert.
Il existe une opportunitÃ© de construire des moteurs de recherche qui amÃ©liorent la recherche sÃ©mantique en utilisant des techniques modernes de NLP telles que les reprÃ©sentations contextualisÃ©es dâ€™un modÃ¨le basÃ© sur les Transformers comme &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT&lt;/a&gt;. Google a publiÃ© il y a quelques mois un &lt;a href=&quot;https://www.blog.google/products/search/search-language-understanding-bert/&quot;&gt;article&lt;/a&gt; sur leur blog sur la faÃ§on dont ils utilisent les modÃ¨les BERT pour amÃ©liorer et comprendre les recherches.
Si vous Ãªtes curieux de savoir comment les reprÃ©sentations contextualisÃ©es peuvent Ãªtre appliquÃ©es Ã  la recherche Ã  lâ€™aide de technologies ouvertes telles que Elasticsearch et TensorFlow, vous pouvez consulter ce &lt;a href=&quot;https://towardsdatascience.com/elasticsearch-meets-bert-building-search-engine-with-elasticsearch-and-bert-9e74bf5b4cf2&quot;&gt;billet&lt;/a&gt; ou celui-&lt;a href=&quot;https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&quot;&gt;ci&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Analyse dâ€™images mÃ©dicales&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/fepegar/torchio&quot;&gt;TorchIO&lt;/a&gt; est un package Python basÃ© PyTorch. TorchIO offre des fonctionnalitÃ©s permettant de lire et dâ€™Ã©chantillonner facilement et efficacement des images mÃ©dicales en 3D. Les fonctionnalitÃ©s comprennent des transformations spatiales pour lâ€™augmentation et le prÃ©traitement des donnÃ©es.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/0*FSPuSC8TK9X-NQ2q.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/fepegar/torchio&quot;&gt;source&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;ethique-en-ia-&quot;&gt;Ethique en IA ğŸš¨&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Comportement frauduleux dans la communautÃ© du ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les gagnants de la premiÃ¨re place dâ€™un concours Kaggle ont Ã©tÃ© disqualifiÃ©s pour activitÃ© frauduleuse. Lâ€™Ã©quipe a utilisÃ© des tactiques intelligentes mais irresponsables et inacceptables pour remporter la premiÃ¨re place du concours. Lâ€™histoire complÃ¨te est disponible &lt;a href=&quot;https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/125436&quot;&gt;ici&lt;/a&gt;. Cette histoire met en Ã©vidence un des nombreux comportements graves et inacceptables que la communautÃ© de lâ€™apprentissage machine veut attÃ©nuer. Lâ€™utilisation correcte et Ã©thique des technologies de ML est la seule faÃ§on de progresser.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Biais concernant le genre dans la traduction automatique&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sur la question de savoir si les systÃ¨mes de traduction automatique reflÃ¨tent des prÃ©jugÃ©s sexistes, un groupe de chercheurs a publiÃ© cet &lt;a href=&quot;https://arxiv.org/abs/1809.02208&quot;&gt;article&lt;/a&gt; prÃ©sentant une Ã©tude de cas utilisant Google Translate. Lâ€™un des rÃ©sultats revendiquÃ©s par les auteurs est que Google Translate â€œprÃ©sente une forte tendance aux dÃ©fauts masculins, en particulier pour les domaines liÃ©s Ã  une rÃ©partition dÃ©sÃ©quilibrÃ©e des sexes, comme les emplois dans les STEM (Science, Technology, Engineering and Mathematics)â€.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Biais en ML et Ã©quitÃ©&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous voulez vous familiariser avec lâ€™Ã©thique et lâ€™Ã©quitÃ© en matiÃ¨re dâ€™IA, vous pouvez Ã©couter ce &lt;a href=&quot;https://twimlai.com/twiml-talk-336-trends-in-fairness-and-ai-ethics-with-timnit-gebru/&quot;&gt;podcast&lt;/a&gt; avec Timnit Gebru et animÃ© par TWIML.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Timnit est un chercheur dans le domaine de lâ€™Ã©quitÃ© en ML qui, avec Eun Seo Jo, a publiÃ© un &lt;a href=&quot;https://arxiv.org/abs/1912.10389&quot;&gt;article&lt;/a&gt; dans lequel ils identifient cinq approches clÃ©s dans les pratiques de collecte de donnÃ©es pouvant ainsi fournir des mÃ©thodes plus fiables dans le domaine du  ML socioculturel. Cela pourrait potentiellement conduire Ã  des mÃ©thodes de collecte de donnÃ©es plus systÃ©matiques, issues de la recherche collaborative interdisciplinaire.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sina Fazelpour et Zachary Lipton ont rÃ©cemment publiÃ© un &lt;a href=&quot;http://zacklipton.com/media/papers/fairness-non-ideal-fazelpour-lipton-2020.pdf&quot;&gt;article&lt;/a&gt; dans lequel ils affirment quâ€™en raison de la nature non idÃ©ale de notre monde, il est possible quâ€™un ML Ã©quitable basÃ© sur la pensÃ©e idÃ©ale puisse potentiellement conduire Ã  des politiques et des interventions mal orientÃ©es. En fait, leur analyse dÃ©montre â€œque les lacunes des algorithmes Ã  vocation Ã©quitable proposÃ©s reflÃ¨tent les problÃ¨mes plus larges rencontrÃ©s par lâ€™approche idÃ©aleâ€.&lt;/p&gt;

&lt;h1 id=&quot;articles-et-blog-ï¸&quot;&gt;Articles et Blog âœï¸&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Lacunes en NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Benjamin Heinzerling a publiÃ© un &lt;a href=&quot;https://thegradient.pub/nlps-clever-hans-moment-has-arrived/&quot;&gt;article&lt;/a&gt; dans The Gradient oÃ¹ il aborde les domaines dans lesquels le NLP est dÃ©faillant, comme la comprÃ©hension des arguments et le raisonnement. Benjamin fait rÃ©fÃ©rence Ã  un &lt;a href=&quot;https://www.aclweb.org/anthology/P19-1459/&quot;&gt;article&lt;/a&gt; rÃ©cent de Nivin &amp;amp; Kao qui remet en question les capacitÃ©s de lâ€™apprentissage par transfert et les modÃ¨les linguistiques pour une comprÃ©hension poussÃ©e du langage naturel.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Temps forts du NLP et du ML en 2019&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pour la nouvelle annÃ©e, Elvis (crÃ©ateur de cette newsletter et du site dair.ai )a publiÃ© un &lt;a href=&quot;https://medium.com/dair-ai/nlp-year-in-review-2019-fb8d523bcb19&quot;&gt;document&lt;/a&gt; sur certains des points intÃ©ressants du NLP et du ML quâ€™il a rencontrÃ© en 2019.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Ruder a Ã©galement Ã©crit rÃ©cemment un &lt;a href=&quot;https://ruder.io/research-highlights-2019/&quot;&gt;article&lt;/a&gt; dÃ©taillÃ© sur les dix principales orientations de recherche en ML et en NLP quâ€™il a trouvÃ©es percutantes en 2019. Parmi la liste figurent des sujets tels que lâ€™entraÃ®nement universel non supervisÃ©e, lâ€™augmentation des modÃ¨les prÃ©-entrainÃ©s, les Transformers, entre autres.
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/0*8zoPc5OnYERIaaMP.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;â€œVideoBERT (&lt;/em&gt;[&lt;em&gt;Sun et al., 2019&lt;/em&gt;](https://arxiv.org/abs/1904.01766 une rÃ©cente variante multimodale de BERT qui gÃ©nÃ¨re des â€œtokensâ€ vidÃ©o en fonction dâ€™une recette (ci-dessus) et prÃ©dit les futurs tokens Ã  diffÃ©rentes Ã©chelles de temps en fonction dâ€™un tokens vidÃ©o (ci-dessous).â€â€Šâ€”*â€Š&lt;a href=&quot;https://arxiv.org/pdf/1904.01766.pdf&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI Research publie un &lt;a href=&quot;https://ai.googleblog.com/2020/01/google-research-looking-back-at-2019.html&quot;&gt;rÃ©sumÃ©&lt;/a&gt; des recherches quâ€™ils ont menÃ©es au cours de lâ€™annÃ©e et les futures orientations de recherche auxquelles ils prÃªtent attention.&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education ğŸ“&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;DÃ©mocratisation des cours dâ€™IA&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans un effort pour dÃ©mocratiser lâ€™enseignement de lâ€™IA et pour Ã©duquer les masses sur les implications de la technologie de lâ€™IA, lâ€™UniversitÃ© dâ€™Helsinki sâ€™est associÃ©e Ã  Reaktor pour publier un cours gratuit couvrant les bases de lâ€™IA. Ce &lt;a href=&quot;https://www.elementsofai.com/&quot;&gt;cours&lt;/a&gt; sâ€™intitule â€œElements de lâ€™IAâ€ et aborde des sujets tels que lâ€™Ã©thique de lâ€™IA, la philosophie de lâ€™IA, les rÃ©seaux de neurones, la rÃ¨gle de Bayes naÃ¯ve, entre autres sujets fondamentaux.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Le Stanford CS224N est de retour avec une nouvelle&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;session&lt;/a&gt; de leurs cours â€œNatural Language Processing with Deep Learningâ€. Le cours a officiellement dÃ©butÃ© le 7 janvier de cette annÃ©e. Si vous souhaitez le suivre, rendez-vous sur leur site web pour obtenir le programme complet, des diapositives, des vidÃ©os, des suggestions de lecture de documents, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Machine Learning avec les mÃ©thodes Ã  noyaux&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les mÃ©thodes Ã  noyaux telles que lâ€™ACP et les K-means existent depuis un certain temps et ont Ã©tÃ© appliquÃ©es avec succÃ¨s pour une grande variÃ©tÃ© dâ€™applications telles que les graphes ou les sÃ©quences biologiques. Sur le sujet, pouvez consulter cette sÃ©rie de &lt;a href=&quot;http://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/slides/master2017/master2017.pdf&quot;&gt;diapositives&lt;/a&gt; de Partis Tech couvrant un large Ã©ventail de mÃ©thodes du noyau et leur fonctionnement interne. Vous pouvez aussi jeter un Å“il au &lt;a href=&quot;https://francisbach.com/cursed-kernels/&quot;&gt;blog&lt;/a&gt; de Francis Bach qui traite de certains aspects des mÃ©thodes du noyau et dâ€™autres sujets liÃ©s Ã  lâ€™apprentissage machine.&lt;/p&gt;

&lt;h1 id=&quot;mentions-spÃ©ciales-ï¸&quot;&gt;Mentions spÃ©ciales â­ï¸&lt;/h1&gt;

&lt;p&gt;Le &lt;a href=&quot;https://hunch.net/&quot;&gt;blog&lt;/a&gt; tenu par John Langford qui aborde les aspects thÃ©oriques de lâ€™apprentissage machine.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous souhaitez apprendre Ã  concevoir et Ã  construire des applications de ML et les amener jusquâ€™Ã  la production, vous pouvez lire le &lt;a href=&quot;https://www.amazon.com/Building-Machine-Learning-Powered-Applications/dp/149204511X/&quot;&gt;livre&lt;/a&gt; dâ€™Emmanuel Ameisen.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Si vous avez des jeux de donnÃ©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine Ã©dition de la newletter, nâ€™hÃ©sitez pas Ã  me contacter Ã  ellfae@gmail.com ou par message sur &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Abonnez-vous&lt;/a&gt; pour recevoir les prochains numÃ©ros dans votre boÃ®te mail.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_-1_-FR/&quot;&gt;NLP Newsletter [FR] #1: Tokenizers, TensorFlow 2.1, TextVectorization, TorchIO, NLP Shortfalls,â€¦&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 09, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP ç®€æŠ¥ï¼ˆIssue#6ï¼‰]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP%E7%AE%80%E6%8A%A5/" />
  <id>https://dair.ai/NLPç®€æŠ¥</id>
  <published>2020-03-02T00:00:00-06:00</published>
  <updated>2020-03-02T00:00:00-06:00</updated>
  <author>
    <name>kaiyuan</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*vWICxAehSy3xOnqGIXtpoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;æ¬¢è¿æ¥åˆ° NLP æ—¶äº‹ç®€æŠ¥ç¬¬å…­æœŸï¼å…¨æ–‡è¾ƒé•¿ï¼Œå»ºè®®æ”¶è—ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;å¦‚æœæƒ³è®©è‡ªå·±æœ‰è¶£çš„ç ”ç©¶/é¡¹ç›®å‡ºç°åœ¨NLPç®€æŠ¥ä¸­ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·åå°ç•™è¨€è”ç³»æˆ‘&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;æ¥çœ‹çœ‹éƒ½æœ‰å“ªäº›å†…å®¹ï¼Œenjoy~
@[TOC]&lt;/p&gt;
&lt;h2 id=&quot;1publications-&quot;&gt;1ã€Publications ğŸ“™&lt;/h2&gt;
&lt;h4 id=&quot;11-bertç»¼è¿°&quot;&gt;1.1 BERTç»¼è¿°&lt;/h4&gt;
&lt;p&gt;åŸºäºTransformerçš„æ¨¡å‹å·²ç»è¢«è¯å®å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†ä»åºåˆ—æ ‡è®°åˆ°é—®é¢˜è§£ç­”ç­‰ä¸åŒç±»å‹çš„NLPä»»åŠ¡ï¼Œå…¶ä¸­ä¸€ç§ç§°ä¸º&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT&lt;/a&gt;çš„æ¨¡å‹å¾—åˆ°äº†å¹¿æ³›ä½¿ç”¨ï¼Œä½†æ˜¯åƒå…¶ä»–é‡‡ç”¨æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¨¡å‹ä¸€æ ·ï¼Œæˆ‘ä»¬å¯¹å…¶å†…éƒ¨è¿ä½œçŸ¥ä¹‹ç”šå°‘ã€‚ ä¸€ç¯‡åä¸ºã€Š &lt;a href=&quot;https://arxiv.org/abs/2002.12327&quot;&gt;A Primer in BERTology: What we know about how BERT works&lt;/a&gt;ã€‹çš„æ–°è®ºæ–‡æ—¨åœ¨å›ç­”ä¸€äº›æœ‰å…³BERTä¸ºä»€ä¹ˆåœ¨è¿™ä¹ˆå¤šNLPä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½çš„é—®é¢˜ã€‚ è®ºæ–‡çš„å†…å®¹åŒ…æ‹¬ï¼šBERTå­¦ä¹ çš„çŸ¥è¯†ç±»å‹åŠå…¶è¡¨ç¤ºçš„ä½ç½®ï¼ŒBERTæ˜¯å¦‚ä½•å­¦ä¹ çŸ¥è¯†çš„ï¼Œä»¥åŠç ”ç©¶äººå‘˜å¦‚ä½•ä½¿ç”¨å…¶ä»–æ–¹æ³•æ¥æ”¹è¿›å®ƒï¼Œç­‰ç­‰ã€‚&lt;/p&gt;
&lt;h4 id=&quot;12-t5&quot;&gt;1.2 T5&lt;/h4&gt;
&lt;p&gt;Google AIæœ€è¿‘å‘å¸ƒäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ä»NLPè¿ç§»å­¦ä¹ æ¨¡å‹ä¸­å­¦åˆ°çš„æ‰€æœ‰çŸ¥è¯†å’Œç»éªŒæ±‡æ€»åˆ°ä¸€ä¸ªç§°ä¸º&lt;code class=&quot;highlighter-rouge&quot;&gt;Text-to-Text Transfer Transformerï¼ˆT5ï¼‰&lt;/code&gt;çš„ç»Ÿä¸€æ¡†æ¶ä¸­ã€‚ è¿™é¡¹å·¥ä½œå»ºè®®å¤§å¤šæ•°NLPä»»åŠ¡å¯ä»¥ç”¨æ–‡æœ¬åˆ°æ–‡æœ¬çš„æ ¼å¼æ¥è¡¨ç¤ºï¼Œè¿™è¡¨æ˜è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯æ–‡æœ¬ã€‚ ä½œè€…å£°ç§°ï¼Œè¿™ç§â€œæ¡†æ¶ä¸ºé¢„è®­ç»ƒå’Œå¾®è°ƒæä¾›äº†ä¸€è‡´çš„è®­ç»ƒç›®æ ‡â€ã€‚ T5æœ¬è´¨ä¸Šæ˜¯ä¸€ç§ç¼–ç å™¨/è§£ç å™¨Transformerï¼Œç‰¹åˆ«æ˜¯å¯¹æ¨¡å‹çš„&lt;code class=&quot;highlighter-rouge&quot;&gt;attention&lt;/code&gt;ç»„ä»¶è¿›è¡Œäº†å„ç§æ”¹è¿›ã€‚ è¯¥æ¨¡å‹åœ¨æ–°å‘å¸ƒçš„åä¸º&lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/c4&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Colossal Clean Crawled Corpusï¼ˆC4ï¼‰&lt;/code&gt;&lt;/a&gt;çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶åœ¨NLPä»»åŠ¡ï¼ˆä¾‹å¦‚æ‘˜è¦ï¼Œé—®é¢˜å›ç­”å’Œæ–‡æœ¬åˆ†ç±»ï¼‰ä¸Šè·å¾—äº†SOTAç»“æœã€‚
&lt;img src=&quot;https://img-blog.csdnimg.cn/20200303100141454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0thaXl1YW5fc2p0dQ==,size_16,color_FFFFFF,t_70&quot; alt=&quot;åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;13-12åˆ1å¤šä»»åŠ¡è§†è§‰å’Œè¯­è¨€è¡¨ç¤ºå­¦ä¹ &quot;&gt;1.3 12åˆ1ï¼šå¤šä»»åŠ¡è§†è§‰å’Œè¯­è¨€è¡¨ç¤ºå­¦ä¹ &lt;/h4&gt;
&lt;p&gt;å½“å‰çš„ç ”ç©¶ä½¿ç”¨ç‹¬ç«‹çš„ä»»åŠ¡å’Œæ•°æ®é›†æ¥æ‰§è¡Œè§†è§‰å’Œè¯­è¨€ç ”ç©¶ï¼Œå³ä½¿æ‰§è¡Œè¿™äº›ä»»åŠ¡æ‰€éœ€çš„â€œå…·æœ‰è§†è§‰åŸºç¡€çš„è¯­è¨€ç†è§£æŠ€èƒ½â€ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ ä¸€ç¯‡æ–°è®ºæ–‡ï¼ˆå°†åœ¨CVPRä¸Šå‘è¡¨ï¼‰ï¼Œ&lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;ã€Š12-in-1: Multi-Task Vision and Language Representation Learningã€‹&lt;/a&gt;ï¼Œæå‡ºäº†ä¸€ç§å¤§è§„æ¨¡å¤šä»»åŠ¡æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°å»ºæ¨¡å¹¶å…±åŒè®­ç»ƒè§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä»¥ç”Ÿæˆæ›´é€šç”¨çš„è§†è§‰å’Œè¯­è¨€æ¨¡å‹ã€‚ è¯¥æ¨¡å‹å‡å°äº†å‚æ•°å¤§å°ï¼Œå¹¶ä¸”åœ¨åŸºäºå­—å¹•çš„å›¾åƒæ£€ç´¢å’Œå¯è§†é—®é¢˜è§£ç­”ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚
&lt;img src=&quot;https://img-blog.csdnimg.cn/2020030310123754.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0thaXl1YW5fc2p0dQ==,size_16,color_FFFFFF,t_70&quot; alt=&quot;åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;14-bertæ–‡æœ¬è¡¨ç¤ºçš„è·¨æ¨¡å¼å¯ä¼ é€’æ€§&quot;&gt;1.4 BERTæ–‡æœ¬è¡¨ç¤ºçš„è·¨æ¨¡å¼å¯ä¼ é€’æ€§&lt;/h4&gt;
&lt;p&gt;ä¼—å¤šç ”ç©¶äººå‘˜å’Œåˆä½œè€…å‘è¡¨äº†ä¸€ç¯‡è®ºæ–‡ï¼Œ&lt;a href=&quot;https://arxiv.org/abs/2002.10832&quot;&gt;BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations&lt;/a&gt;ï¼Œæ—¨åœ¨å›ç­”BERTæ¨¡å‹æ˜¯å¦å¯ä»¥äº§ç”Ÿå¯ä»¥æ¨å¹¿åˆ°è¯¸å¦‚è§†è§‰ä¹‹ç±»çš„æ–‡æœ¬ä¹‹å¤–çš„å…¶ä»–æ–¹å¼çš„é—®é¢˜ã€‚ ä»–ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸ºBERT-gençš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨äº†å•æ¨¡æ€æˆ–å¤šæ¨¡æ€è¡¨ç¤ºï¼Œå¹¶åœ¨è§†è§‰é—®é¢˜ç”Ÿæˆæ•°æ®é›†ä¸Šè·å¾—äº†æ”¹è¿›çš„ç»“æœã€‚
&lt;img src=&quot;https://img-blog.csdnimg.cn/20200303101728570.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0thaXl1YW5fc2p0dQ==,size_16,color_FFFFFF,t_70&quot; alt=&quot;åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;2creativity-and-society-&quot;&gt;2ã€Creativity and Society ğŸ¨&lt;/h2&gt;
&lt;h4 id=&quot;21-the-next-decade-in-ai&quot;&gt;2.1 The Next Decade in AI&lt;/h4&gt;
&lt;p&gt;Gary Marcusæœ€è¿‘å‘è¡¨äº†ä¸€ç¯‡è®ºæ–‡ï¼Œ&lt;a href=&quot;https://arxiv.org/abs/2002.06177&quot;&gt;The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence&lt;/a&gt;ï¼Œä»–åœ¨å…¶ä¸­è§£é‡Šäº†ä¸€ç³»åˆ—æ­¥éª¤ï¼Œä»–è®¤ä¸ºï¼Œæˆ‘ä»¬åº”è¯¥é‡‡å–è¿™äº›æ­¥éª¤æ¥æ„å»ºæ›´å¼ºå¤§çš„AIç³»ç»Ÿã€‚ Garyåœ¨è®ºæ–‡ä¸­çš„ä¸­å¿ƒæ€æƒ³æ˜¯ç€é‡äºæ„å»ºç”±è®¤çŸ¥æ¨¡å‹æŒ‡å¯¼çš„æ··åˆå’ŒçŸ¥è¯†é©±åŠ¨ç³»ç»Ÿï¼Œè€Œä¸æ˜¯ç€é‡äºæ„å»ºéœ€è¦æ›´å¤šæ•°æ®å’Œè®¡ç®—èƒ½åŠ›çš„å¤§å‹ç³»ç»Ÿã€‚&lt;/p&gt;
&lt;h4 id=&quot;22-2020å¹´çš„10ç§çªç ´æ€§æŠ€æœ¯&quot;&gt;2.2 2020å¹´çš„10ç§çªç ´æ€§æŠ€æœ¯&lt;/h4&gt;
&lt;p&gt;MIT Technology Review å‡ºç‰ˆäº†ä¸€ä»½æ¸…å•ï¼Œåˆ—å‡ºäº†ä»–ä»¬ç¡®å®šçš„&lt;a href=&quot;https://www.technologyreview.com/lists/technologies/2020/&quot;&gt;10é¡¹çªç ´&lt;/a&gt;ï¼Œè¿™äº›çªç ´å°†å¯¹è§£å†³å¯èƒ½æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»å’Œå·¥ä½œæ–¹å¼çš„é—®é¢˜äº§ç”Ÿå½±å“ã€‚ åˆ—è¡¨å¦‚ä¸‹ï¼ˆæ’åä¸åˆ†å…ˆåï¼‰ï¼šunhackable internet,  hyper-personalized medicine, digital money, anti-aging drugs, AI-discovered molecules, satellite mega-constellations, quantum supremacy, Tiny AI, differential privacy, and climate attribution.&lt;/p&gt;
&lt;h4 id=&quot;23-é‡æ–°è€ƒè™‘æœºå™¨å­¦ä¹ çš„å‘è¡¨è¿‡ç¨‹&quot;&gt;2.3 é‡æ–°è€ƒè™‘æœºå™¨å­¦ä¹ çš„å‘è¡¨è¿‡ç¨‹&lt;/h4&gt;
&lt;p&gt;Yoshua Bengioæœ€è¿‘å†™äº†&lt;a href=&quot;https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/&quot;&gt;å…³äºMLå‡ºç‰ˆç‰©å¿«èŠ‚å¥å‘å±•çš„æ‹…å¿§&lt;/a&gt;ã€‚ ä¸»è¦æ‹…å¿ƒçš„æ˜¯ï¼Œç”±äºå‘å¸ƒçš„é€Ÿåº¦å¿«ï¼Œå¾ˆå¤šè®ºæ–‡éƒ½åŒ…å«é”™è¯¯å¹¶ä¸”åªæ˜¯æ¸è¿›å¼å‡ºç‰ˆï¼Œè€ŒèŠ±è´¹æ›´å¤šçš„æ—¶é—´å¹¶ç¡®ä¿ä¸¥è°¨ï¼ˆè¿™æ˜¯å¤šå¹´ä»¥å‰çš„å·¥ä½œæ–¹å¼ï¼‰ä¼¼ä¹æ­£åœ¨æ¶ˆå¤±ã€‚ æœ€é‡è¦çš„æ˜¯ï¼Œå­¦ç”Ÿæ˜¯é‚£äº›å¿…é¡»åº”å¯¹è¿™ç§å‹åŠ›å’Œå‹åŠ›çš„è´Ÿé¢åæœçš„äººã€‚ ä¸ºäº†è§£å†³è¿™ç§æƒ…å†µï¼ŒBengioè°ˆè®ºäº†ä»–çš„è¡ŒåŠ¨ï¼Œä»¥å¸®åŠ©å‡æ…¢ç ”ç©¶å‡ºç‰ˆç‰©çš„å‘å±•ï¼Œä»¥é€ ç¦ç§‘å­¦ã€‚&lt;/p&gt;
&lt;h2 id=&quot;3tools-and-datasets-ï¸&quot;&gt;3ã€Tools and Datasets âš™ï¸&lt;/h2&gt;
&lt;h4 id=&quot;31-allennlpä¸­çš„pointergeneratorç½‘ç»œå®ç°&quot;&gt;3.1 AllenNLPä¸­çš„PointerGeneratorç½‘ç»œå®ç°&lt;/h4&gt;
&lt;p&gt;Pointer-Generatorç½‘ç»œæ—¨åœ¨å¢å¼ºç”¨äºæ”¹è¿›&lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;æŠ½è±¡æ‘˜è¦&lt;/a&gt;çš„åºåˆ—åˆ°åºåˆ—æ³¨æ„æ¨¡å‹ã€‚ å¦‚æœæ‚¨å¸Œæœ›ä½¿ç”¨AllenNLPè¿›è¡ŒPointer-GeneratoræŠ½è±¡æ‘˜è¦ï¼ŒKundan Krishnaå·²å¼€å‘äº†ä¸€ä¸ªåº“ï¼Œ&lt;a href=&quot;https://github.com/kukrishna/pointer-generator-pytorch-allennlp&quot;&gt;PointerGenerator network implementation in AllenNLP&lt;/a&gt;ï¼Œè¯¥åº“å¯è®©æ‚¨è¿è¡Œé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹ï¼ˆæä¾›ï¼‰æˆ–è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ã€‚
&lt;img src=&quot;https://img-blog.csdnimg.cn/20200303103010155.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0thaXl1YW5fc2p0dQ==,size_16,color_FFFFFF,t_70&quot; alt=&quot;åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;32-ä¸åŒè¯­è¨€çš„qa&quot;&gt;3.2 ä¸åŒè¯­è¨€çš„QA&lt;/h4&gt;
&lt;p&gt;éšç€Transformeræ¨¡å‹çš„å‘å±•ä»¥åŠå®ƒä»¬å¯¹ä»¥å…¶ä»–è¯­è¨€æ‰§è¡Œçš„å¤§è§„æ¨¡NLPä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼Œäººä»¬ä»˜å‡ºäº†å·¨å¤§çš„åŠªåŠ›æ¥å‘å¸ƒä¸åŒè¯­è¨€çš„ä¸åŒç±»å‹çš„æ•°æ®é›†ã€‚ ä¾‹å¦‚ï¼ŒSebastian Ruderå…±äº«äº†å¯ç”¨äºä¸åŒè¯­è¨€é—®ç­”ç ”ç©¶çš„æ•°æ®é›†åˆ—è¡¨ï¼š&lt;a href=&quot;https://www.aclweb.org/anthology/W18-2605/&quot;&gt;DuReader&lt;/a&gt;ï¼Œ&lt;a href=&quot;https://arxiv.org/abs/1909.07005&quot;&gt;KorQuAD&lt;/a&gt;ï¼Œ&lt;a href=&quot;https://arxiv.org/abs/1912.09723&quot;&gt;SberQuAD&lt;/a&gt;ï¼Œ&lt;a href=&quot;https://arxiv.org/abs/2002.06071&quot;&gt;FQuAD&lt;/a&gt;ï¼Œ&lt;a href=&quot;https://arxiv.org/abs/1906.05394&quot;&gt;Arabic-SQuAD&lt;/a&gt;ï¼Œ&lt;a href=&quot;https://github.com/crux82/squad-it&quot;&gt;SQuAD-it&lt;/a&gt;å’Œ&lt;a href=&quot;https://arxiv.org/abs/1912.05200v2&quot;&gt;Spanish SQuAD&lt;/a&gt;ã€‚&lt;/p&gt;
&lt;h4 id=&quot;33-pytorch-lightning&quot;&gt;3.3 PyTorch Lightning&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09&quot;&gt;PyTorch Lightning&lt;/a&gt;æ˜¯ä¸€ç§å¯è®©æ‚¨æŠ½è±¡åŒ–å¯èƒ½éœ€è¦è®¾ç½®GPU / TPUè®­ç»ƒå’Œä½¿ç”¨16ä½ç²¾åº¦çš„è®­ç»ƒçš„å·¥å…·ã€‚ ä½¿è¿™äº›äº‹æƒ…æ­£å¸¸å·¥ä½œå¯èƒ½ä¼šå˜å¾—å¾ˆä¹å‘³ï¼Œä½†æ˜¯å¥½æ¶ˆæ¯æ˜¯PyTorch Lightningç®€åŒ–äº†æ­¤è¿‡ç¨‹ï¼Œå¹¶å…è®¸æ‚¨åœ¨å¤šGPUå’ŒTPUä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè€Œæ— éœ€æ›´æ”¹å½“å‰çš„PyTorchä»£ç ã€‚&lt;/p&gt;
&lt;h4 id=&quot;34-tf2ä¸­çš„å›¾ç¥ç»ç½‘ç»œ&quot;&gt;3.4 TF2ä¸­çš„å›¾ç¥ç»ç½‘ç»œ&lt;/h4&gt;
&lt;p&gt;Microsoftç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº†ä¸€ä¸ªåº“ï¼Œè¯¥åº“æä¾›å¯¹&lt;a href=&quot;https://github.com/microsoft/tf2-gnn&quot;&gt;è®¸å¤šä¸åŒçš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¶æ„çš„å®ç°&lt;/a&gt;çš„è®¿é—®ã€‚ è¯¥åº“åŸºäºTensorFlow 2ï¼Œè¿˜æä¾›å¯ç›´æ¥åœ¨è®­ç»ƒ/è¯„ä¼°å¾ªç¯ä¸­ä½¿ç”¨çš„æ•°æ®æ•´ç†æ¨¡å—ã€‚&lt;/p&gt;
&lt;h4 id=&quot;35-é¢„è®­ç»ƒ-smallberta&quot;&gt;3.5 é¢„è®­ç»ƒ SmallBERTa&lt;/h4&gt;
&lt;p&gt;ä½ æ˜¯å¦æ›¾ç»æƒ³ä»å¤´å¼€å§‹è®­ç»ƒè‡ªå·±çš„è¯­è¨€æ¨¡å‹ï¼Œä½†æ˜¯æ²¡æœ‰è¶³å¤Ÿçš„èµ„æºæ¥è®­ç»ƒå‘¢ï¼Ÿ å¦‚æœæ˜¯è¿™æ ·ï¼Œé‚£ä¹ˆAditya Malteæä¾›äº†ä¸€ç§ä¼˜é›…çš„æ–¹å¼ï¼Œå®ƒæ•™æ‚¨å¦‚ä½•&lt;a href=&quot;https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb&quot;&gt;ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†ä»å¤´è®­ç»ƒè¯­è¨€æ¨¡å‹&lt;/a&gt;ã€‚&lt;/p&gt;
&lt;h4 id=&quot;36-cluedatasetsearch&quot;&gt;3.6 CLUEDatasetSearch&lt;/h4&gt;
&lt;p&gt;CLUE benchmarkå›¢é˜Ÿæ•´ç†äº†æ‰€æœ‰ä¸­æ–‡NLPæ•°æ®é›†ï¼Œé™„å¸¸ç”¨è‹±æ–‡NLPæ•°æ®é›†ï¼Œå¯ä»¥åœ¨&lt;a href=&quot;https://github.com/CLUEbenchmark/CLUEDatasetSearch#qa&quot;&gt;CLUEbenchmark/CLUEDatasetSearch&lt;/a&gt;æ‰¾åˆ°ã€‚&lt;/p&gt;

&lt;h2 id=&quot;4ethics-in-ai-&quot;&gt;4ã€Ethics in AI ğŸš¨&lt;/h2&gt;
&lt;h4 id=&quot;41-é¢éƒ¨è¡¨æƒ…ä¸çœŸå®æƒ…æ„Ÿ&quot;&gt;4.1 é¢éƒ¨è¡¨æƒ…ä¸çœŸå®æƒ…æ„Ÿ&lt;/h4&gt;
&lt;p&gt;ä¸€æ®µæ—¶é—´ä»¥æ¥ï¼Œè®¸å¤šç ”ç©¶äººå‘˜å’Œå…¬å¸å·²å°è¯•å»ºç«‹å¯ç†è§£å¹¶å¯ä»¥è¯†åˆ«æ–‡æœ¬æˆ–è§†è§‰ç¯å¢ƒä¸­çš„æƒ…ç»ªçš„AIæ¨¡å‹ã€‚ ä¸€ç¯‡æ–°æ–‡ç« é‡æ–°å¼•å‘äº†è¾©è®ºï¼Œ&lt;a href=&quot;https://www.nature.com/articles/d41586-020-00507-5&quot;&gt;Why faces donâ€™t always tell the truth about feelings&lt;/a&gt;ï¼Œå³æ—¨åœ¨ç›´æ¥ä»é¢éƒ¨å›¾åƒè¯†åˆ«æƒ…ç»ªçš„AIæŠ€æœ¯åšå¾—ä¸å¥½ã€‚ è¯¥é¢†åŸŸçš„æ°å‡ºå¿ƒç†å­¦å®¶æå‡ºçš„ä¸»è¦è®ºç‚¹æ˜¯ï¼Œæ²¡æœ‰è¯æ®è¡¨æ˜å¯ä»¥ä»…ä»é¢éƒ¨å›¾åƒè¿›è¡Œæƒ…æ„Ÿæ£€æµ‹çš„é€šç”¨è¡¨è¾¾æ–¹å¼ã€‚ å®ƒå°†éœ€è¦ä¸€ä¸ªæ¨¡å‹æ›´å¥½åœ°äº†è§£äººæ ¼ç‰¹å¾ï¼Œèº«ä½“åŠ¨ä½œç­‰ï¼Œæ‰èƒ½çœŸæ­£æ›´æ¥è¿‘åœ°æ›´å‡†ç¡®åœ°æ£€æµ‹äººç±»æ‰€è¡¨ç°å‡ºçš„æƒ…ç»ªã€‚&lt;/p&gt;
&lt;h4 id=&quot;42-å·®å¼‚éšç§å’Œè”åˆå­¦ä¹ &quot;&gt;4.2 å·®å¼‚éšç§å’Œè”åˆå­¦ä¹ &lt;/h4&gt;
&lt;p&gt;æ„å»ºAIç³»ç»Ÿæ—¶çš„é“å¾·è€ƒé‡ä¹‹ä¸€æ˜¯ç¡®ä¿éšç§ã€‚ å½“å‰ï¼Œè¿™å¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼æ¥å®ç°ï¼Œå³ä½¿ç”¨&lt;a href=&quot;https://www.youtube.com/watch?v=MOcTGM_UteM&quot;&gt;å·®å¼‚éšç§æˆ–è”åˆå­¦ä¹ &lt;/a&gt;ã€‚ å¦‚æœä½ æƒ³äº†è§£æ›´å¤šæœ‰å…³è¿™äº›ä¸»é¢˜çš„ä¿¡æ¯ï¼ŒJordan Harrodåœ¨æ­¤è§†é¢‘ä¸­ä¸ºæˆ‘ä»¬åšäº†å¾ˆå¥½çš„ä»‹ç»ï¼Œå…¶ä¸­è¿˜åŒ…æ‹¬ä½¿ç”¨Colab notebookçš„åŠ¨æ‰‹å®è·µè¯¾ç¨‹ã€‚&lt;/p&gt;
&lt;h2 id=&quot;5articles-and-blog-posts-ï¸&quot;&gt;5ã€Articles and Blog posts âœï¸&lt;/h2&gt;
&lt;h4 id=&quot;51-æ·±å…¥reformer&quot;&gt;5.1 æ·±å…¥Reformer&lt;/h4&gt;
&lt;p&gt;Madison Mayæ’°å†™äº†ä¸€ç¯‡æ–°åšå®¢æ–‡ç« ï¼Œ&lt;a href=&quot;https://www.pragmatic.ml/reformer-deep-dive/&quot;&gt;A Deep Dive into the Reformer&lt;/a&gt;ï¼Œæ·±å…¥æ¢è®¨äº†&lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;Reformer&lt;/a&gt;ï¼Œè¿™æ˜¯Google AIæœ€è¿‘æå‡ºçš„ä¸€ç§æ–°æ”¹è¿›çš„åŸºäºTransformerçš„æ¨¡å‹ã€‚ åœ¨ä¸Šä¸€æœŸæ–°é—»é€šè®¯ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿä»‹ç»äº†Reformerã€‚&lt;/p&gt;
&lt;h4 id=&quot;52-ä¸€ä¸ªå…è´¹çš„åšå®¢å¹³å°&quot;&gt;5.2 ä¸€ä¸ªå…è´¹çš„åšå®¢å¹³å°&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://fastpages.fast.ai/fastpages/jupyter/2020/02/21/introducing-fastpages.html&quot;&gt;fastpages&lt;/a&gt;å…è®¸ä½ å…è´¹ä½¿ç”¨GitHubé¡µé¢è‡ªåŠ¨å»ºç«‹åšå®¢ã€‚ è¯¥è§£å†³æ–¹æ¡ˆç®€åŒ–äº†å‘å¸ƒåšå®¢çš„è¿‡ç¨‹ï¼Œè¿˜æ”¯æŒä½¿ç”¨å¯¼å‡ºçš„Wordæ–‡æ¡£å’ŒJupyter notebookã€‚&lt;/p&gt;
&lt;h4 id=&quot;53-googleé¢è¯•æŠ€å·§&quot;&gt;5.3 Googleé¢è¯•æŠ€å·§&lt;/h4&gt;
&lt;p&gt;Google Brainå›¢é˜Ÿçš„Pablo Castroå‘è¡¨äº†ä¸€ç¯‡å‡ºè‰²çš„åšå®¢æ–‡ç« ï¼Œ&lt;a href=&quot;https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html&quot;&gt;Tips for interviewing at Google&lt;/a&gt;ï¼Œé‡ç‚¹ä»‹ç»äº†é‚£äº›æœ‰å…´è¶£åœ¨Googleæ±‚èŒé¢è¯•çš„äººçš„æŠ€å·§ã€‚ ä¸»é¢˜åŒ…æ‹¬æœ‰å…³å¦‚ä½•å‡†å¤‡é¢è¯•ï¼Œé¢è¯•è¿‡ç¨‹ä¸­ä¼šå‘ç”Ÿä»€ä¹ˆä»¥åŠé¢è¯•åä¼šå‘ç”Ÿä»€ä¹ˆçš„å»ºè®®ã€‚&lt;/p&gt;
&lt;h4 id=&quot;54-transformeræ˜¯å›¾ç¥ç»ç½‘ç»œ&quot;&gt;5.4 Transformeræ˜¯å›¾ç¥ç»ç½‘ç»œ&lt;/h4&gt;
&lt;p&gt;å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å’Œå˜å‹å™¨éƒ½å·²è¯æ˜åœ¨ä¸åŒçš„NLPä»»åŠ¡ä¸Šæœ‰æ•ˆã€‚ ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™äº›æ–¹æ³•èƒŒåçš„å†…éƒ¨å·¥ä½œåŸç†ä»¥åŠå®ƒä»¬ä¹‹é—´çš„è”ç³»ï¼ŒChaitanya Joshiæ’°å†™äº†ä¸€ç¯‡å¾ˆæ£’çš„æ–‡ç« ï¼Œ&lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;Transformers are Graph Neural Networks&lt;/a&gt;ï¼Œè§£é‡Šäº†GNNä¸Transformersä¹‹é—´çš„è”ç³»ä»¥åŠè¿™äº›æ–¹æ³•å¯ä»¥ä»¥ä¸åŒçš„æ–¹å¼ç»„åˆæˆä¸€ç§æ··åˆæ¨¡å‹ã€‚
&lt;img src=&quot;https://img-blog.csdnimg.cn/20200303123803796.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0thaXl1YW5fc2p0dQ==,size_16,color_FFFFFF,t_70&quot; alt=&quot;åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;55-cnns-and-equivariance&quot;&gt;5.5 CNNs and Equivariance&lt;/h4&gt;
&lt;p&gt;Fabian Fuchså’ŒEd Wagstaffè®¨è®ºäº†&lt;a href=&quot;https://fabianfuchsml.github.io/equivariance1of2/&quot;&gt;ç­‰æ–¹å·®çš„é‡è¦æ€§ä»¥åŠCNN&lt;/a&gt;å¦‚ä½•å®æ–½ã€‚ ä»–ä»¬é¦–å…ˆå®šä¹‰ç­‰æ–¹å·®çš„æ¦‚å¿µï¼Œç„¶ååœ¨CNNçš„ä¸Šä¸‹æ–‡ä¸­è®¨è®ºç¿»è¯‘ã€‚&lt;/p&gt;
&lt;h4 id=&quot;56-å›¾åƒè‡ªç›‘ç£å­¦ä¹ &quot;&gt;5.6 å›¾åƒè‡ªç›‘ç£å­¦ä¹ &lt;/h4&gt;
&lt;p&gt;ç”±äºè‡ªæˆ‘ç›‘ç£åœ¨è¯­è¨€å»ºæ¨¡çš„ç°ä»£æŠ€æœ¯ä¸­å‘æŒ¥äº†ä½œç”¨ï¼Œå› æ­¤åœ¨NLPç®€æŠ¥çš„å‰å‡ æœŸä¸­å·²ç»è¿›è¡Œäº†å¾ˆå¤šè®¨è®ºã€‚ Jonathan Whitakerçš„è¿™ç¯‡åšå®¢æ–‡ç« æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„ï¼Œç›´è§‚çš„&lt;a href=&quot;https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%E7%BD%91/&quot;&gt;å›¾åƒè‡ªæˆ‘ç›‘ç£è§£é‡Š&lt;/a&gt;ã€‚ å¦‚æœä½ çœŸçš„å¯¹è¯¥ä¸»é¢˜æ„Ÿå…´è¶£ï¼ŒAmit Chaudharyè¿˜æ’°å†™äº†ä¸€ç¯‡å‡ºè‰²çš„åšå®¢æ–‡ç« ï¼Œä»¥&lt;a href=&quot;https://amitness.com/2020/02/illustrated-self-supervised-learning/&quot;&gt;å¯è§†åŒ–æ–¹å¼æè¿°è‡ªç›‘ç£å­¦ä¹ &lt;/a&gt;ã€‚&lt;/p&gt;
&lt;h2 id=&quot;6education-&quot;&gt;6ã€Education ğŸ“&lt;/h2&gt;
&lt;h4 id=&quot;61-stanford-cs330&quot;&gt;6.1 Stanford CS330&lt;/h4&gt;
&lt;p&gt;æ–¯å¦ç¦å¤§å­¦æœ€è¿‘ä»¥YouTubeæ’­æ”¾åˆ—è¡¨çš„å½¢å¼å‘å¸ƒäº†æœ‰å…³&lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&quot;&gt;æ·±å±‚å¤šä»»åŠ¡å’Œå…ƒå­¦ä¹ çš„æ–°è¯¾ç¨‹&lt;/a&gt;çš„å½•åƒã€‚ ä¸»é¢˜åŒ…æ‹¬è´å¶æ–¯å…ƒå­¦ä¹ ï¼Œç»ˆèº«å­¦ä¹ ï¼Œå¼ºåŒ–å­¦ä¹ å…¥é—¨ï¼ŒåŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç­‰ã€‚&lt;/p&gt;
&lt;h4 id=&quot;62-pytorch-notebooks&quot;&gt;6.2 PyTorch Notebooks&lt;/h4&gt;
&lt;p&gt;dair.aiå‘å¸ƒäº†ä¸€ç³»åˆ—æ•™ç¨‹ï¼Œæ—¨åœ¨å¸®åŠ©æ‚¨å¼€å§‹&lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;ä½¿ç”¨PyTorchè¿›è¡Œæ·±åº¦ç¥ç»ç½‘ç»œå­¦ä¹ &lt;/a&gt;ã€‚ è¿™æ˜¯ä¸€é¡¹æ­£åœ¨è¿›è¡Œçš„å·¥ä½œï¼Œå½“å‰çš„ä¸€äº›ä¸»é¢˜åŒ…æ‹¬å¦‚ä½•ä»å¤´å¼€å§‹å®ç°é€»è¾‘å›å½’æ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•ä»å¤´å¼€å§‹ç¼–ç¨‹ç¥ç»ç½‘ç»œæˆ–å¾ªç¯ç¥ç»ç½‘ç»œã€‚&lt;/p&gt;
&lt;h4 id=&quot;63-fastaiæ–°ä¹¦è‰ç¨¿&quot;&gt;6.3 fastaiæ–°ä¹¦è‰ç¨¿&lt;/h4&gt;
&lt;p&gt;Jeremy Howardå’ŒSylvain Guggerå°†ä¸ºå³å°†ä¸¾è¡Œçš„è¯¾ç¨‹å‘å¸ƒä¸€ä»½&lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;å®Œæ•´çš„æ•™ç¨‹fastbook&lt;/a&gt;ï¼Œå…¶ä¸­ä»‹ç»äº†æ·±åº¦å­¦ä¹ çš„æ¦‚å¿µä»¥åŠå¦‚ä½•ä½¿ç”¨PyTorchå’Œfastaiåº“å¼€å‘ä¸åŒçš„æ–¹æ³•ã€‚&lt;/p&gt;
&lt;h4 id=&quot;64-å…è´¹æ•°æ®ç§‘å­¦è¯¾ç¨‹&quot;&gt;6.4 å…è´¹æ•°æ®ç§‘å­¦è¯¾ç¨‹&lt;/h4&gt;
&lt;p&gt;Kaggleæä¾›äº†ä¸€ç³»åˆ—&lt;a href=&quot;https://www.kaggle.com/learn/overview&quot;&gt;å…è´¹çš„å¾®å‹è¯¾ç¨‹&lt;/a&gt;ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹è¿›è¡Œæ•°æ®ç§‘å­¦ä¹‹æ—…ã€‚ å…¶ä¸­ä¸€äº›è¯¾ç¨‹åŒ…æ‹¬æœºå™¨å­¦ä¹ çš„å¯è§£é‡Šæ€§ï¼Œæœºå™¨å­¦ä¹ å’ŒPythonå…¥é—¨ï¼Œæ•°æ®å¯è§†åŒ–ï¼Œç‰¹å¾å·¥ç¨‹å’Œæ·±åº¦å­¦ä¹ ç­‰ã€‚&lt;/p&gt;

&lt;p&gt;è¿™æ˜¯å¦ä¸€é—¨ä¸é”™çš„&lt;a href=&quot;https://lewtun.github.io/dslectures/&quot;&gt;åœ¨çº¿æ•°æ®ç§‘å­¦è¯¾ç¨‹&lt;/a&gt;ï¼Œæä¾›äº†è¯¾ç¨‹è¡¨ï¼Œå¹»ç¯ç‰‡å’Œbotebookï¼Œå†…å®¹æ¶‰åŠä»æ¢ç´¢æ€§æ•°æ®åˆ†æï¼Œæ¨¡å‹è§£é‡Šåˆ°è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å„ç§ä¸»é¢˜ã€‚&lt;/p&gt;
&lt;h4 id=&quot;65-pytorchç”Ÿæ€ç³»ç»Ÿ&quot;&gt;6.5 PyTorchç”Ÿæ€ç³»ç»Ÿ&lt;/h4&gt;
&lt;p&gt;nepture.aiå‘è¡¨äº†ä¸€ç¯‡æ–‡ç« ï¼Œ&lt;a href=&quot;https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&amp;amp;utm_medium=tweet&amp;amp;utm_campaign=blog-model-training-libraries-pytorch-ecosystem&quot;&gt;8 Creators and Core Contributors Talk About Their Model Training Libraries From PyTorch Ecosystem&lt;/a&gt;ï¼Œå…¶ä¸­åŒ…å«ä¸æ ¸å¿ƒåˆ›ä½œè€…å’Œè´¡çŒ®è€…çš„è¯¦ç»†è®¨è®ºï¼Œè®¨è®ºäº†ä»–ä»¬çš„æ—…ç¨‹ä»¥åŠæ„å»ºPyTorchåŠå…¶å·¥å…·çš„å“²å­¦ã€‚&lt;/p&gt;
&lt;h4 id=&quot;66-å¯è§†åŒ–è‡ªé€‚åº”ç¨€ç–æ³¨æ„æ¨¡å‹&quot;&gt;6.6 å¯è§†åŒ–è‡ªé€‚åº”ç¨€ç–æ³¨æ„æ¨¡å‹&lt;/h4&gt;
&lt;p&gt;Sasha Rushåˆ†äº«äº†ä¸€éƒ¨ä»¤äººå°è±¡æ·±åˆ»çš„Colab notebookï¼Œè¯¥ç¬”è®°æœ¬è§£é‡Šå¹¶æ˜¾ç¤ºäº†å¦‚ä½•äº§ç”Ÿç¨€ç–softmaxè¾“å‡ºå¹¶å°†ç¨€ç–æ€§å¼•å…¥Transformeræ¨¡å‹çš„å…³æ³¨ç»„ä»¶çš„æŠ€æœ¯ç»†èŠ‚ï¼Œè¯¥ç»„ä»¶æœ‰åŠ©äºåœ¨ç»™å®šä¸Šä¸‹æ–‡ä¸­å¯¹æ— å…³å•è¯äº§ç”Ÿé›¶æ¦‚ç‡ï¼Œä»è€Œæé«˜äº†æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚
&lt;img src=&quot;https://img-blog.csdnimg.cn/20200303155430277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0thaXl1YW5fc2p0dQ==,size_16,color_FFFFFF,t_70&quot; alt=&quot;åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;7noteworthy-mentions-ï¸&quot;&gt;7ã€Noteworthy Mentions â­ï¸&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4&quot;&gt;Conor Bell&lt;/a&gt;å†™äº†è¿™ä¸ªéå¸¸æ£’çš„pythonè„šæœ¬ï¼Œä½¿æ‚¨å¯ä»¥æŸ¥çœ‹å’Œå‡†å¤‡å¯ç”¨äºStyleGANæ¨¡å‹çš„æ•°æ®é›†ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manu Romeroä¸ºè¥¿ç­ç‰™è¯­æä¾›äº†ä¸€ç§&lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos&quot;&gt;ç»è¿‡å¾®è°ƒçš„POSæ¨¡å‹&lt;/a&gt;ï¼Œè¯¥æ¨¡å‹å¯åœ¨Hugging Face Transformeråº“ä¸­è¿›è¡Œè°ƒç”¨ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
è¯¥githubåº“ï¼Œ&lt;a href=&quot;https://github.com/tomohideshibata/BERT-related-papers&quot;&gt;BERT-related-papers&lt;/a&gt;åŒ…å«ä¸€é•¿ä¸²ç²¾å¿ƒæŒ‘é€‰çš„ä¸BERTç›¸å…³çš„è®ºæ–‡ï¼Œè¿™äº›è®ºæ–‡æ¶‰åŠè¯¸å¦‚æ¨¡å‹å‹ç¼©ï¼Œç‰¹å®šé¢†åŸŸï¼Œå¤šæ¨¡å‹ï¼Œç”Ÿæˆï¼Œä¸‹æ¸¸ä»»åŠ¡ç­‰ä¸åŒé—®é¢˜ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Connor Shortenå‘å¸ƒäº†ä¸€ä¸ª15åˆ†é’Ÿçš„ç®€çŸ­è§†é¢‘ï¼Œ&lt;a href=&quot;https://www.youtube.com/watch?time_continue=79&amp;amp;v=-Bh_7tzyoR4&amp;amp;feature=emb_logo&quot;&gt;Automatic Shortcur Removal for Self-supervised Learning&lt;/a&gt;ï¼Œè§£é‡Šäº†ä¸€ä¸ªæ–°çš„é€šç”¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨å‡å°‘è‡ªæˆ‘ç›‘ç£è¡¨ç¤ºå­¦ä¹ ä¸­â€œsortcutâ€çš„å½±å“ã€‚ è¿™ä¸€ç‚¹å¾ˆé‡è¦ï¼Œå› ä¸ºå¦‚æœå¤„ç†ä¸æ­£ç¡®ï¼Œè¯¥æ¨¡å‹å¯èƒ½æ— æ³•å­¦ä¹ æœ‰ç”¨çš„è¯­ä¹‰è¡¨ç¤ºï¼Œå¹¶å¯èƒ½è¯æ˜åœ¨è½¬ç§»å­¦ä¹ ç¯å¢ƒä¸­æ— æ•ˆã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195&quot;&gt;Sebastian Ruder&lt;/a&gt;å‘å¸ƒäº†æ–°ä¸€æœŸçš„NLP News newsletterï¼Œä¸»é¢˜å’Œèµ„æºåŒ…æ‹¬å¯¹2019å¹´NLPå’ŒMLè®ºæ–‡çš„åˆ†æï¼Œåˆ°ç”¨äºå­¦ä¹ æœ‰å…³è½¬ç§»å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ è¦ç‚¹çš„å¹»ç¯ç‰‡ã€‚&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP%E7%AE%80%E6%8A%A5/&quot;&gt;NLP ç®€æŠ¥ï¼ˆIssue#6ï¼‰&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 02, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_BERTology_Primer_fastpages_T5/" />
  <id>https://dair.ai/NLP_Newsletter_BERTology_Primer_fastpages_T5</id>
  <published>2020-03-02T00:00:00-06:00</published>
  <updated>2020-03-02T00:00:00-06:00</updated>
  <author>
    <name>Elvis Saravia</name>
    <uri>https://dair.ai</uri>
    <email>ellfae@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*vWICxAehSy3xOnqGIXtpoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Welcome to the sixth issue of the NLP Newsletter. Thanks for all the support and for taking the time to read through the latest in ML and NLP. This issue covers topics that range from extending the Transformer model to slowing publication in ML to a series of ML and NLP book and project launches.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;A few updates about the NLP Newsletter and dair.ai&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
We have been translating the newsletter to other languages such as Brazilian Portuguese, Chinese, Arabic, Spanish, among others. Thanks to those folks that have helped with the translations ğŸ¤—. You can also contribute &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A month ago, we officially launched our new &lt;a href=&quot;https://dair.ai/&quot;&gt;website&lt;/a&gt;. You can look at our &lt;a href=&quot;https://github.com/dair-ai&quot;&gt;GitHub organization&lt;/a&gt; for more information about dair.ai and projects. If you are interested in seeing how others are already contributing to dair.ai or are interested in contributing to democratizing artificial intelligence research, education, and technologies, check our &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues&quot;&gt;issues&lt;/a&gt; section.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Subscribe&lt;/em&gt;&lt;/a&gt; &lt;em&gt;ğŸ”– to the NLP Newsletter to receive future issues in your inbox.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications ğŸ“™&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A Primer in BERTology: What we know about how BERT works&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Transformer-based models have shown to be effective at approaching different types of NLP tasks that range from &lt;em&gt;sequence labeling&lt;/em&gt; to &lt;em&gt;question answering&lt;/em&gt;. One of those models called BERT &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;(Devlin et al. 2019)&lt;/a&gt; is widely used but, like other models that employ deep neural networks, we know very little about their inner workings. A new &lt;a href=&quot;https://arxiv.org/abs/2002.12327&quot;&gt;paper&lt;/a&gt; titled â€œ&lt;strong&gt;A Primer in BERTology: What we know about how BERT works&lt;/strong&gt;â€ aims to answer some of the questions about why BERT performs well on so many NLP tasks. Some of the topics addressed in the paper include the type of knowledge learned by BERT and where it is represented, and how that knowledge is learned and other methods researchers are using to improve it.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI recently published a &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;method&lt;/a&gt; that brings together all the lessons learned and improvements from NLP transfer learning models into one unified framework called Text-to-Text Transfer Transformer (T5). This work proposes that most NLP tasks can be formulated in a text-to-text format, suggesting that both the inputs and outputs are texts. The authors claim that this â€œ&lt;em&gt;framework provides a consistent training objective both for pre-training and fine-tuning&lt;/em&gt;â€. T5 is essentially an encoder-decoder Transformer that applies various improvements in particular to the attention components of the model. The model was pre-trained on a newly released dataset called &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/c4&quot;&gt;Colossal Clean Crawled Corpus&lt;/a&gt; and achieved SOTA results on NLP tasks such as summarization, question answering, and text classification.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*T9MXxcDOd2fX6xblbu7VdQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;&lt;em&gt;(Raffel et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;12-in-1: Multi-Task Vision and Language Representation Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Current research uses independent tasks and datasets to perform vision-and-language research even when the â€œ&lt;em&gt;visually-grounded language understanding skill&lt;/em&gt;sâ€ required to perform these tasks overlap. A new &lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;paper&lt;/a&gt; (to be presented at CVPR) proposes a large-scale multi-task approach to better model and jointly train vision-and-language tasks to generate a more generic vision-and-language model. The model reduces the parameter size and performs well on tasks like caption-based image retrieval and visual question answering.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yyvN4bK0K2iykyJ2-QVBjw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;&lt;em&gt;(Lu et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
reciTAL researchers and collaborators published a paper that aims to answer the question of whether a BERT model can produce representations that generalize to other modalities beyond text such as vision. They propose a model called BERT-gen that leverages mono or multi-modal representations and achieve improved results on visual question generation datasets.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2NgR7yBuVLDcEza9UT41dw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.10832&quot;&gt;&lt;em&gt;(Scialom et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;creativity-and-society-&quot;&gt;Creativity and Society ğŸ¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Gary Marcus recently published a &lt;a href=&quot;https://arxiv.org/abs/2002.06177&quot;&gt;paper&lt;/a&gt; where he explains a series of steps that, in his view, we should be taking to build more robust AI systems. Garyâ€™s central idea in this paper is to focus on building hybrid and knowledge-driven systems guided by cognitive models as opposed to focusing on building larger systems that require more data and computation power.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;10 Breakthrough Technologies 2020&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
MIT Technology Review published a list of the &lt;a href=&quot;https://www.technologyreview.com/lists/technologies/2020/&quot;&gt;10 breakthroughs&lt;/a&gt; they have identified that will make a difference in solving problems that could change the way we live and work. The listâ€Šâ€”â€Šin no particular orderâ€Šâ€”â€Šincludes unhackable internet, hyper-personalized medicine, digital money, anti-aging drugs, AI-discovered molecules, satellite mega-constellations, quantum supremacy, Tiny AI, differential privacy, and climate attribution.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Time to rethink the publication process in machine learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Yoshua Bengio recently &lt;a href=&quot;https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/&quot;&gt;wrote&lt;/a&gt; on his concerns about the fast-paced cycles of ML publications. The main concern is that due to the velocity of publishing, a lot of papers get published that contain errors and are just incremental, whereas spending more time and ensuring rigour, which is how it used to work many years ago, seems to be vanishing. On top of it all, students are the ones that have to deal with the negative consequences of this pressure and stress. To address the situation, Bengio talks about his actions to help in the process of slowing down research publications for the good of science.&lt;/p&gt;

&lt;h1 id=&quot;tools-and-datasets-ï¸&quot;&gt;Tools and Datasets âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;PointerGenerator network implementation in AllenNLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pointer-Generator networks aim to augment sequence-to-sequence attentional models that are used to &lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;improve&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;&lt;em&gt;abstractive summarization&lt;/em&gt;&lt;/a&gt;. If you wish to use this technique for abstractive summarization using AllenNLP, Kundan Krishna has developed a &lt;a href=&quot;https://github.com/kukrishna/pointer-generator-pytorch-allennlp&quot;&gt;library&lt;/a&gt; that allows you to run a pretrained model (provided) or train your own model.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Fa4G6BrnJm3NSDr3TDHhfw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Question answering for different languages&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
With the proliferation of Transformer models and their effectiveness for large-scale NLP tasks performed in other languages, there has been an impressive amount of effort to release different types of datasets in different languages. For instance, Sebastian Ruder &lt;a href=&quot;https://twitter.com/seb_ruder/status/1231713840502657025?s=20&quot;&gt;shared&lt;/a&gt; a list of datasets that can be used for question answering research in different languages: &lt;a href=&quot;https://www.aclweb.org/anthology/W18-2605/&quot;&gt;DuReader&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1909.07005&quot;&gt;KorQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1912.09723&quot;&gt;SberQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2002.06071&quot;&gt;FQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1906.05394&quot;&gt;Arabic-SQuAD&lt;/a&gt;, &lt;a href=&quot;https://github.com/crux82/squad-it&quot;&gt;SQuAD-it&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/1912.05200v2&quot;&gt;Spanish SQuAD&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PyTorch Lightning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
PyTorch Lightning is a &lt;a href=&quot;https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09&quot;&gt;tool&lt;/a&gt; that allows you to abstract training that could require setting up GPU/TPU training and the use of 16-bit precision. Getting those things to work can become tedious but the great news is that PyTorch Lightning simplifies this process and allows you to train models on multi GPUs and TPUs without the need to change your current PyTorch code.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Graph Neural Networks in TF2&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A Microsoft Research team releases a &lt;a href=&quot;https://github.com/microsoft/tf2-gnn&quot;&gt;library&lt;/a&gt; that provides access to implementations of many different graph neural network (GNN) architectures. This library is based on TensorFlow 2 and also provides data-wrangling modules that can directly be used in training/evaluation loops.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Pre-training SmallBERTaâ€Šâ€”â€ŠA tiny model to train on a tiny dataset&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Have you ever wanted to train your own language model from scratch but didnâ€™t have enough resources to do so? If so, then Aditya Malte have you covered with this great &lt;a href=&quot;https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb&quot;&gt;Colab notebook&lt;/a&gt; that teaches you how to train an LM from scratch with a smaller dataset.&lt;/p&gt;

&lt;h1 id=&quot;ethics-in-ai-&quot;&gt;Ethics in AI ğŸš¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Why faces donâ€™t always tell the truth about feelings&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
For some time now, many researchers and companies have attempted to build AI models that understand and can recognize emotions either in the textual or visual context. A new &lt;a href=&quot;https://www.nature.com/articles/d41586-020-00507-5&quot;&gt;article&lt;/a&gt; reopens the debate that AI techniques that aim to recognize emotion directly from face images are not doing it right. The main argument, raised by prominent psychologists in the space, is that there is no evidence of universal expressions that can be used for emotion detection from face images alone. It would take a model better understanding of personality traits, body movement, among other things to really get closer to more accurately detecting the emotions displayed by humans.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Differential Privacy and Federated Learning Explained&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
One of the ethical considerations when building AI systems is to ensure privacy. Currently, this can be achieved in two ways, either using differential privacy or federated learning. If you want to know more about these topics, Jordan Harrod provides us a great introduction in this &lt;a href=&quot;https://www.youtube.com/watch?v=MOcTGM_UteM&quot;&gt;video&lt;/a&gt; which also includes a hands-on practice session with the use of a Colab notebook.&lt;/p&gt;

&lt;h1 id=&quot;articles-and-blog-posts-ï¸&quot;&gt;Articles and Blog posts âœï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A Deep Dive into the Reformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Madison May wrote a &lt;a href=&quot;https://www.pragmatic.ml/reformer-deep-dive/&quot;&gt;new blog post&lt;/a&gt; that provides a deep dive into &lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;Reformer&lt;/a&gt;, which is a new and improved Transformer-based model recently proposed by Google AI. We also featured Reformer in a &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-reformer-deepmath-electra-tinybert-for-search-vizseq-open-sourcing-ml-68d5b6eed057&quot;&gt;previous issue&lt;/a&gt; of the newsletter.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;A free blogging platform&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://fastpages.fast.ai/fastpages/jupyter/2020/02/21/introducing-fastpages.html&quot;&gt;fastpages&lt;/a&gt; allows you to automatically set up a blog using GitHub pages for free. This solution simplifies the process of publishing a blog and it also supports the use of exported word documents and Jupyter notebooks.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Tips for interviewing at Google&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pablo Castro, from the Google Brain team, published an &lt;a href=&quot;https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html&quot;&gt;excellent blog post&lt;/a&gt; highlighting a list of tips for those interested in interviewing for a job at Google. Topics include advice on how to prepare for the interview, what to expect during the interview, and what happens after the interview.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Transformers are Graph Neural Networks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Both graph neural networks (GNNs) and Transformers have shown to be effective at different NLP tasks. To better understand the inner workings behind these approaches and how they relate, Chaitanya Joshi wrote a great &lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;article&lt;/a&gt; explaining the connection between GNNs and Transformers and different ways these methods can be combined in a sort of hybrid model.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*u-BkejfKSKcnWOBx.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Representing a sentence as a fully-connected word graphâ€Šâ€”&lt;/em&gt;â€Š&lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;CNNs and Equivariance&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Fabian Fuchs and Ed Wagstaff &lt;a href=&quot;https://fabianfuchsml.github.io/equivariance1of2/&quot;&gt;discuss&lt;/a&gt; the importance of equivariance and how CNNs enforce it. The concept of equivariance is first defined and then discussed in the context of CNNs with respect to translation.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Self-supervised learning with images&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Self-supervised has been discussed a lot in previous issues of the NLP Newsletter due to the role it has played in modern techniques for language modeling. This &lt;a href=&quot;https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%e7%bd%91/&quot;&gt;blog post&lt;/a&gt; by Jonathan Whitaker provides a nice and intuitive explanation of self-supervision in the context of images. If you are really interested in the topic, Amit Chaudhary also wrote an excellent &lt;a href=&quot;https://amitness.com/2020/02/illustrated-self-supervised-learning/&quot;&gt;blog post&lt;/a&gt; describing the concept in a visual way.&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education ğŸ“&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Stanford CS330: Deep Multi-Task and Meta-Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Stanford recently released video recordings, in the form of a YouTube playlist, for their new &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&quot;&gt;course on deep multi-task and meta-learning&lt;/a&gt;. Topics include bayesian meta-learning, lifelong learning, a reinforcement learning primer, model-based reinforcement learning, among others.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PyTorch Notebooks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
dair.ai releases a &lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;series of notebooks&lt;/a&gt; that aim to get you started with deep neural networks using PyTorch. This is a work in progress and some current topics include how to implement a logistic regression model from scratch and how to program a neural network or recurrent neural network from scratch. Colab notebooks are also available in the GitHub repository.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;The fastai book (draft)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jeremy Howard and Sylvain Gugger release a &lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;comprehensive list&lt;/a&gt; of draft notebooks for an upcoming course that introduces deep learning concepts and how to develop different methods using PyTorch and the fastai library.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Free Data Science courses&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In case you missed it, Kaggle provides a series of &lt;a href=&quot;https://www.kaggle.com/learn/overview&quot;&gt;free micro-courses&lt;/a&gt; to get you started with your Data Science journey. Some of these courses include machine learning explainability, an intro to machine learning and Python, data visualization, feature engineering, and deep learning, among others.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Here is another excellent &lt;a href=&quot;https://lewtun.github.io/dslectures/&quot;&gt;online data science course&lt;/a&gt; that provides a syllabus, slides, and notebooks on topics that range from exploratory data analysis to model interpretation to natural language processing.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;8 Creators and Core Contributors Talk About Their Model Training Libraries From PyTorch Ecosystem&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
nepture.ai published an &lt;a href=&quot;https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&amp;amp;utm_medium=tweet&amp;amp;utm_campaign=blog-model-training-libraries-pytorch-ecosystem&quot;&gt;extensive article&lt;/a&gt; that contains detailed discussions with core creators and contributors about their journey and philosophy of building PyTorch and tools around it.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Visualizing Adaptive Sparse Attention Models&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sasha Rush shares an impressive &lt;a href=&quot;http://Visualizing%20Adaptive%20Sparse%20Attention%20Models&quot;&gt;Colab notebook&lt;/a&gt; that explains and shows the technical details of how to produce sparse softmax outputs and induce sparsity into the attention component of a Transformer model which helps to produce zero probability for irrelevant words in a given context, improving performance and interpretability all at once.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*7BB322LlVgt1zzk-cviSoA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Visualizing probability distribution of a softmax output&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;noteworthy-mentions-ï¸&quot;&gt;Noteworthy Mentions â­ï¸&lt;/h1&gt;

&lt;p&gt;You can access the previous issue of the ğŸ— NLP Newsletter &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-the-annotated-gpt-2-understanding-self-distillation-haiku-ganilla-sparkwiki-b0f47f595c82&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Conor Bell wrote this nice &lt;a href=&quot;https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4&quot;&gt;python script&lt;/a&gt; that allows you to view and prepare a dataset that can be used for a StyleGAN model.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manu Romero &lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos&quot;&gt;contributes&lt;/a&gt; a fine-tuned POS model for Spanish. The model is available for use in the Hugging Face Transformer library. It will be interesting to see this effort in other languages.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
This &lt;a href=&quot;https://github.com/tomohideshibata/BERT-related-papers&quot;&gt;repo&lt;/a&gt; contains a long list of carefully curated BERT-related papers that approach different problems such as model compression, domain-specific, multi-model, generation, downstream tasks, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Connor Shorten published a short &lt;a href=&quot;https://www.youtube.com/watch?time_continue=79&amp;amp;v=-Bh_7tzyoR4&amp;amp;feature=emb_logo&quot;&gt;15-minute video&lt;/a&gt; explaining a new general framework that aims to reduce the effect of â€œshortcutâ€ features in self-supervised representation learning. This is important because if not done right, the model can fail to learn useful semantic representations and potentially prove ineffective in a transfer learning setting.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Ruder published a new issue of the NLP News newsletter that highlights topics and resources that range from an analysis of NLP and ML papers in 2019 to slides for learning about transfer learning and deep learning essentials. Check it out &lt;a href=&quot;http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Subscribe&lt;/em&gt;&lt;/a&gt; &lt;em&gt;ğŸ”– to the NLP Newsletter to receive future issues in your inbox.&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_BERTology_Primer_fastpages_T5/&quot;&gt;NLP Newsletter #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 02, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [PT-BR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5/" />
  <id>https://dair.ai/NLP_Newsletter[PT-BR]_BERTology_Primer_fastpages_T5</id>
  <published>2020-03-02T00:00:00-06:00</published>
  <updated>2020-03-02T00:00:00-06:00</updated>
  <author>
    <name>Victor Garritano Noronha</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*vWICxAehSy3xOnqGIXtpoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- Welcome to the sixth issue of the NLP Newsletter. Thanks for all the support and for taking the time to read through the latest in ML and NLP. This issue covers topics that range from extending the Transformer model to slowing publication in ML to a series of ML and NLP book and project launches. --&gt;

&lt;p&gt;&lt;br /&gt;
Seja muito bem-vindo Ã  sexta ediÃ§Ã£o da &lt;em&gt;NLP Newsletter&lt;/em&gt;. Agradecemos por todo o suporte e dedicaÃ§Ã£o Ã  leitura dos temas mais recentes em ML e NLP. Essa ediÃ§Ã£o cobre tÃ³picos como extensÃµes ao modelo Transformer, desaceleraÃ§Ã£o no processo de publicaÃ§Ã£o em Aprendizado de MÃ¡quina, divulgaÃ§Ã£o de livros e projetos sobre ML e NLP e muito mais.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Algumas atualizaÃ§Ã£oes sobre a NLP Newsletter e o dair.ai&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- We have been translating the newsletter to other languages such as Brazilian Portuguese, Chinese, Arabic, Spanish, among others. Thanks to those folks that have helped with the translations ğŸ¤—. You can also contribute [here](https://github.com/dair-ai/dair-ai.github.io/issues/11). --&gt;
&lt;p&gt;&lt;br /&gt;
NÃ³s estamos traduzindo a Newsletter para outros idiomas, como o PortuguÃªs Brasileiro, ChinÃªs, Ãrabe, Espanhol, dentre outros. Agradecemos aos colegas que realizaram as traduÃ§Ãµes ğŸ¤—. VocÃª tambÃ©m pode contribuir &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;aqui&lt;/a&gt;!&lt;/p&gt;

&lt;!-- A month ago, we officially launched our new [website](https://dair.ai/). You can look at our [GitHub organization](https://github.com/dair-ai) for more information about dair.ai and projects. If you are interested in seeing how others are already contributing to dair.ai or are interested in contributing to democratizing artificial intelligence research, education, and technologies, check our [issues](https://github.com/dair-ai/dair-ai.github.io/issues) section. --&gt;

&lt;p&gt;&lt;br /&gt;
No mÃªs passado, nÃ³s realizamos o lanÃ§amento oficial do nosso novo &lt;a href=&quot;https://dair.ai/&quot;&gt;website&lt;/a&gt;. VocÃª pode dar uma olhada em nossa [organizaÃ§Ã£o no GitHub] (https://github.com/dair-ai) para mais informaÃ§Ãµes sobre os projetos em andamento. Se vocÃª estÃ¡ interessado em saber mais sobre as contribuiÃ§Ãµes jÃ¡ realizadas para a dar.ai, ou mesmo contribuir para a democratizaÃ§Ã£o das tecnologias, ensino e pesquisa sobre InteligÃªncia Artificial, veja nossa seÃ§Ã£o de &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues&quot;&gt;issues&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Inscreva-se&lt;/em&gt;&lt;/a&gt; &lt;em&gt;ğŸ”– para receber as prÃ³ximas ediÃ§Ãµes na sua caixa de entrada!&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;publicaÃ§Ãµes-&quot;&gt;PublicaÃ§Ãµes ğŸ“™&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A Primer in BERTology: What we know about how BERT works&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Transformer-based models have shown to be effective at approaching different types of NLP tasks that range from *sequence labeling* to *question answering*. One of those models called BERT [(Devlin et al. 2019)](https://arxiv.org/abs/1810.04805) is widely used but, like other models that employ deep neural networks, we know very little about their inner workings. A new [paper](https://arxiv.org/abs/2002.12327) titled â€œ**A Primer in BERTology: What we know about how BERT works**â€ aims to answer some of the questions about why BERT performs well on so many NLP tasks. Some of the topics addressed in the paper include the type of knowledge learned by BERT and where it is represented, and how that knowledge is learned and other methods researchers are using to improve it. --&gt;

&lt;p&gt;&lt;br /&gt;
Modelos baseados no &lt;em&gt;Transformer&lt;/em&gt; mostraram-se bastante efetivos na abordagem das mais diversas tarefas de Processamento de Linguagem Natural, como &lt;em&gt;sequence labeling&lt;/em&gt; e &lt;em&gt;question answering&lt;/em&gt;. Um desses modelos, o BERT &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;(Devlin et al. 2019)&lt;/a&gt;, vem sendo amplamente utilizado. Entretanto, assim como acontece com outros modelos que utilizam redes neurais profundas, ainda sabemos muito pouco sobre seu funcionamento interno. Um novo &lt;a href=&quot;https://arxiv.org/abs/2002.12327&quot;&gt;artigo&lt;/a&gt; entitulado â€œ&lt;strong&gt;A Primer in BERTology: What we know about how BERT works&lt;/strong&gt;â€ busca comeÃ§ar a responder questÃµes sobre as razÃµes que possibilitam o BERT funcionar tÃ£o bem em tantas tarefas de NLP. Alguns dos tÃ³picos investigados no trabalho incluem o tipo de conhecimento aprendido pelo modelo e como o mesmo Ã© representado, alÃ©m de mÃ©todos que outros pesquisadores estÃ£o utilizando para melhorar o processo de aprendizado.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Google AI recently published a [method](https://arxiv.org/abs/1910.10683) that brings together all the lessons learned and improvements from NLP transfer learning models into one unified framework called Text-to-Text Transfer Transformer (T5). This work proposes that most NLP tasks can be formulated in a text-to-text format, suggesting that both the inputs and outputs are texts. The authors claim that this â€œ*framework provides a consistent training objective both for pre-training and fine-tuning*â€. T5 is essentially an encoder-decoder Transformer that applies various improvements in particular to the attention components of the model. The model was pre-trained on a newly released dataset called [Colossal Clean Crawled Corpus](https://www.tensorflow.org/datasets/catalog/c4) and achieved SOTA results on NLP tasks such as summarization, question answering, and text classification. --&gt;

&lt;p&gt;&lt;br /&gt;
A Google AI publicou recentemente um &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;mÃ©todo&lt;/a&gt; que incorpora todas as liÃ§Ãµes aprendidas e melhorias do &lt;em&gt;Transfer Learning&lt;/em&gt; para NLP num &lt;em&gt;franework&lt;/em&gt; unificado, denominado Text-to-Text Transfer Transformer (T5). O trabalho propÃµe que a maioria das tarefas de NLP podem ser formuladas no formato &lt;em&gt;text-to-text&lt;/em&gt;, onde tanto a entrada quanto a saÃ­da do problema apresentam-se na forma de texto. Os autores alegam que â€œesse framework fornece uma funÃ§Ã£o objetivo para treinamento que Ã© consistente tanto na fase de prÃ©-treinamento quanto no &lt;em&gt;fine-tuning&lt;/em&gt;â€. O T5 Ã© essencialmente um &lt;em&gt;encoder-decoder&lt;/em&gt; baseado no &lt;em&gt;Transformer&lt;/em&gt;, com vÃ¡rias melhorias, em especial nos componentes de atenÃ§Ã£o da arquitetura. O modelo foi prÃ©-treinado sobre uma nova base de dados disponibilizada recentemente, conhecida como &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/c4&quot;&gt;Colossal Clean Crawled Corpus&lt;/a&gt;, onde foi estabelecido um novo estado-da-arte para tarefas como sumarizaÃ§Ã£o, &lt;em&gt;question answering&lt;/em&gt; e classificaÃ§Ã£o de texto.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*T9MXxcDOd2fX6xblbu7VdQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;&lt;em&gt;(Raffel et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;12-in-1: Multi-Task Vision and Language Representation Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Current research uses independent tasks and datasets to perform vision-and-language research even when the â€œ*visually-grounded language understanding skill*sâ€ required to perform these tasks overlap. A new [paper](https://arxiv.org/abs/1912.02315) (to be presented at CVPR) proposes a large-scale multi-task approach to better model and jointly train vision-and-language tasks to generate a more generic vision-and-language model. The model reduces the parameter size and performs well on tasks like caption-based image retrieval and visual question answering. --&gt;

&lt;p&gt;&lt;br /&gt;
Os esforÃ§os de pesquisa atuais utilizam tarefas e bases de dados independentes para realizar avanÃ§os na Ã¡rea de linguÃ­stica e visÃ£o computacional, mesmo quando os conhecimentos necessÃ¡rios para abordar essas tarefas possuem interseÃ§Ã£o. Um novo &lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;artigo&lt;/a&gt; (que serÃ¡ apresentado na CVPR) propÃµe uma abordagem multi-tarefa em larga escala para uma melhor modelagem e treinamento conjunto em tarefas de linguÃ­stica e visÃ£o computacional, gerando uma modelo mais genÃ©rico para as mesmas. O mÃ©todo reduz a quantidade de parÃ¢metros e apresenta um bom desempenho em problemas como recuperaÃ§Ã£o de imagens baseadas em legendas, e &lt;em&gt;question answering&lt;/em&gt; visual.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yyvN4bK0K2iykyJ2-QVBjw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;&lt;em&gt;(Lu et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- reciTAL researchers and collaborators published a paper that aims to answer the question of whether a BERT model can produce representations that generalize to other modalities beyond text such as vision. They propose a model called BERT-gen that leverages mono or multi-modal representations and achieve improved results on visual question generation datasets. --&gt;

&lt;p&gt;&lt;br /&gt;
Pesquisadores e colaboradores da reciTAL publicaram um trabalho que busca responder se um modelo BERT Ã© capaz de gerar representaÃ§Ãµes que generalizam para outras Ã¡reas, alÃ©m de texto e visÃ£o computacional. Os autores apresentam um modelo denominado &lt;em&gt;BERT-gen&lt;/em&gt;, que tira proveito de representaÃ§Ãµes mono e multi-modais para obter desempenhos superiores em bases de dados de geraÃ§Ãµes de perguntas baseadas em imagens.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2NgR7yBuVLDcEza9UT41dw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.10832&quot;&gt;&lt;em&gt;(Scialom et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;criatividade-e-sociedade-&quot;&gt;Criatividade e Sociedade ğŸ¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Gary Marcus recently published a [paper](https://arxiv.org/abs/2002.06177) where he explains a series of steps that, in his view, we should be taking to build more robust AI systems. Garyâ€™s central idea in this paper is to focus on building hybrid and knowledge-driven systems guided by cognitive models as opposed to focusing on building larger systems that require more data and computation power. --&gt;

&lt;p&gt;&lt;br /&gt;
Gary Marcus publicou recentemente um &lt;a href=&quot;https://arxiv.org/abs/2002.06177&quot;&gt;trabalho&lt;/a&gt; onde ele explica a sÃ©rie de passos que, na opiniÃ£o dele, devem ser seguidos para o desenvolvimento de sistemas de IA mais robustos. A ideia central do artigo Ã© priorizar a construÃ§Ã£o de sistemas hÃ­bridos e orientados Ã  conhecimento, guiados por modelos cognitivos, ao invÃ©s da proposiÃ§Ã£o de modelos com mais parÃ¢metros que exigem mais dados e poder computacional.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;10 Breakthrough Technologies 2020&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- MIT Technology Review published a list of the [10 breakthroughs](https://www.technologyreview.com/lists/technologies/2020/) they have identified that will make a difference in solving problems that could change the way we live and work. The listâ€Šâ€”â€Šin no particular orderâ€Šâ€”â€Šincludes unhackable internet, hyper-personalized medicine, digital money, anti-aging drugs, AI-discovered molecules, satellite mega-constellations, quantum supremacy, Tiny AI, differential privacy, and climate attribution. --&gt;

&lt;p&gt;&lt;br /&gt;
A revista &lt;em&gt;MIT Technology Review&lt;/em&gt; publicou a lista dos &lt;a href=&quot;https://www.technologyreview.com/lists/technologies/2020/&quot;&gt;10 avanÃ§os&lt;/a&gt; tecnolÃ³gicos que segundo eles farÃ£o a diferenÃ§a na resoluÃ§Ã£o de problemas que podem mudar a maneira como vivemos e trabalhamos. A lista â€” sem ordem especÃ­fica â€” inclui a internet &lt;em&gt;nÃ£o-hackÃ¡vel&lt;/em&gt;, medicina hiper-personalizada, moedas digitais, medicamentos anti-idade, molÃ©culas descobertas por sistemas de IA, mega-constelaÃ§Ãµes de satÃ©lites artificias, supremacia quÃ¢ntica, IA em aparelhos celulares, privacidade diferencial e &lt;em&gt;climate attribution&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Time to rethink the publication process in machine learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Yoshua Bengio recently [wrote](https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/) on his concerns about the fast-paced cycles of ML publications. The main concern is that due to the velocity of publishing, a lot of papers get published that contain errors and are just incremental, whereas spending more time and ensuring rigour, which is how it used to work many years ago, seems to be vanishing. On top of it all, students are the ones that have to deal with the negative consequences of this pressure and stress. To address the situation, Bengio talks about his actions to help in the process of slowing down research publications for the good of science. --&gt;

&lt;p&gt;&lt;br /&gt;
Yoshua Bengio &lt;a href=&quot;https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/&quot;&gt;escreveu&lt;/a&gt; recentemente sobre suas preocupaÃ§Ãµes em relaÃ§Ã£o aos atuais ciclos acelerados de publicaÃ§Ãµes em Aprendizado de MÃ¡quina. O ponto principal Ã© que, por causa da velocidade dessas, diversos trabalhos publicados apresentam erros e sÃ£o apenas incrementais, deixando o investimento de tempo na revisÃ£o e verificaÃ§Ã£o do rigor empregado na metodologia e experimentos de lado. Diante de tudo isso, os estudantes sÃ£o aqueles que precisam lidar com as consequÃªncias negativas da pressÃ£o e estresse gerados por essa situaÃ§Ã£o. Com o objetivo de solucionar esse problema, Bengio compartilha suas aÃ§Ãµes para ajudar no processo de desaceleraÃ§Ã£o das publicaÃ§Ãµes para o bem da ciÃªncia.&lt;/p&gt;

&lt;h1 id=&quot;ferramentas-e-bases-de-dados-ï¸&quot;&gt;Ferramentas e Bases de Dados âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;ImplementaÃ§Ã£o da PointerGenerator network com a AllenNLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Pointer-Generator networks aim to augment sequence-to-sequence attentional models that are used to [improve](https://arxiv.org/abs/1704.04368) [*abstractive summarization*](https://arxiv.org/abs/1704.04368). If you wish to use this technique for abstractive summarization using AllenNLP, Kundan Krishna has developed a [library](https://github.com/kukrishna/pointer-generator-pytorch-allennlp) that allows you to run a pretrained model (provided) or train your own model. --&gt;

&lt;p&gt;&lt;br /&gt;
Redes &lt;em&gt;Pointer-Generator&lt;/em&gt; buscam aprimorar o mecanismo de atenÃ§Ã£o de modelos &lt;em&gt;sequence-to-sequence&lt;/em&gt; e sÃ£o utilizadas para &lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;melhorar o desempenho&lt;/a&gt; em tarefas como &lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;sumarizaÃ§Ã£o abstrata&lt;/a&gt;. Se vocÃª gostaria de utilizando essa tÃ©cnica com a &lt;em&gt;framework&lt;/em&gt; AllenNLP, saiba que o Kundan Krishna desenvolveu um &lt;a href=&quot;https://github.com/kukrishna/pointer-generator-pytorch-allennlp&quot;&gt;mÃ³dulo&lt;/a&gt; que permite a execuÃ§Ã£o de um modelo prÃ©-treinado dessa categoria, alÃ©m do treinamento de um novo modelo do zero.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Fa4G6BrnJm3NSDr3TDHhfw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Question answering para diferentes idiomas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- With the proliferation of Transformer models and their effectiveness for large-scale NLP tasks performed in other languages, there has been an impressive amount of effort to release different types of datasets in different languages. For instance, Sebastian Ruder [shared](https://twitter.com/seb_ruder/status/1231713840502657025?s=20) a list of datasets that can be used for question answering research in different languages: [DuReader](https://www.aclweb.org/anthology/W18-2605/), [KorQuAD](https://arxiv.org/abs/1909.07005), [SberQuAD](https://arxiv.org/abs/1912.09723), [FQuAD](https://arxiv.org/abs/2002.06071), [Arabic-SQuAD](https://arxiv.org/abs/1906.05394), [SQuAD-it](https://github.com/crux82/squad-it), and [Spanish SQuAD](https://arxiv.org/abs/1912.05200v2). --&gt;

&lt;p&gt;&lt;br /&gt;
Com a disseminaÃ§Ã£o de modelos baseados no &lt;em&gt;Transformer&lt;/em&gt; e sua efetividade em tarefas de NLP aplicadas a outros idiomas, existe um esforÃ§o significativo na construÃ§Ã£o e liberaÃ§Ã£o de diferentes bases de dados em diferentes dialetos. Por exemplo, o Sebastian Ruder &lt;a href=&quot;https://twitter.com/seb_ruder/status/1231713840502657025?s=20&quot;&gt;compartilhou&lt;/a&gt; uma lista de &lt;em&gt;datasets&lt;/em&gt; que podem ser utilizados no desenvolvimento de mÃ©todos para &lt;em&gt;question answering&lt;/em&gt; em diversas lÃ­nguas: DuReader](https://www.aclweb.org/anthology/W18-2605/), &lt;a href=&quot;https://arxiv.org/abs/1909.07005&quot;&gt;KorQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1912.09723&quot;&gt;SberQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2002.06071&quot;&gt;FQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1906.05394&quot;&gt;Arabic-SQuAD&lt;/a&gt;, &lt;a href=&quot;https://github.com/crux82/squad-it&quot;&gt;SQuAD-it&lt;/a&gt; e &lt;a href=&quot;https://arxiv.org/abs/1912.05200v2&quot;&gt;Spanish SQuAD&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PyTorch Lightning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- PyTorch Lightning is a [tool](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09) that allows you to abstract training that could require setting up GPU/TPU training and the use of 16-bit precision. Getting those things to work can become tedious but the great news is that PyTorch Lightning simplifies this process and allows you to train models on multi GPUs and TPUs without the need to change your current PyTorch code. --&gt;

&lt;p&gt;&lt;br /&gt;
A PyTorch Lightning Ã© uma &lt;a href=&quot;https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09&quot;&gt;ferramenta&lt;/a&gt; que possibilita a abstraÃ§Ã£o da escolha do dispositivo utilizado durante o treinamento de redes neurais (CPU ou GPU), alÃ©m do uso de precisÃ£o de 16 bits. Fazer essas configuraÃ§Ãµes funcionarem pode ser um trabalho entediante, mas felizmente os colaboradores da PyTorch Lightning simplificaram esse processo, permitindo o treinamento de modelos em vÃ¡rias GPUs/TPUs sem a necessidade de alteraÃ§Ã£o do cÃ³digo.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Graph Neural Networks no TF2&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- A Microsoft Research team releases a [library](https://github.com/microsoft/tf2-gnn) that provides access to implementations of many different graph neural network (GNN) architectures. This library is based on TensorFlow 2 and also provides data-wrangling modules that can directly be used in training/evaluation loops. --&gt;

&lt;p&gt;&lt;br /&gt;
O time de pesquisa da Microsoft liberou uma &lt;a href=&quot;https://github.com/microsoft/tf2-gnn&quot;&gt;biblioteca&lt;/a&gt; com a implementaÃ§Ã£o de diversas arquiteturas de &lt;em&gt;Graph Neural Networks (GNNs)&lt;/em&gt;. A biblioteca, baseada na versÃ£o 2.0 do TensorFlow, fornece funcionalidades para manipulaÃ§Ã£o de dados que podem ser utilizadas diretamente nas iteraÃ§Ãµes de treino/avaliaÃ§Ã£o.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Pre-training SmallBERTaâ€Šâ€”â€ŠA tiny model to train on a tiny dataset&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Have you ever wanted to train your own language model from scratch but didnâ€™t have enough resources to do so? If so, then Aditya Malte have you covered with this great [Colab notebook](https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb) that teaches you how to train an LM from scratch with a smaller dataset. --&gt;

&lt;p&gt;&lt;br /&gt;
VocÃª jÃ¡ pensou em treinar o seu prÃ³prio modelo de linguagem do zero, mas nunca teve o poder computacional necessÃ¡rio para isso? Se jÃ¡, entÃ£o o Aditya Malte pode lhe ajudar com esse excelente &lt;a href=&quot;https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb&quot;&gt;notebook no Colab&lt;/a&gt; que exemplifica o processo de treinamento de um modelo de linguagem numa base de dados reduzida.&lt;/p&gt;

&lt;h1 id=&quot;Ã©tica-em-ia-&quot;&gt;Ã‰tica em IA ğŸš¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Why faces donâ€™t always tell the truth about feelings&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
HÃ¡ algum tempo, diversos pesquisadores e empresas tentam construir modelos de IA que consigam entender e reconhecer emoÃ§Ãµes em contextos visuais ou textuais. Um novo &lt;a href=&quot;https://www.nature.com/articles/d41586-020-00507-5&quot;&gt;artigo&lt;/a&gt; reabre o debate que tÃ©cnicas de IA que tentam reconhecer emoÃ§Ãµes diretamente de imagens faciais nÃ£o estÃ£o fazendo seu trabalho direito. O argumento principal, formulado por psicÃ³logos proeminentes na Ã¡rea, Ã© que nÃ£o existe evidÃªncia da existÃªncia de expressÃµes universais que possam ser utilizadas na detecÃ§Ã£o de emoÃ§Ãµes de maneira independente. Seria necessÃ¡ria uma melhor compreensÃ£o de traÃ§os de personalidade e movimentos corporais por parte do modelo, dentre outras caracterÃ­sticas, para que seja possÃ­vel detectar as emoÃ§Ãµes humanas de maneira mais precisa.&lt;/p&gt;

&lt;!-- For some time now, many researchers and companies have attempted to build AI models that understand and can recognize emotions either in the textual or visual context. A new [article](https://www.nature.com/articles/d41586-020-00507-5) reopens the debate that AI techniques that aim to recognize emotion directly from face images are not doing it right. The main argument, raised by prominent psychologists in the space, is that there is no evidence of universal expressions that can be used for emotion detection from face images alone. It would take a model better understanding of personality traits, body movement, among other things to really get closer to more accurately detecting the emotions displayed by humans. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Differential Privacy and Federated Learning Explicadas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- One of the ethical considerations when building AI systems is to ensure privacy. Currently, this can be achieved in two ways, either using differential privacy or federated learning. If you want to know more about these topics, Jordan Harrod provides us a great introduction in this [video](https://www.youtube.com/watch?v=MOcTGM_UteM) which also includes a hands-on practice session with the use of a Colab notebook. --&gt;

&lt;p&gt;&lt;br /&gt;
Uma das consideraÃ§Ãµes Ã©ticas que devem ser levadas em consideraÃ§Ã£o durante a construÃ§Ã£o de sistemas de IA Ã© a garantia de privacidade. Atualmente, essa garantia pode ser obtida de duas maneiras: atravÃ©s da &lt;em&gt;differential privacy&lt;/em&gt; ou do &lt;em&gt;federated learning&lt;/em&gt;. Se vocÃª quiser saber mais sobre esses dois tÃ³picos, Jordan Harrod produziu uma excelente introduÃ§Ã£o nesse &lt;a href=&quot;https://www.youtube.com/watch?v=MOcTGM_UteM&quot;&gt;vÃ­deo&lt;/a&gt;, que inclui uma sessÃ£o &lt;em&gt;hands-on&lt;/em&gt; utilizando &lt;em&gt;notebooks&lt;/em&gt; do Colab.&lt;/p&gt;

&lt;h1 id=&quot;artigos-e-postagens-ï¸&quot;&gt;Artigos e Postagens âœï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A Deep Dive into the Reformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Madison May wrote a [new blog post](https://www.pragmatic.ml/reformer-deep-dive/) that provides a deep dive into [Reformer](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html), which is a new and improved Transformer-based model recently proposed by Google AI. We also featured Reformer in a [previous issue](https://medium.com/dair-ai/nlp-newsletter-reformer-deepmath-electra-tinybert-for-search-vizseq-open-sourcing-ml-68d5b6eed057) of the newsletter. --&gt;

&lt;p&gt;&lt;br /&gt;
Madison May realizou uma &lt;a href=&quot;https://www.pragmatic.ml/reformer-deep-dive/&quot;&gt;postagem&lt;/a&gt; em seu &lt;em&gt;blog&lt;/em&gt; que fornece uma anÃ¡lise mais profunda do &lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;Reformer&lt;/a&gt;, um novo modelo baseado no &lt;em&gt;Transformer&lt;/em&gt;, proposto recentemente pela Google AI. O &lt;em&gt;Reformer&lt;/em&gt; jÃ¡ havia aparecido numa &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-reformer-deepmath-electra-tinybert-for-search-vizseq-open-sourcing-ml-68d5b6eed057&quot;&gt;ediÃ§Ã£o anterior&lt;/a&gt; da Newsletter.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Uma plataforma de blogs gratuita&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A &lt;a href=&quot;https://fastpages.fast.ai/fastpages/jupyter/2020/02/21/introducing-fastpages.html&quot;&gt;fastpages&lt;/a&gt; permite a criaÃ§Ã£o e configuraÃ§Ã£o automÃ¡tica de um &lt;em&gt;blog&lt;/em&gt; utilizando a &lt;em&gt;GitHub pages&lt;/em&gt; de maneira gratuita. Essa soluÃ§Ã£o simplifica o processo de publicaÃ§Ã£o e tambÃ©m oferece suporte Ã  utilizaÃ§Ã£o de documentos exportados e &lt;em&gt;Jupyter notebooks&lt;/em&gt;.&lt;/p&gt;

&lt;!-- allows you to automatically set up a blog using GitHub pages for free. This solution simplifies the process of publishing a blog and it also supports the use of exported word documents and Jupyter notebooks. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Dicas para entrevistas na Google&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Pablo Castro, from the Google Brain team, published an [excellent blog post](https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html) highlighting a list of tips for those interested in interviewing for a job at Google. Topics include advice on how to prepare for the interview, what to expect during the interview, and what happens after the interview. --&gt;

&lt;p&gt;&lt;br /&gt;
Pablo Castro, do time da Google Brain, publicou uma &lt;a href=&quot;https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html&quot;&gt;excelente postagem&lt;/a&gt; destacando as principais dicas para aqueles interessados em aplicar para uma posiÃ§Ã£o na Google. Os tÃ³picos abordados incluem dicas sobre o processo de entrevistas, como preparaÃ§Ã£o, o que esperar durante e o que acontece depois delas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Transformers are Graph Neural Networks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Both graph neural networks (GNNs) and Transformers have shown to be effective at different NLP tasks. To better understand the inner workings behind these approaches and how they relate, Chaitanya Joshi wrote a great [article](https://graphdeeplearning.github.io/post/transformers-are-gnns/) explaining the connection between GNNs and Transformers and different ways these methods can be combined in a sort of hybrid model. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;Graph Neural Networks (GNNs)&lt;/em&gt; e &lt;em&gt;Transformers&lt;/em&gt; mostraram-se bastante efetivos em diversas tarefas de NLP. Com o objetivo de compreender melhor o funcionamento interno dessas arquiteturas e como elas se relacionam, Chaitanya Joshi escreveu um excelente &lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;artigo&lt;/a&gt; em seu &lt;em&gt;blog&lt;/em&gt;, evidenciando a conexÃ£o entre GNNs e &lt;em&gt;Transformers&lt;/em&gt;, e as diversas maneiras pelas quais esses mÃ©todos podem ser combinados e utilizados em conjunto.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*u-BkejfKSKcnWOBx.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;RepresentaÃ§Ã£o de uma frase como um grafo completo de palavrasâ€Šâ€”&lt;/em&gt;â€Š&lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;CNNs e EquivariÃ¢ncia&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Fabian Fuchs and Ed Wagstaff [discuss](https://fabianfuchsml.github.io/equivariance1of2/) the importance of equivariance and how CNNs enforce it. The concept of equivariance is first defined and then discussed in the context of CNNs with respect to translation. --&gt;

&lt;p&gt;&lt;br /&gt;
Fabian Fuchs e Ed Wagstaff &lt;a href=&quot;https://fabianfuchsml.github.io/equivariance1of2/&quot;&gt;discutiram&lt;/a&gt; a importÃ¢ncia da equivariÃ¢ncia e como as &lt;em&gt;Convolutional Neural Networks (CNNs)&lt;/em&gt; garantem essa propriedade. O conceito Ã© apresentado e discutido posteriormente no contexto de CNNs em relaÃ§Ã£o Ã  translaÃ§Ã£o.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Self-supervised learning com imagens&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A tÃ©cnica de &lt;em&gt;self-supervised learning&lt;/em&gt; foi amplamente discutida nas ediÃ§Ãµes anteriores da Newsletter devido ao seu papel em modelos recentes para &lt;em&gt;language modeling&lt;/em&gt;. Esse &lt;a href=&quot;https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%e7%bd%91/&quot;&gt;&lt;em&gt;blog post&lt;/em&gt;&lt;/a&gt;, feito pelo Jonathan Whitaker, fornece uma explicaÃ§Ã£o intuitiva da tÃ©cnica de aprendizado no contexto de imagens. Se vocÃª deseja um conhecimento mais profundo sobre o assunto, o Amit Chaudhary tambÃ©m publicou um &lt;a href=&quot;https://amitness.com/2020/02/illustrated-self-supervised-learning/&quot;&gt;artigo interessante&lt;/a&gt; descrevendo o conceito de maneira visual.&lt;/p&gt;

&lt;!-- Self-supervised has been discussed a lot in previous issues of the NLP Newsletter due to the role it has played in modern techniques for language modeling. This [blog post](https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%e7%bd%91/) by Jonathan Whitaker provides a nice and intuitive explanation of self-supervision in the context of images. If you are really interested in the topic, Amit Chaudhary also wrote an excellent [blog post](https://amitness.com/2020/02/illustrated-self-supervised-learning/) describing the concept in a visual way. --&gt;

&lt;h1 id=&quot;educaÃ§Ã£o-&quot;&gt;EducaÃ§Ã£o ğŸ“&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Stanford CS330: Deep Multi-Task and Meta-Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A universidade de Stanford liberou recentemente suas vÃ­deo-aulas, numa &lt;em&gt;playlist&lt;/em&gt; no YouTube, para o &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&quot;&gt;novo curso em &lt;em&gt;deep multi-task e meta-learning&lt;/em&gt;&lt;/a&gt;. Os assuntos apresentados incluem &lt;em&gt;bayesian meta-learning&lt;/em&gt;, &lt;em&gt;lifelong learning&lt;/em&gt;, uma visÃ£o geral sobre aprendizado por reforÃ§o, &lt;em&gt;model-based reinforcement learning&lt;/em&gt;, entre outros.&lt;/p&gt;

&lt;!-- Stanford recently released video recordings, in the form of a YouTube playlist, for their new [course on deep multi-task and meta-learning](https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5). Topics include bayesian meta-learning, lifelong learning, a reinforcement learning primer, model-based reinforcement learning, among others. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PyTorch Notebooks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A dar.ai liberou recentemente um &lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;compilado de &lt;em&gt;notebooks&lt;/em&gt;&lt;/a&gt; apresentando uma introduÃ§Ã£o Ã  redes neurais profundas utilizando o PyTorch. O trabalho continua em desenvolvimento, e alguns dos tÃ³picos jÃ¡ disponÃ­veis incluem como implementar um modelo de regressÃ£o logÃ­stica do zero, assim como a programaÃ§Ã£o de redes neurais &lt;em&gt;feed-forward&lt;/em&gt; e recorrentes. Notebooks no Colab estÃ£o disponÃ­veis no GitHub.&lt;/p&gt;

&lt;!-- dair.ai releases a [series of notebooks](https://github.com/dair-ai/pytorch_notebooks) that aim to get you started with deep neural networks using PyTorch. This is a work in progress and some current topics include how to implement a logistic regression model from scratch and how to program a neural network or recurrent neural network from scratch. Colab notebooks are also available in the GitHub repository. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;The fastai book (draft)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jeremy Howard e Sylvain Gugger liberaram uma &lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;lista&lt;/a&gt; com alguns &lt;em&gt;notebooks&lt;/em&gt; para um futuro curso que introduz conceitos de &lt;em&gt;Deep Learning&lt;/em&gt; e como implementar diferentes mÃ©todos utilizando o PyTorch e a biblioteca da fastai.&lt;/p&gt;

&lt;!-- Jeremy Howard and Sylvain Gugger release a [comprehensive list](https://github.com/fastai/fastbook) of draft notebooks for an upcoming course that introduces deep learning concepts and how to develop different methods using PyTorch and the fastai library. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Cursos gratuitos de CiÃªncia de Dados&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- In case you missed it, Kaggle provides a series of [free micro-courses](https://www.kaggle.com/learn/overview) to get you started with your Data Science journey. Some of these courses include machine learning explainability, an intro to machine learning and Python, data visualization, feature engineering, and deep learning, among others. --&gt;

&lt;p&gt;&lt;br /&gt;
O Kaggle disponibilizou uma sÃ©rie de [mini-cursos gratuitos]https://www.kaggle.com/learn/overview) para o pontapÃ© inicial da sua carreira como Cientista de Dados. Os cursos abordam assuntos como Explicabilidade em ML, IntroduÃ§Ã£o ao Aprendizado de MÃ¡quina e ao Python, VisualizaÃ§Ã£o de Dados, &lt;em&gt;Feature Engineering&lt;/em&gt;, &lt;em&gt;Deep Learning&lt;/em&gt;, entre outros.&lt;/p&gt;

&lt;!-- Here is another excellent [online data science course](https://lewtun.github.io/dslectures/) that provides a syllabus, slides, and notebooks on topics that range from exploratory data analysis to model interpretation to natural language processing. --&gt;

&lt;p&gt;&lt;br /&gt;
Um outro &lt;a href=&quot;https://lewtun.github.io/dslectures/&quot;&gt;excelente curso online&lt;/a&gt; de CiÃªncia de Dados disponibiliza notas de aulas, &lt;em&gt;slides&lt;/em&gt; e &lt;em&gt;notebooks&lt;/em&gt; sobre tÃ³picos que vÃ£o desde anÃ¡lise exploratÃ³ria atÃ© interpretaÃ§Ã£o de modelos para Processamento de Linguagem Natural.&lt;/p&gt;

&lt;!-- ***8 Creators and Core Contributors Talk About Their Model Training Libraries From PyTorch Ecosystem*** --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;8 Criadores e Colaboradores discutem suas bibliotecas de treinamento de modelos no ecossistema do PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- nepture.ai published an [extensive article](https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&amp;utm_medium=tweet&amp;utm_campaign=blog-model-training-libraries-pytorch-ecosystem) that contains detailed discussions with core creators and contributors about their journey and philosophy of building PyTorch and tools around it. --&gt;

&lt;p&gt;&lt;br /&gt;
A nepture.ai publicou um &lt;a href=&quot;https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&amp;amp;utm_medium=tweet&amp;amp;utm_campaign=blog-model-training-libraries-pytorch-ecosystem&quot;&gt;excelente artigo&lt;/a&gt; que contÃ©m discussÃµes detalhadas com criadores e colaboradores sobre suas jornadas e a filosofia utilizada na criaÃ§Ã£o do PyTorch e nas ferramentas construÃ­das com base na biblioteca.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Visualizando Adaptive Sparse Attention Models&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Sasha Rush shares an impressive [Colab notebook](http://Visualizing%20Adaptive%20Sparse%20Attention%20Models) that explains and shows the technical details of how to produce sparse softmax outputs and induce sparsity into the attention component of a Transformer model which helps to produce zero probability for irrelevant words in a given context, improving performance and interpretability all at once. --&gt;

&lt;p&gt;&lt;br /&gt;
Sashs Rush compartilhou um &lt;a href=&quot;https://colab.research.google.com/drive/1EB7MI_3gzAR1gFwPPO27YU9uYzE_odSu&quot;&gt;notebook impressionante&lt;/a&gt; que explica e mostra os detalhes tÃ©cnicos sobre como produzir saÃ­das esparsas com a softmax e induzir esparsidade nos componentes de atenÃ§Ã£o do modelo &lt;em&gt;Transformer&lt;/em&gt;, auxiliando na atribuiÃ§Ã£o de probabilidade zero para palavras irrelevantes num dado contexto, melhorando simultaneamente o desempenho e a interpretabilidade.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*7BB322LlVgt1zzk-cviSoA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Visualizando a distribuiÃ§Ã£o de probabilidade da saÃ­da da softmax&lt;/em&gt;
&lt;!-- *Visualizing probability distribution of a softmax output* --&gt;&lt;/p&gt;

&lt;h1 id=&quot;menÃ§Ãµes-honrosas-ï¸&quot;&gt;MenÃ§Ãµes Honrosas â­ï¸&lt;/h1&gt;

&lt;!-- You can access the previous issue of the ğŸ— NLP Newsletter [here](https://medium.com/dair-ai/nlp-newsletter-the-annotated-gpt-2-understanding-self-distillation-haiku-ganilla-sparkwiki-b0f47f595c82). --&gt;

&lt;p&gt;VocÃª pode conferir a ediÃ§Ã£o da passada da ğŸ— Newsletter &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-the-annotated-gpt-2-understanding-self-distillation-haiku-ganilla-sparkwiki-b0f47f595c82&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;!-- Conor Bell wrote this nice [python script](https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4) that allows you to view and prepare a dataset that can be used for a StyleGAN model. --&gt;

&lt;p&gt;&lt;br /&gt;
Conor Bell escreveu esse &lt;a href=&quot;https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4&quot;&gt;script em Python&lt;/a&gt; que permite a visualizaÃ§Ã£o e preparaÃ§Ã£o de uma base de dados que pode ser utilizada no modelo StyleGAN.&lt;/p&gt;

&lt;!-- Manu Romero [contributes](https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos) a fine-tuned POS model for Spanish. The model is available for use in the Hugging Face Transformer library. It will be interesting to see this effort in other languages. --&gt;

&lt;p&gt;&lt;br /&gt;
Manu Romero &lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos&quot;&gt;compartilhou&lt;/a&gt; um modelo de POS tagging para o espanhol. O modelo estÃ¡ disponÃ­vel para uso utilizando a biblioteca &lt;em&gt;Transformers&lt;/em&gt; da Hugging Face. SerÃ¡ interessante acompanhar a divulgaÃ§Ã£o de modelos para outros idiomas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Esse &lt;a href=&quot;https://github.com/tomohideshibata/BERT-related-papers&quot;&gt;repositÃ³rio&lt;/a&gt; contÃ©m uma extensa lista de artigos, cuidadosamente selecionados, que possuem relaÃ§Ã£o com o BERT e abordam diversos problemas como compressÃ£o de modelos, tarefas de domÃ­nios especÃ­ficos, entre outros.&lt;/p&gt;

&lt;!-- This [repo](https://github.com/tomohideshibata/BERT-related-papers) contains a long list of carefully curated BERT-related papers that approach different problems such as model compression, domain-specific, multi-model, generation, downstream tasks, etc. --&gt;

&lt;!-- Connor Shorten published a short [15-minute video](https://www.youtube.com/watch?time_continue=79&amp;v=-Bh_7tzyoR4&amp;feature=emb_logo) explaining a new general framework that aims to reduce the effect of â€œshortcutâ€ features in self-supervised representation learning. This is important because if not done right, the model can fail to learn useful semantic representations and potentially prove ineffective in a transfer learning setting. --&gt;

&lt;p&gt;&lt;br /&gt;
Connor Shorten publicou um &lt;a href=&quot;https://www.youtube.com/watch?time_continue=79&amp;amp;v=-Bh_7tzyoR4&amp;amp;feature=emb_logo&quot;&gt;vÃ­deo de 15 minutos&lt;/a&gt; explicando um novo &lt;em&gt;framework&lt;/em&gt; que busca reduzir o efeito das &lt;em&gt;â€œshortcutâ€ features&lt;/em&gt; no &lt;em&gt;self-supervised representation learning&lt;/em&gt;. Essa Ã© uma tarefa importante porquÃª, caso nÃ£o seja realizada corretamente, o modelo pode falhar em aprender representaÃ§Ãµes semÃ¢nticas Ãºteis e potencialmente se tornar ineficiente durante o &lt;em&gt;transfer learning&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Ruder publicou uma nova ediÃ§Ã£o da newsletter &lt;em&gt;NLP News&lt;/em&gt;, que apresenta tÃ³picos e recursos como anÃ¡lises de artigos de ML e NLP em 2019, e apresentaÃ§Ãµes sobre os fundamentos do &lt;em&gt;Deep Learning&lt;/em&gt; e &lt;em&gt;Transfer Learning&lt;/em&gt;. Confira &lt;a href=&quot;http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;!-- Sebastian Ruder published a new issue of the NLP News newsletter that highlights topics and resources that range from an analysis of NLP and ML papers in 2019 to slides for learning about transfer learning and deep learning essentials. Check it out [here](http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Inscreva-se&lt;/em&gt;&lt;/a&gt; &lt;em&gt;ğŸ”– para receber as prÃ³ximas ediÃ§Ãµes na sua caixa de entrada!&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5/&quot;&gt;NLP Newsletter [PT-BR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 02, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLPç®€æŠ¥ï¼ˆIssue#5ï¼‰ï¼šThe Annotated GPT-2ã€CodeBERTã€JAXã€GANILLAç­‰]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP%E7%AE%80%E6%8A%A5-Issue-5-The_Annotated_GPT-2-CodeBERT-JAX-GA/" />
  <id>https://dair.ai/NLPç®€æŠ¥ï¼ˆIssue#5ï¼‰ï¼šThe_Annotated_GPT-2ã€CodeBERTã€JAXã€GA</id>
  <published>2020-02-29T00:00:00-06:00</published>
  <updated>2020-02-29T00:00:00-06:00</updated>
  <author>
    <name>Elvis Saravia</name>
    <uri>https://dair.ai</uri>
    <email>ellfae@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
æ¬¢è¿æ¥åˆ° NLP æ—¶äº‹ç®€æŠ¥ï¼å…¨æ–‡è¾ƒé•¿ï¼Œå»ºè®®æ”¶è—ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;å¦‚æœæƒ³è®©è‡ªå·±æœ‰è¶£çš„ç ”ç©¶/é¡¹ç›®å‡ºç°åœ¨NLPç®€æŠ¥ä¸­ï¼Œéšæ—¶åœ¨å…¬ä¼—å·åå°ç•™è¨€è”ç³»æˆ‘&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
æ¥çœ‹çœ‹éƒ½æœ‰å“ªäº›å†…å®¹ï¼Œenjoy&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;1ã€Publications ğŸ“™&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;1.1 ç†è§£self-distillation&lt;/li&gt;
      &lt;li&gt;1.2 æ·±åº¦å­¦ä¹ åå¹´ç®€å²&lt;/li&gt;
      &lt;li&gt;1.3 åˆ©ç”¨ç¥ç»ç½‘ç»œæ±‚è§£é«˜ç­‰æ•°å­¦æ–¹ç¨‹&lt;/li&gt;
      &lt;li&gt;1.4 CodeBERT&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;2ã€Creativity and Society ğŸ¨&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;2.1 AI for scientific discovery&lt;/li&gt;
      &lt;li&gt;2.2 æ”¹å–„image-to-illustration&lt;/li&gt;
      &lt;li&gt;2.3 Andrew Ngè°ˆè‡ªç›‘ç£å­¦ä¹ &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;3ã€Tools and Datasets âš™ï¸&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;3.1 JAX libraries&lt;/li&gt;
      &lt;li&gt;3.2 å¤„ç†ç»´åŸºç™¾ç§‘æ•°æ®çš„å·¥å…·&lt;/li&gt;
      &lt;li&gt;3.3 Rust Tokenizers, DistilBERT base cased&lt;/li&gt;
      &lt;li&gt;3.4 å¤¸å¤¸è¯­æ–™&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;4ã€Ethics in AI ğŸš¨&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;4.1 NLPå’ŒMLæ¨¡å‹çš„é“å¾·è€ƒé‡&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;5ã€Articles and Blog posts âœï¸&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;5.1 The Annotated GPT-2&lt;/li&gt;
      &lt;li&gt;5.2 Beyond BERT?&lt;/li&gt;
      &lt;li&gt;5.3 çŸ©é˜µå‹ç¼©ç®—å­&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;6ã€Education ğŸ“&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;6.1 NLPåŸºç¡€&lt;/li&gt;
      &lt;li&gt;6.2 æ•°å­¦åŸºç¡€è¯¾&lt;/li&gt;
      &lt;li&gt;6.3 ä¹¦ç±æ¨è&lt;/li&gt;
      &lt;li&gt;6.4 è®¡ç®—æœºç§‘å­¦è‡ªå­¦æŒ‡å—&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;7ã€Noteworthy Mentions â­ï¸&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1publications-&quot;&gt;&lt;strong&gt;1ã€Publications ğŸ“™&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1.1 ç†è§£self-distillation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œself-distillation[1]æ˜¯å°†çŸ¥è¯†ä»ä¸€ç§æ¶æ„è½¬ç§»åˆ°å¦ä¸€ç§ç›¸åŒæ¶æ„çš„è¿‡ç¨‹ã€‚åœ¨è®­ç»ƒæ—¶ï¼ŒåŸå§‹æ¨¡å‹çš„é¢„æµ‹ä½œä¸ºç›®æ ‡å€¼æä¾›ç»™å¦ä¸€ä¸ªæ¨¡å‹ã€‚é™¤å…·æœ‰æ‰€éœ€çš„å±æ€§ï¼ˆä¾‹å¦‚å‡å°æ¨¡å‹å¤§å°ï¼‰å¤–ï¼Œç»éªŒç»“æœè¿˜è¡¨æ˜è¯¥æ–¹æ³•åœ¨held-out datasetsä¸Šæ•ˆæœå¾ˆå¥½ã€‚&lt;/p&gt;

&lt;p&gt;ä¸€ç»„ç ”ç©¶äººå‘˜æœ€è¿‘å‘è¡¨äº†ä¸€ç¯‡è®ºæ–‡ï¼ŒSelf-Distillation Amplifies Regularization in Hilbert Space[2]ï¼Œæä¾›äº†ç†è®ºåˆ†æï¼Œé‡ç‚¹æ˜¯æ›´å¥½åœ°äº†è§£çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä¸­æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ä»¥åŠä¸ºä½•æœ‰æ•ˆã€‚ç»“æœè¡¨æ˜ï¼Œå‡ è½®self-distillationä¼šé€šè¿‡é€æ¸é™åˆ¶ä»£è¡¨è§£çš„åŸºå‡½æ•°çš„æ•°é‡æ”¾å¤§æ­£åˆ™åŒ–ï¼Œè¿™å¾€å¾€ä¼šå‡å°‘è¿‡åº¦æ‹Ÿåˆã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.2 æ·±åº¦å­¦ä¹ åå¹´ç®€å²&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
äººå·¥æ™ºèƒ½çš„å…ˆé©±ã€LSTMä¹‹çˆ¶JÃ¼rgenSchmidhuberæœ€è¿‘å‘å¸ƒäº†ä¸€ä¸ªæ–°åšå®¢ï¼ŒThe 2010s: Our Decade of Deep Learning / Outlook on the 2020s[3]ï¼Œæä¾›è‡ª2010å¹´ä»¥æ¥çš„æ·±åº¦å­¦ä¹ å†å²æ¦‚è¿°ï¼ŒåŒ…æ‹¬LSTMï¼Œå‰é¦ˆç¥ç»ç½‘ç»œï¼ŒGANï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œå…ƒå­¦ä¹ ï¼Œä¸–ç•Œæ¨¡å‹ ï¼Œè’¸é¦ç¥ç»ç½‘ç»œï¼Œæ³¨æ„å­¦ä¹ ç­‰ä¸€äº›ä¸»é¢˜ã€‚æ–‡ç« æœ€åæ€»ç»“äº†2020å¹´ä»£çš„å‰æ™¯ï¼Œé¼“åŠ±äººä»¬å…³æ³¨ç´§è¿«çš„é—®é¢˜ï¼Œä¾‹å¦‚éšç§å’Œæ•°æ®å¸‚åœºã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.3 åˆ©ç”¨ç¥ç»ç½‘ç»œæ±‚è§£é«˜ç­‰æ•°å­¦æ–¹ç¨‹&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Facebook AIç ”ç©¶äººå‘˜å‘è¡¨äº†ä¸€ç¯‡è®ºæ–‡ï¼ŒDeep Learning for Symbolic Mathematics[4]ï¼Œå£°ç§°æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹æ•°å­¦é—®é¢˜å’ŒåŒ¹é…è§£å†³æ–¹æ¡ˆè¿›è¡Œè®­ç»ƒçš„æ¨¡å‹ï¼Œä»¥å­¦ä¹ é¢„æµ‹è¯¸å¦‚è§£å†³é›†æˆé—®é¢˜ä¹‹ç±»çš„ä»»åŠ¡çš„å¯èƒ½è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•åŸºäºç±»ä¼¼äºæœºå™¨ç¿»è¯‘ä¸­ä½¿ç”¨çš„æ–°é¢–æ¡†æ¶ï¼Œåœ¨è¯¥æ¡†æ¶ä¸­ï¼Œæ•°å­¦è¡¨è¾¾å¼è¡¨ç¤ºä¸ºä¸€ç§è¯­è¨€ï¼Œè€Œè§£å†³æ–¹æ¡ˆåˆ™è§†ä¸ºç¿»è¯‘é—®é¢˜ã€‚å› æ­¤ï¼Œè¾“å‡ºæ˜¯è§£å†³æ–¹æ¡ˆæœ¬èº«ï¼Œè€Œä¸æ˜¯æ¨¡å‹è¾“å‡ºç¿»è¯‘ã€‚æ®æ­¤ï¼Œç ”ç©¶äººå‘˜å£°ç§°æ·±åº¦ç¥ç»ç½‘ç»œä¸ä»…æ“…é•¿ç¬¦å·æ¨ç†ï¼Œè€Œä¸”è¿˜å¯ä»¥èƒœä»»å„ç§ä»»åŠ¡ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.4 CodeBERT&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
åœ¨è¿™ç¯‡åä¸ºã€ŠCodeBERT: A Pre-Trained Model for Programming and Natural Languagesã€‹[5]çš„è®ºæ–‡ä¸­ï¼Œæ¥è‡ªå“ˆå·¥å¤§ã€ä¸­å±±å¤§å­¦å’Œå¾®è½¯çš„ç ”ç©¶äººå‘˜è¯¦ç»†ä»‹ç»äº†è¿™ä¸€æ–°é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯å¤„ç†åŒæ¨¡æ€æ•°æ®ï¼šç¼–ç¨‹è¯­è¨€ï¼ˆPLï¼‰å’Œè‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
CodeBERT å­¦ä¹ èƒ½å¤Ÿæ”¯æŒä¸‹æ¸¸ NL-PL åº”ç”¨çš„é€šç”¨è¡¨ç¤ºï¼Œæ¯”å¦‚è‡ªç„¶è¯­è¨€ä»£ç æœç´¢ã€ä»£ç æ–‡æ¡£ç”Ÿæˆï¼Œç»å®éªŒ CodeBERT æ¨¡å‹åœ¨ä¸¤é¡¹ä»»åŠ¡å‡å–å¾— SOTA æ•ˆæœï¼ŒåŒæ—¶ç ”ç©¶è€…æ„å»ºäº† NL-PL æ¢æµ‹æ•°æ®é›†ï¼ŒCodeBERT åœ¨ zero-shot è®¾ç½®ä¸­çš„æ€§èƒ½è¡¨ç°ä¹ŸæŒç»­ä¼˜äº RoBERTaã€‚&lt;/p&gt;

&lt;h2 id=&quot;2creativity-and-society-&quot;&gt;&lt;strong&gt;2ã€Creativity and Society ğŸ¨&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;2.1 AI for scientific discovery&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Mattew HutsonæŠ¥å‘Šäº†å¦‚ä½•ä½¿ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¥ç”Ÿæˆä»¿çœŸå™¨[6]ï¼Œè¿™äº›ä»¿çœŸå™¨åœ¨å¯¹å¤æ‚è‡ªç„¶ç°è±¡è¿›è¡Œå»ºæ¨¡æ–¹é¢å…·æœ‰é‡è¦ä½œç”¨ï¼Œè€Œè‡ªç„¶ç°è±¡åˆå¯èƒ½å¯¼è‡´ä¸åŒç±»å‹çš„ç§‘å­¦å‘ç°ã€‚æ„å»ºè¿™äº›ä»¿çœŸå™¨çš„å˜åŒ–æ˜¯ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦å¤§è§„æ¨¡æ•°æ®å’Œå¹¿æ³›çš„å‚æ•°æ¢ç´¢ã€‚æœ€è¿‘çš„è®ºæ–‡æå‡ºäº†DENSEæ–¹æ³•[7]ï¼Œä¸€ç§åŸºäºç¥ç»ç»“æ„æœç´¢[8]æ¥æ„å»ºå‡†ç¡®çš„ä»¿çœŸå™¨ï¼Œè€Œä»…ä¾èµ–æœ‰é™æ•°é‡çš„è®­ç»ƒæ•°æ®ã€‚ä»–ä»¬é€šè¿‡å¯¹åŒ…æ‹¬å¤©ä½“ç‰©ç†å­¦ï¼Œæ°”å€™ç§‘å­¦å’Œèšå˜èƒ½ç­‰åœ¨å†…çš„æ¡ˆä¾‹è¿›è¡Œä»¿çœŸæ¥å¯¹å…¶è¿›è¡Œæµ‹è¯•ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;2.2 æ”¹å–„image-to-illustration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
GANILLA[9]æ˜¯ä¸€ç§ä½¿ç”¨GANæ¥æ”¹è¿›æœªé…å¯¹çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»»åŠ¡[10]ä¸­æ ·å¼å’Œå†…å®¹ä¼ é€’çš„æ–¹æ³•ã€‚æå‡ºäº†ä¸€ç§å…·æœ‰æ”¹è¿›çš„ç”Ÿæˆå™¨ç½‘ç»œç”¨äºå›¾åƒåˆ°æ’å›¾çš„æ¨¡å‹ï¼Œå¹¶åŸºäºæ–°çš„å®šé‡è¯„ä¼°æ¡†æ¶å¯¹æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯¥æ¡†æ¶åŒæ—¶è€ƒè™‘äº†å†…å®¹å’Œæ ·å¼ã€‚è¿™é¡¹å·¥ä½œçš„æ–°é¢–æ€§åœ¨äºæ‹Ÿè®®çš„ç”Ÿæˆå™¨ç½‘ç»œï¼Œè¯¥ç”Ÿæˆå™¨ç½‘ç»œè€ƒè™‘äº†å…ˆå‰æ¨¡å‹æ— æ³•å®ç°çš„æ ·å¼å’Œå†…å®¹ä¹‹é—´çš„å¹³è¡¡ã€‚å¯ä»¥åœ¨æ­¤å¤„é˜…è¯»åŸæ–‡ï¼šGANILLA: Generative Adversarial Networks for Image to Illustration Translation[11]&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;2.3 Andrew Ngè°ˆè‡ªç›‘ç£å­¦ä¹ &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
deeplearning.aiçš„åˆ›å§‹äººAndrew NgåŠ å…¥äººå·¥æ™ºèƒ½æ’­å®¢[12]ï¼Œè®¨è®ºçš„ä¸»é¢˜åŒ…æ‹¬ä»–æ—©æœŸä»äº‹MLçš„å·¥ä½œï¼ŒAIçš„æœªæ¥å’ŒAIæ•™è‚²ï¼Œæ­£ç¡®ä½¿ç”¨MLçš„å»ºè®®ï¼Œä»–çš„ä¸ªäººç›®æ ‡ä»¥åŠåœ¨2020å¹´ä»£åº”è¯¥å…³æ³¨MLæŠ€æœ¯ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrewè§£é‡Šäº†ä¸ºä»€ä¹ˆä»–å¯¹è‡ªç›‘ç£çš„è¡¨ç¤ºå­¦ä¹ æ„Ÿåˆ°éå¸¸å…´å¥‹ã€‚è‡ªç›‘ç£å¼å­¦ä¹ æ¶‰åŠä¸€ä¸ªå­¦ä¹ é—®é¢˜ï¼Œè¯¥é—®é¢˜æ—¨åœ¨ä»æ•°æ®æœ¬èº«è·å¾—ç›‘ç£ï¼Œä»¥åˆ©ç”¨å¤§é‡æœªæ ‡è®°æ•°æ®ï¼Œè¿™æ¯”çº¯å‡€æ ‡è®°æ•°æ®æ›´å¸¸è§ã€‚è¿™äº›è¡¨ç¤ºå¾ˆé‡è¦ï¼Œå¯ç”¨äºå¤„ç†ä¸‹æ¸¸ä»»åŠ¡ï¼Œç±»ä¼¼äºBERTç­‰è¯­è¨€æ¨¡å‹ä¸­ä½¿ç”¨çš„ä»»åŠ¡ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ æ¥å­¦ä¹ å¹¿ä¹‰çš„è§†è§‰è¡¨ç¤ºä¹Ÿå¼•èµ·äº†å¾ˆå¤§å…³æ³¨ï¼Œè¿™ä½¿æ¨¡å‹åœ¨èµ„æºåŒ®ä¹çš„ç¯å¢ƒä¸­æ›´åŠ å‡†ç¡®ã€‚ä¾‹å¦‚ï¼Œæœ€è¿‘ä¸€ç§åä¸ºSimCLR[13]çš„æ–¹æ³•ï¼ˆç”±Geoffrey Hintoné¢†å¯¼ï¼‰æå‡ºäº†ä¸€ç§è§†è§‰è¡¨ç¤ºçš„å¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œä»¥æ”¹å–„åœ¨ä¸åŒç¯å¢ƒä¸‹çš„å›¾åƒåˆ†ç±»ç»“æœï¼Œä¾‹å¦‚è½¬ç§»å­¦ä¹ å’ŒåŠç›‘ç£å­¦ä¹ ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3tools-and-datasets-ï¸&quot;&gt;&lt;strong&gt;3ã€Tools and Datasets âš™ï¸&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;3.1 JAX libraries&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
JAX[14]æ˜¯ä¸€ä¸ªæ–°åº“ï¼Œç»“åˆäº†NumPyå’Œè‡ªåŠ¨å¾®åˆ†åŠŸèƒ½ï¼Œå¯ä»¥è¿›è¡Œé«˜æ€§èƒ½MLç ”ç©¶ã€‚ä¸ºäº†ç®€åŒ–ä½¿ç”¨JAXæ„å»ºç¥ç»ç½‘ç»œçš„ç®¡é“ï¼ŒDeepMindå‘å¸ƒäº†Haiku[15]å’ŒRLax[16]ã€‚ä½¿ç”¨ç†Ÿæ‚‰çš„é¢å‘å¯¹è±¡ç¼–ç¨‹æ¨¡å‹ï¼ŒRLaxç®€åŒ–äº†å¼ºåŒ–å­¦ä¹ ä»£ç†çš„å®ç°ï¼Œè€ŒHaikuç®€åŒ–äº†ç¥ç»ç½‘ç»œçš„æ„å»ºã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.2 å¤„ç†ç»´åŸºç™¾ç§‘æ•°æ®çš„å·¥å…·&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sparkwiki[17] æ˜¯å¤„ç†Wikipediaæ•°æ®çš„å·¥å…·ã€‚æ­¤ç‰ˆæœ¬æ˜¯æœ‰è¶£çš„è¡Œä¸ºåˆ†æç ”ç©¶çš„ä¼—å¤šåŠªåŠ›çš„ä¸€éƒ¨åˆ†ï¼Œä¾‹å¦‚ï¼Œæ•è·è·¨ä¸åŒè¯­è¨€ç‰ˆæœ¬çš„Wikipediaçš„è¶‹åŠ¿å’Œè¯­è¨€åè§[18]ã€‚ä½œè€…å‘ç°ï¼Œç‹¬ç«‹äºè¯­è¨€ï¼Œç»´åŸºç™¾ç§‘ç”¨æˆ·çš„æµè§ˆè¡Œä¸ºè¡¨æ˜ï¼Œä»–ä»¬å€¾å‘äºåœ¨ç”µå½±ï¼ŒéŸ³ä¹å’Œä½“è‚²ç­‰ç±»åˆ«ä¸Šæ‹¥æœ‰å…±åŒçš„å…´è¶£ï¼Œä½†æ˜¯éšç€å½“åœ°äº‹ä»¶å’Œæ–‡åŒ–ç‰¹æ®Šæ€§çš„å‡ºç°ï¼Œå·®å¼‚å˜å¾—æ›´åŠ æ˜æ˜¾ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.3 Rust Tokenizers, DistilBERT base cased, Model cards&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hugging Faceå‘è¡Œçš„æ–°ç‰ˆTransformers[19]åŒ…æ‹¬å…¶å¿«é€Ÿåˆ†è¯å™¨åº“çš„é›†æˆï¼Œè¯¥åº“æ—¨åœ¨åŠ é€ŸBERTï¼ŒRoBERTaï¼ŒGPT2ç­‰æ¨¡å‹ä»¥åŠå…¶ä»–ç¤¾åŒºæ„å»ºçš„æ¨¡å‹ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.4 å¤¸å¤¸è¯­æ–™&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å¤¸å¤¸è¯­æ–™[20]ï¼Œæ¥è‡ªè±†ç“£äº’ç›¸è¡¨æ‰¬ç»„æ•°æ®ã€‚&lt;/p&gt;

&lt;h2 id=&quot;4ethics-in-ai-&quot;&gt;&lt;strong&gt;4ã€Ethics in AI ğŸš¨&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;4.1 NLPå’ŒMLæ¨¡å‹çš„é“å¾·è€ƒé‡&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
åœ¨NLP Highlightsçš„æ–°å†…å®¹ä¸­[21]ï¼ŒEmily Benderå’Œä¸»æŒäººè®¨è®ºäº†åœ¨å­¦æœ¯ç•Œå’Œå®é™…ä½¿ç”¨æƒ…å†µä¸‹å¼€å‘NLPæ¨¡å‹å’ŒæŠ€æœ¯æ—¶çš„ä¸€äº›é“å¾·è€ƒé‡ã€‚è®¨è®ºä¸­çš„ä¸€äº›ä¸»é¢˜åŒ…æ‹¬è®¾è®¡NLPä»»åŠ¡ï¼Œæ•°æ®æ”¶é›†æ–¹æ³•ä»¥åŠæœ€ç»ˆå‘å¸ƒç»“æœæ—¶çš„é“å¾·è€ƒè™‘ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
é™¤äº†ä¸Šè¿°æ‰€æœ‰è€ƒè™‘å› ç´ ä¹‹å¤–ï¼ŒAIç¤¾åŒºä¸­ç»å¸¸è®¨è®ºçš„ä¸€ä¸ªé—®é¢˜è¿‡äºå…³æ³¨ä¼˜åŒ–æŒ‡æ ‡ï¼Œè¿™ä¸AIæ—¨åœ¨å®ç°çš„ç›®æ ‡èƒŒé“è€Œé©°ã€‚Rachel Thomaså’ŒDavid Uminsky[22]è®¨è®ºäº†é€šè¿‡å¯¹ä¸åŒç”¨ä¾‹è¿›è¡Œé€å½»åˆ†æè€Œå¯èƒ½å‡ºé”™çš„åœ°æ–¹ã€‚ä»–ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªç¼“è§£è¯¥é—®é¢˜çš„ç®€å•æ¡†æ¶ï¼Œå…¶ä¸­æ¶‰åŠå¤šä¸ªæŒ‡æ ‡çš„ä½¿ç”¨å’Œç»„åˆï¼Œç„¶åæ˜¯é‚£äº›ç›´æ¥å—åˆ°è¯¥æŠ€æœ¯å½±å“çš„äººçš„å‚ä¸ã€‚&lt;/p&gt;

&lt;h2 id=&quot;5articles-and-blog-posts-ï¸&quot;&gt;&lt;strong&gt;5ã€Articles and Blog posts âœï¸&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;5.1 The Annotated GPT-2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aman Aroraæœ€è¿‘å‘è¡¨äº†ä¸€ç¯‡ç‰¹åˆ«çš„åšå®¢æ–‡ç« ï¼Œæ ‡é¢˜ä¸ºâ€œ The Annotated GPT-2[23]â€ï¼Œè§£é‡Šäº†åŸºäºTransformerçš„æ¨¡å‹GPT-2çš„å†…éƒ¨å·¥ä½œåŸç†ã€‚ä»–çš„æ–¹æ³•å—åˆ°The Annotated Transformer[24]çš„å¯å‘ï¼Œé‡‡ç”¨äº†æ³¨é‡Šæ–¹æ³•ï¼Œé€šè¿‡ä»£ç å’Œæ˜“äºç†è§£çš„è§£é‡Šæ¥è§£é‡Šæ¨¡å‹çš„é‡è¦éƒ¨åˆ†ã€‚Amanä»˜å‡ºäº†å·¨å¤§çš„åŠªåŠ›ï¼Œä½¿ç”¨PyTorchå’ŒHugging Faceçš„Transformersåº“é‡æ–°å®ç°OpenAIçš„GPT-2ã€‚è¿™æ˜¯å‡ºè‰²çš„å·¥ä½œï¼&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;5.2 Beyond BERT?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sergi Castella[25]å¯¹BERTä»¥å¤–çš„å†…å®¹æ„Ÿå…´è¶£ã€‚ä¸»è¦ä¸»é¢˜åŒ…æ‹¬æ”¹å–„æŒ‡æ ‡ï¼ŒHugging Faceçš„Transformersåº“å¦‚ä½•æ”¯æŒç ”ç©¶ï¼ŒæŸ¥çœ‹æœ‰è¶£çš„æ•°æ®é›†ï¼Œè§£å‹ç¼©æ¨¡å‹ç­‰ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;5.3 çŸ©é˜µå‹ç¼©ç®—å­&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
TensorFlowåšå®¢å‘å¸ƒäº†ä¸€ç¯‡åšå®¢æ–‡ç« ï¼ŒMatrix Compression Operator[26]ï¼Œè§£é‡Šäº†åœ¨æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­å‹ç¼©çŸ©é˜µèƒŒåçš„æŠ€æœ¯å’Œé‡è¦æ€§ã€‚çŸ©é˜µå‹ç¼©å¯ä»¥å¸®åŠ©æ„å»ºæ›´æœ‰æ•ˆçš„å¾®å‹æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥é›†æˆåˆ°è¾ƒå°çš„è®¾å¤‡ä¸­ï¼Œä¾‹å¦‚ç”µè¯å’Œå®¶åº­åŠ©ç†ã€‚é€šè¿‡ä½ç§©é€¼è¿‘å’Œé‡åŒ–ç­‰æ–¹æ³•ä¸“æ³¨äºæ¨¡å‹çš„å‹ç¼©ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æ— éœ€ç‰ºç‰²æ¨¡å‹çš„è´¨é‡ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;6education-&quot;&gt;&lt;strong&gt;6ã€Education ğŸ“&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;6.1 NLPåŸºç¡€&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
NLPåŸºç¡€[27]ä»åŸºç¡€å¼€å§‹è®²æˆNLPæ¦‚å¿µï¼ŒåŒæ—¶åˆ†äº«æœ€ä½³å®è·µï¼Œé‡è¦å‚è€ƒï¼Œåº”é¿å…çš„å¸¸è§é”™è¯¯ä»¥åŠNLPçš„æœªæ¥ã€‚åŒ…å«ä¸€ä¸ªColabç¬”è®°æœ¬[28]ï¼Œè¯¥é¡¹ç›®å°†åœ¨æ­¤github[29]ç»´æŠ¤ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.2 æ•°å­¦åŸºç¡€è¯¾&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Machine Learning Tokyo å°†åœ¨3æœˆ8æ—¥ä¸»æŒä¸€ä¸ªè¿œç¨‹åœ¨çº¿è®¨è®ºï¼Œå…¶ä¸­å›é¡¾ä»–ä»¬æœ€è¿‘çš„åœ¨çº¿å­¦ä¹ è¯¾ç¨‹ä¸­[30]æ¶‰åŠçš„ç« èŠ‚ã€‚è¯¥å°ç»„ä»¥å‰ç ”ç©¶è¿‡Marc Peter Deisenrothï¼ŒAdo Faisalå’ŒCheng Soon Ongæ‰€è‘—çš„ã€Šæœºå™¨å­¦ä¹ æ•°å­¦ã€‹[31]ä¸€ä¹¦ç« èŠ‚ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.3 ä¹¦ç±æ¨è&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
åœ¨ä¸Šä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†çŸ©é˜µå‹ç¼©å¯¹äºæ„å»ºå¾®å‹MLæ¨¡å‹çš„é‡è¦æ€§ã€‚å¦‚æœä½ æœ‰å…´è¶£äº†è§£æœ‰å…³å¦‚ä½•ä¸ºåµŒå…¥å¼ç³»ç»Ÿæ„å»ºæ›´å°çš„æ·±åº¦ç¥ç»ç½‘ç»œçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹Pete Wardenå’ŒDaniel Situnayakeæ’°å†™çš„è¿™æœ¬åä¸ºTinyML[32]çš„å‡ºè‰²è‘—ä½œã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å¦ä¸€æœ¬å€¼å¾—å…³æ³¨çš„æœ‰è¶£ä¹¦ç±æ˜¯å³å°†å‡ºç‰ˆçš„é¢˜ä¸ºâ€œDeep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD[33]â€ï¼Œä½œè€…ä¸ºJeremy Howardå’ŒSylvain Guggerã€‚è¯¥ä¹¦æ—¨åœ¨æä¾›å¿…è¦çš„æ•°å­¦åŸºç¡€ï¼Œä»¥å»ºç«‹å’Œè®­ç»ƒæ¨¡å‹æ¥å¤„ç†è®¡ç®—æœºè§†è§‰å’ŒNLPé¢†åŸŸä¸­çš„ä»»åŠ¡ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.4 è®¡ç®—æœºç§‘å­¦è‡ªå­¦æŒ‡å—&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
ä½¿ç”¨å»ºè®®çš„æ•™ç§‘ä¹¦æˆ–è§†é¢‘è®²åº§ç³»åˆ—ï¼Œä»¥å¤§è‡´é¡ºåºå­¦ä¹ ä»¥ä¸‹æ‰€æœ‰ä¹ä¸ªä¸»é¢˜ï¼Œä½†ç†æƒ³æƒ…å†µä¸‹ä¸¤è€…éƒ½å­¦ä¹ ã€‚é’ˆå¯¹æ¯ä¸ªä¸»é¢˜è¿›è¡Œ100-200å°æ—¶çš„å­¦ä¹ ï¼Œç„¶ååœ¨æ•´ä¸ªèŒä¸šç”Ÿæ¶¯ä¸­é‡æ¸©æœ€çˆ±favoritesã€‚å¦å¤–åœ¨redditä¸Šä¹Ÿæœ‰ç±»ä¼¼çš„è®¨è®º[34]ã€‚&lt;/p&gt;

&lt;h2 id=&quot;7noteworthy-mentions-ï¸&quot;&gt;&lt;strong&gt;7ã€Noteworthy Mentions â­ï¸&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Torchmeta[35]æ˜¯ä¸€ä¸ªæ˜¯ç”±Tristan Deleuåˆ›ä½œçš„å¯ä»¥è½»æ¾ä½¿ç”¨ç›¸å…³çš„æ•°æ®åŠ è½½å™¨è¿›è¡Œå…ƒå­¦ä¹ ç ”ç©¶çš„åº“ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manuel Tonneauæ’°å†™äº†ä¸€ç¯‡æ–‡ç« ï¼Œä»”ç»†ç ”ç©¶äº†è¯­è¨€å»ºæ¨¡ä¸­æ¶‰åŠçš„ä¸€äº›æœºåˆ¶[36]ï¼ŒåŒ…æ‹¬è´ªå©ªå’Œæ³¢æŸæœç´¢ä»¥åŠåŸå­æ ¸é‡‡æ ·ç­‰ä¸»é¢˜ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
MITå‘å¸ƒäº†åä¸ºâ€œIntroduction to Deep Learning[37]â€çš„è¯¾ç¨‹çš„å®Œæ•´æçº²å’Œè¯¾ç¨‹è¡¨ï¼Œå…¶ä¸­åŒ…æ‹¬å·²æˆè¯¾çš„è§†é¢‘ï¼Œ ä»–ä»¬çš„ç›®æ ‡æ˜¯æ¯å‘¨å‘å¸ƒè§†é¢‘è®²åº§å’Œå¹»ç¯ç‰‡ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
äº†è§£å¦‚ä½•ä½¿ç”¨åŸºäºTransformerçš„æ–¹æ³•åœ¨ä¸åˆ°300è¡Œä»£ç ä¸­è®­ç»ƒç”¨äºå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰çš„æ¨¡å‹[38]ã€‚æ‚¨å¯ä»¥åœ¨æ­¤å¤„æ‰¾åˆ°éšé™„çš„Google Colab[39]ã€‚&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;æœ¬æ–‡å‚è€ƒèµ„æ–™&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[1] &lt;strong&gt;self-distillation:&lt;/strong&gt; https://arxiv.org/pdf/1503.02531.pdf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[2] &lt;strong&gt;Self-Distillation Amplifies Regularization in Hilbert Space:&lt;/strong&gt; http://xxx.itp.ac.cn/abs/2002.05715&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[3] &lt;strong&gt;The 2010s: Our Decade of Deep Learning / Outlook on the 2020s:&lt;/strong&gt; http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[4] &lt;strong&gt;Deep Learning for Symbolic Mathematics:&lt;/strong&gt; https://arxiv.org/abs/1912.01412&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[5] &lt;strong&gt;ã€ŠCodeBERT: A Pre-Trained Model for Programming and Natural Languagesã€‹:&lt;/strong&gt; https://arxiv.org/abs/2002.08155&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[6] &lt;strong&gt;å¦‚ä½•ä½¿ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¥ç”Ÿæˆä»¿çœŸå™¨:&lt;/strong&gt; https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[7] &lt;strong&gt;è®ºæ–‡æå‡ºäº†DENSEæ–¹æ³•:&lt;/strong&gt; https://arxiv.org/abs/2001.08055&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[8] &lt;strong&gt;ç¥ç»ç»“æ„æœç´¢:&lt;/strong&gt; https://en.wikipedia.org/wiki/Neural_architecture_search&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[9] &lt;strong&gt;GANILLA:&lt;/strong&gt; https://github.com/giddyyupp/ganilla&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[10] &lt;strong&gt;å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»»åŠ¡:&lt;/strong&gt; https://paperswithcode.com/task/image-to-image-translation&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[11] &lt;strong&gt;GANILLA: Generative Adversarial Networks for Image to Illustration Translation:&lt;/strong&gt; https://arxiv.org/abs/2002.05638&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[12] &lt;strong&gt;äººå·¥æ™ºèƒ½æ’­å®¢:&lt;/strong&gt; https://www.youtube.com/watch?v=0jspaMLxBig&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[13] &lt;strong&gt;SimCLR:&lt;/strong&gt; https://arxiv.org/abs/2002.05709&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[14] &lt;strong&gt;JAX:&lt;/strong&gt; https://github.com/google/jax&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[15] &lt;strong&gt;Haiku:&lt;/strong&gt; https://github.com/deepmind/dm-haiku&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[16] &lt;strong&gt;RLax:&lt;/strong&gt; https://github.com/deepmind/rlax&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[17] &lt;strong&gt;Sparkwiki:&lt;/strong&gt; https://github.com/epfl-lts2/sparkwiki&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[18] &lt;strong&gt;æ•è·è·¨ä¸åŒè¯­è¨€ç‰ˆæœ¬çš„Wikipediaçš„è¶‹åŠ¿å’Œè¯­è¨€åè§:&lt;/strong&gt; https://arxiv.org/abs/2002.06885&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[19] &lt;strong&gt;æ–°ç‰ˆTransformers:&lt;/strong&gt; https://github.com/huggingface/transformers/releases/tag/v2.5.0&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[20] &lt;strong&gt;å¤¸å¤¸è¯­æ–™:&lt;/strong&gt; https://github.com/xiaopangxia/kuakua_corpus&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[21] &lt;strong&gt;NLP Highlightsçš„æ–°å†…å®¹ä¸­:&lt;/strong&gt; https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[22] &lt;strong&gt;Rachel Thomaså’ŒDavid Uminsky:&lt;/strong&gt; https://arxiv.org/abs/2002.08512&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[23] &lt;strong&gt;The Annotated GPT-2:&lt;/strong&gt; https://amaarora.github.io/2020/02/18/annotatedGPT2.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[24] &lt;strong&gt;The Annotated Transformer:&lt;/strong&gt; https://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[25] &lt;strong&gt;Sergi Castella:&lt;/strong&gt; https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[26] &lt;strong&gt;Matrix Compression Operator:&lt;/strong&gt; https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[27] &lt;strong&gt;NLPåŸºç¡€:&lt;/strong&gt; https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[28] &lt;strong&gt;Colabç¬”è®°æœ¬:&lt;/strong&gt; https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[29] &lt;strong&gt;æ­¤github:&lt;/strong&gt; https://github.com/dair-ai/nlp_fundamentals&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[30] &lt;strong&gt;åœ¨çº¿å­¦ä¹ è¯¾ç¨‹ä¸­:&lt;/strong&gt; https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[31] &lt;strong&gt;ã€Šæœºå™¨å­¦ä¹ æ•°å­¦ã€‹:&lt;/strong&gt; https://mml-book.github.io/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[32] &lt;strong&gt;TinyML:&lt;/strong&gt; https://tinymlbook.com/?linkId=82595412&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[33] &lt;strong&gt;Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD:&lt;/strong&gt; https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[34] &lt;strong&gt;redditä¸Šä¹Ÿæœ‰ç±»ä¼¼çš„è®¨è®º:&lt;/strong&gt; https://www.reddit.com/r/learnprogramming/comments/87j7fw/teach_yourself_computer_science_a_diy_curriculum/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[35] &lt;strong&gt;Torchmeta:&lt;/strong&gt; https://arxiv.org/abs/1909.06576&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[36] &lt;strong&gt;è¯­è¨€å»ºæ¨¡ä¸­æ¶‰åŠçš„ä¸€äº›æœºåˆ¶:&lt;/strong&gt; https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[37] &lt;strong&gt;Introduction to Deep Learning:&lt;/strong&gt; http://introtodeeplearning.com/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[38] &lt;strong&gt;è®­ç»ƒç”¨äºå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰çš„æ¨¡å‹:&lt;/strong&gt; https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[39] &lt;strong&gt;éšé™„çš„Google Colab:&lt;/strong&gt; https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP%E7%AE%80%E6%8A%A5-Issue-5-The_Annotated_GPT-2-CodeBERT-JAX-GA/&quot;&gt;NLPç®€æŠ¥ï¼ˆIssue#5ï¼‰ï¼šThe Annotated GPT-2ã€CodeBERTã€JAXã€GANILLAç­‰&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 29, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, Ã‰tica em NLP, Torchmeta,â€¦]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/" />
  <id>https://dair.ai/NLP_Newsletter[PT-BR]_The_Annotated_GPT-2,_Understanding</id>
  <published>2020-02-29T00:00:00-06:00</published>
  <updated>2020-02-29T00:00:00-06:00</updated>
  <author>
    <name>Flavio Clesio</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Antes de tudo, gostaria de agradecer de â¤ï¸ a todos vocÃªs pelo incrÃ­vel apoio e incentivo para continuar com a NLP Newsletter. Esse esforÃ§o requer pesquisa, ediÃ§Ã£o, e traduÃ§Ã£o tediosas, mas que considero gratificantes e Ãºteis para fornecer o melhor conteÃºdo. Espero que vocÃª esteja gostando deste conteÃºdo. ğŸ˜‰&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Assine a NLP Newsletter&lt;/em&gt;&lt;/a&gt; &lt;em&gt;ğŸ”– para receber ediÃ§Ãµes futuras via e-mail.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;publicaÃ§Ãµes-&quot;&gt;PublicaÃ§Ãµes ğŸ“™&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Um entendimento teÃ³rico do self-distillation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
No contexto de Deep Learning, &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;&gt;&lt;em&gt;self-distillation&lt;/em&gt;&lt;/a&gt; (&lt;em&gt;NT: auto-destilaÃ§Ã£o&lt;/em&gt;) Ã© o processo de transferÃªncia de conhecimento de uma arquitetura para outra. As previsÃµes do modelo original sÃ£o alimentadas como valores de destino para o outro modelo durante o treinamento. AlÃ©m de ter propriedades desejÃ¡veis como a reduÃ§Ã£o do tamanho dos modelos, os resultados empÃ­ricos mostram que essa abordagem funciona bem em conjuntos de dados nÃ£o vistos anteriormente pelo modelo (NT: amostras &lt;em&gt;held out&lt;/em&gt;). Um grupo de pesquisadores publicou recentemente um artigo que fornece uma anÃ¡lise teÃ³rica com o foco em um melhor entendimento sobre o que estÃ¡ acontecendo neste processo de &lt;em&gt;destilaÃ§Ã£o do conhecimento&lt;/em&gt; e o porque ele Ã© eficaz. Os resultados mostram que alguns poucos ciclos de destilaÃ§Ã£o amplificam a regularizaÃ§Ã£o (devido ao fato que a tÃ©cnica &lt;a href=&quot;https://twitter.com/TheGradient/status/1228132843630387201?s=20&quot;&gt;&lt;em&gt;progressivamente ajuda a limitar o nÃºmero de funÃ§Ãµes base que representam a soluÃ§Ã£o&lt;/em&gt;&lt;/a&gt;) as quais tendem a reduzir o over-fitting. (Leia o paper &lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;&lt;strong&gt;aqui&lt;/strong&gt;&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;&lt;em&gt;Fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Os anos 2010s: Nossa dÃ©cada de Deep Learning / Perspectivas para os 2020s&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;http://people.idsia.ch/~juergen/&quot;&gt;JÃ¼rgen Schmidhuber,&lt;/a&gt; um dos pioneiros em InteligÃªncia Artificial,  &lt;a href=&quot;http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html&quot;&gt;&lt;strong&gt;postou recentemente em seu blog&lt;/strong&gt;&lt;/a&gt; uma visÃ£o histÃ³rica sobre Deep Learning desde o ano de 2010. Alguns tÃ³picos incluem &lt;a href=&quot;https://en.wikipedia.org/wiki/Long_short-term_memory&quot;&gt;LSTMs&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;&gt;feedforward neural networks&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_adversarial_network&quot;&gt;GANs&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_reinforcement_learning&quot;&gt;deep reinforcement learning&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Meta_learning_(computer_science)&quot;&gt;meta-learning&lt;/a&gt;, world models, &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;&gt;distilling NNs&lt;/a&gt;, &lt;a href=&quot;https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&quot;&gt;attention learning&lt;/a&gt;, etc. O artigo traz algumas perspectivas futuras para os anos 2020 chamando atenÃ§Ã£o para questÃµes como privacidade e mercado de dados.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Usando Redes Neurais para a resoluÃ§Ã£o de equaÃ§Ãµes matemÃ¡ticas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pesquisadores do Facebook AI publicaram um &lt;a href=&quot;https://arxiv.org/abs/1912.01412&quot;&gt;&lt;strong&gt;paper&lt;/strong&gt;&lt;/a&gt; em que apresentam um modelo treinado em problemas de matemÃ¡tica para prever possÃ­veis soluÃ§Ãµes para inÃºmeras tarefas como, por exemplo, problemas de integraÃ§Ã£o. A abordagem Ã© baseada em uma nova estrutura semelhante Ã  usada na &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_machine_translation&quot;&gt;neural machine translation&lt;/a&gt; (&lt;em&gt;NT: traduÃ§Ã£o automÃ¡tica neural&lt;/em&gt;), em que expressÃµes matemÃ¡ticas sÃ£o representadas como um tipo de linguagem e as soluÃ§Ãµes tratadas como um problema de traduÃ§Ã£o. Assim, ao invÃ©s do modelo produzir uma traduÃ§Ã£o, a saÃ­da desta traduÃ§Ã£o Ã© a prÃ³pria soluÃ§Ã£o do problema. Com isso, os pesquisadores afirmam que as redes Deep Learning nÃ£o sÃ£o apenas boas em raciocÃ­nio simbÃ³lico, mas podem ser usadas tambÃ©m para tarefas mais diversas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;EquaÃ§Ãµes sendo usadas como entrada, juntamente com a soluÃ§Ã£o correspondente gerada pelo modeloâ€”&lt;/em&gt;â€Š&lt;a href=&quot;https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;criatividade-e-sociedade-&quot;&gt;Criatividade e Sociedade ğŸ¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;InteligÃªncia Artificial para descobertas cientÃ­ficas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Mattew Hutson &lt;a href=&quot;https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times&quot;&gt;&lt;strong&gt;informa&lt;/strong&gt;&lt;/a&gt; como a inteligÃªncia artificial (IA) pode ser utilizada para produzir emuladores que tÃªm um uso importante na modelagem de fenÃ´menos naturais complexos e que, por sua vez, podem levar a diferentes tipos de &lt;em&gt;descobertas cientÃ­ficas&lt;/em&gt;. A mudanÃ§a na construÃ§Ã£o desses emuladores acontece devido ao fato de que estes modelos geralmente exigem dados em larga escala e uma vasta exploraÃ§Ã£o de parÃ¢metros. Um &lt;a href=&quot;https://arxiv.org/abs/2001.08055&quot;&gt;&lt;strong&gt;paper recente&lt;/strong&gt;&lt;/a&gt; propÃµe um mÃ©todo chamado DENSE que Ã© uma abordagem baseada em &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_architecture_search&quot;&gt;&lt;em&gt;neural architecture search (NAS)&lt;/em&gt;&lt;/a&gt; (NT: ExploraÃ§Ã£o e busca de arquitetura de Redes Neurais) para criar emuladores precisos, contando apenas com uma quantidade limitada de dados de treinamento. Eles o testaram executando simulaÃ§Ãµes para casos que incluem astrofÃ­sica, ciÃªncia climÃ¡tica e energia de fusÃ£o, entre outros.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Melhorando a traduÃ§Ã£o de imagem para imagem&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/2002.05638&quot;&gt;GANILLA&lt;/a&gt; Ã© uma abordagem que propÃµe o uso de &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_adversarial_network&quot;&gt;GANs&lt;/a&gt; para melhorar a transferÃªncia de estilo e conteÃºdo em pares para tarefas de traduÃ§Ã£o &lt;a href=&quot;https://paperswithcode.com/task/image-to-image-translation&quot;&gt;&lt;em&gt;image-to-image&lt;/em&gt;&lt;/a&gt; (NT: imagem para imagem). A abordagem propÅe um modelo de imagem para imagem (com uma rede de geradores aprimorada) e este modelo Ã© avaliado com base em uma nova estrutura de avaliaÃ§Ã£o quantitativa que considera tanto o conteÃºdo quanto o estilo. A novidade do trabalho estÃ¡ na rede de geradores proposta, que considera um equilÃ­brio entre estilo e conteÃºdo que os modelos anteriores nÃ£o conseguem. O cÃ³digo e os modelos prÃ©-treinados estÃ£o &lt;a href=&quot;https://github.com/giddyyupp/ganilla&quot;&gt;disponÃ­veis&lt;/a&gt;. Leia o artigo completo &lt;a href=&quot;https://arxiv.org/abs/2002.05638&quot;&gt;&lt;strong&gt;aqui&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Andrew Ng fala sobre o interesse em aprendizagem auto-supervisionada&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew Ng, o fundador do &lt;a href=&quot;deeplearning.ai&quot;&gt;deeplearning.ai&lt;/a&gt;, falou no &lt;a href=&quot;https://www.youtube.com/watch?v=0jspaMLxBig&quot;&gt;&lt;strong&gt;podcast de InteligÃªncia Artificial do Lex Friedman&lt;/strong&gt;&lt;/a&gt; sobre os seguintes tÃ³picos: seus primeiros anos em ML, o futuro da IA, educaÃ§Ã£o em IA, recomendaÃ§Ãµes para o uso adequado da ML, seus objetivos pessoais e quais tÃ©cnicas de ML que devemos prestar atenÃ§Ã£o nesta dÃ©cada de 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew explicou o motivo da sua animaÃ§Ã£o em relaÃ§Ã£o ao &lt;em&gt;self-supervised representation learning.&lt;/em&gt; &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html&quot;&gt;&lt;strong&gt;Self-supervised learning&lt;/strong&gt;&lt;/a&gt; (NT: aprendizado de representaÃ§Ã£o auto-supervisionado) envolve a estruturaÃ§Ã£o de um problema de aprendizagem que visa obter supervisÃ£o dos prÃ³prios dados para fazer uso de grandes quantidades de dados nÃ£o rotulados, o que Ã© mais comum que os dados rotulados limpos. As representaÃ§Ãµes sÃ£o importantes e podem ser usadas para lidar com tarefas posteriores, semelhantes Ã s usadas em modelos de linguagem como o &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert&quot;&gt;BERT&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
TambÃ©m hÃ¡ muito interesse em usar o aprendizado auto-supervisionado para treinamento de representaÃ§Ãµes visuais generalizadas que tornam os modelos mais precisos em ambientes com poucos recursos. Por exemplo, um mÃ©todo recente chamado &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;&lt;strong&gt;SimCLR&lt;/strong&gt;&lt;/a&gt; (liderado por &lt;a href=&quot;https://en.wikipedia.org/wiki/Geoffrey_Hinton&quot;&gt;Geoffrey Hinton&lt;/a&gt;) propÃµe uma estrutura para &lt;em&gt;aprendizagem auto-supervisionada contrastante&lt;/em&gt; (&lt;em&gt;NT: contrastive self-supervised learning&lt;/em&gt;) de representaÃ§Ãµes visuais para melhorar a classificaÃ§Ã£o de imagens em diferentes configuraÃ§Ãµes, como transferÃªncia de aprendizado (NT: &lt;a href=&quot;https://en.wikipedia.org/wiki/Transfer_learning&quot;&gt;transfer learning&lt;/a&gt;) e aprendizado semi-supervisionado.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;ferramentas-e-datasets-ï¸&quot;&gt;Ferramentas e Datasets âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Bibliotecas JAX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/google/jax&quot;&gt;JAX&lt;/a&gt; Ã© uma nova biblioteca que combina o NumPy e &lt;a href=&quot;https://en.wikipedia.org/wiki/Automatic_differentiation&quot;&gt;diferenciaÃ§Ã£o automÃ¡tica&lt;/a&gt; para realizar pesquisas de ML de alto desempenho. Para simplificar os pipelines para a construÃ§Ã£o de redes neurais usando JAX, a &lt;a href=&quot;https://deepmind.com/&quot;&gt;DeepMind&lt;/a&gt; lanÃ§ou o &lt;a href=&quot;https://github.com/deepmind/dm-haiku&quot;&gt;&lt;strong&gt;Haiku&lt;/strong&gt;&lt;/a&gt; e &lt;a href=&quot;https://github.com/deepmind/rlax&quot;&gt;&lt;strong&gt;RLax&lt;/strong&gt;&lt;/a&gt;. O RLax simplifica a implementaÃ§Ã£o de agentes de aprendizado por reforÃ§o e o Haiku simplifica a construÃ§Ã£o de redes neurais usando &lt;em&gt;modelos familiares com o paradigma de programaÃ§Ã£o orientada a objetos.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Uma ferramenta para processar dados da WikipÃ©dia&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/epfl-lts2/sparkwiki&quot;&gt;&lt;strong&gt;Sparkwiki&lt;/strong&gt;&lt;/a&gt; Ã© uma ferramenta para processar dados da WikipÃ©dia. Esta versÃ£o faz parte de muitos esforÃ§os para permitir pesquisas interessantes de anÃ¡lise comportamental, como &lt;a href=&quot;https://arxiv.org/abs/2002.06885&quot;&gt;a captura de tendÃªncias e preconceitos em diferentes idiomas na WikipÃ©dia&lt;/a&gt;. Os autores descobriram que, independentemente do idioma, o comportamento de navegaÃ§Ã£o dos usuÃ¡rios da WikipÃ©dia mostra que eles tendem a compartilhar interesses comuns por categorias como filmes, mÃºsica e esportes, mas as diferenÃ§as se tornam mais aparentes com eventos locais e particularidades culturais.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Tokenizers em Rust, DistilBERT e outros&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Um novo release dos &lt;a href=&quot;https://github.com/huggingface/transformers/releases/tag/v2.5.0&quot;&gt;&lt;strong&gt;Transformers&lt;/strong&gt;&lt;/a&gt; da Hugging Face inclui a integraÃ§Ã£o de sua biblioteca de tokenizaÃ§Ã£o rÃ¡pida, que visa acelerar modelos como o &lt;a href=&quot;https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270&quot;&gt;BERT&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;&gt;RoBERTa&lt;/a&gt;, &lt;a href=&quot;https://openai.com/blog/better-language-models/&quot;&gt;GPT-2&lt;/a&gt; e outros modelos criados pela comunidade.&lt;/p&gt;

&lt;h1 id=&quot;Ã©tica-em-inteligÃªncia-artificial-&quot;&gt;Ã‰tica em InteligÃªncia Artificial ğŸš¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;ConsideraÃ§Ãµes Ã©ticas para modelos de NLP (Processamento de Linguagem Natural) e Machine Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Em um novo &lt;a href=&quot;https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender&quot;&gt;&lt;strong&gt;episÃ³dio&lt;/strong&gt;&lt;/a&gt; do postcast &lt;a href=&quot;https://soundcloud.com/nlp-highlights&quot;&gt;NLP Highlights,&lt;/a&gt; &lt;a href=&quot;https://twitter.com/emilymbender&quot;&gt;Emily Bender&lt;/a&gt; e os hosts conversaram sobre algumas consideraÃ§Ãµes Ã©ticas no desenvolvimento de modelos e tecnologias de NLP no contexto da academia e do seu uso no mundo real. Alguns dos tÃ³picos da discussÃ£o incluem consideraÃ§Ãµes Ã©ticas nas tarefas de NLP, abordagens sobre coleta de dados e eventualmente consideraÃ§Ãµes na publicaÃ§Ã£o de resultados.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
AlÃ©m de todas as consideraÃ§Ãµes acima, uma preocupaÃ§Ã£o discutida Ã© que a comunidade de IA estÃ¡ se concentrando demais na otimizaÃ§Ã£o de mÃ©tricas especÃ­ficas, o que contraria os objetivos que a IA pretende alcanÃ§ar. Rachel Thomas e David Uminsky discutem os problemas dessa abordagem atravÃ©s de uma &lt;a href=&quot;https://arxiv.org/abs/2002.08512&quot;&gt;&lt;strong&gt;anÃ¡lise completa&lt;/strong&gt;&lt;/a&gt; de diferentes casos de uso. Eles tambÃ©m propÃµem uma estrutura simples para mitigar este problema, que envolve o uso e a combinaÃ§Ã£o de vÃ¡rias mÃ©tricas, seguidas pelo envolvimento das pessoas afetadas diretamente pela tecnologia.&lt;/p&gt;

&lt;h1 id=&quot;artigos-e-blog-posts-ï¸&quot;&gt;Artigos e Blog posts âœï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;â€œThe Annotated GPT-2â€&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aman Arora publicou recentemente uma postagem no blog excepcionalmente intitulada &lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;â€œThe Annotated GPT-2â€œ&lt;/a&gt; explicando o funcionamento interno do modelo baseado em &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;&gt;Transformer&lt;/a&gt; chamado &lt;a href=&quot;https://openai.com/blog/better-language-models/&quot;&gt;GPT-2&lt;/a&gt;. Sua abordagem foi inspirada em &lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;The Annotated Transformer&lt;/a&gt; que adotou uma abordagem de anotaÃ§Ã£o para explicar as partes importantes do modelo. Aman fez um grande esforÃ§o para reimplementar o &lt;a href=&quot;https://openai.com/blog/better-language-models/&quot;&gt;GPT-2&lt;/a&gt; da &lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt; usando o &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt; e a biblioteca &lt;a href=&quot;https://huggingface.co/transformers/&quot;&gt;Transformers da Hugging Face&lt;/a&gt;. Ã‰ um trabalho brilhante!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;AlÃ©m do BERT?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1&quot;&gt;&lt;strong&gt;Um ponto interessante foi levantado&lt;/strong&gt;&lt;/a&gt; por Sergi Castella sobre o que estÃ¡ alÃ©m do &lt;a href=&quot;https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270&quot;&gt;BERT&lt;/a&gt;. Os principais tÃ³picos incluem o aprimoramento das mÃ©tricas, uma reflexÃ£o de como a biblioteca &lt;a href=&quot;https://huggingface.co/transformers/&quot;&gt;Transformers da Hugging Face&lt;/a&gt; ajuda na pesquisa, alguns conjuntos de dados interessantes para anÃ¡lise, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Operador de CompressÃ£o de Matrizes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O Blog do TensorFlow publicou um &lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;&lt;strong&gt;post&lt;/strong&gt;&lt;/a&gt; explicando as tÃ©cnicas e a importÃ¢ncia por trÃ¡s da compressÃ£o matrizes em um modelo de Deep Learning. &lt;em&gt;A compactaÃ§Ã£o matricial&lt;/em&gt; (&lt;em&gt;NT: Matrix compression&lt;/em&gt;) pode ajudar a criar modelos menores e mais eficientes que podem ser incorporados a dispositivos menores, como telefones e assistentes domÃ©sticos. Concentrar-se na compressÃ£o dos modelos por meio de mÃ©todos como &lt;a href=&quot;https://en.wikipedia.org/wiki/Low-rank_approximation&quot;&gt;low-rank-approximation&lt;/a&gt; e &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantization_(signal_processing)&quot;&gt;quantizaÃ§Ã£o&lt;/a&gt; significa que nÃ£o precisamos comprometer a qualidade do modelo.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;educaÃ§Ã£o-&quot;&gt;EducaÃ§Ã£o ğŸ“&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Fundamentos de NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Estou animado por lanÃ§ado um rascunho do CapÃ­tulo 1 da minha nova sÃ©rie chamado &lt;a href=&quot;https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentenÃ§a-segmentaÃ§Ã£o-b362c5d07684&quot;&gt;&lt;strong&gt;Fundamentos de NLP&lt;/strong&gt;&lt;/a&gt;. Esta sÃ©rie ensina conceitos de NLP a partir do bÃ¡sico, compartilhando boas prÃ¡ticas, referÃªncias importantes, erros comuns a serem evitados e o que estÃ¡ por vir no que se refere a NLP. Um &lt;a href=&quot;https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ&quot;&gt;notebook no Colab&lt;/a&gt; foi incluÃ­do e o projeto serÃ¡ mantido &lt;a href=&quot;https://github.com/dair-ai/nlp_fundamentals&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;RevisÃ£o/DiscussÃ£o Online: Parte I sessÃ£o de leitura para fundamentos da matemÃ¡tica&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O time do &lt;a href=&quot;https://www.meetup.com/Machine-Learning-Tokyo/&quot;&gt;Meetup â€œMachine Learning Tokyoâ€&lt;/a&gt; estÃ¡ hospedando uma discussÃ£o on-line remota, revisando capÃ­tulos que foram abordados em suas recentes sessÃµes de estudo on-line. O grupo jÃ¡ havia estudado capÃ­tulos com base no livro &lt;a href=&quot;https://mml-book.github.io/&quot;&gt;Mathematics For Machine Learning&lt;/a&gt; escrito por Marc Peter Deisenroth, A Aldo Faisal e Cheng Soon Ong. O &lt;a href=&quot;https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/&quot;&gt;&lt;strong&gt;evento&lt;/strong&gt;&lt;/a&gt; estÃ¡ programado para 8 de marÃ§o de 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;RecomendaÃ§Ãµes de livros&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Em um segmento anterior, discutimos a importÃ¢ncia da compressÃ£o de matriz para a construÃ§Ã£o de modelos pequenos (em termos de espaÃ§o) de ML. Se vocÃª estiver interessado em aprender mais sobre como construir redes neurais profundas menores para sistemas embarcados, confira este Ã³timo livro chamado &lt;a href=&quot;https://tinymlbook.com/?linkId=82595412&quot;&gt;&lt;strong&gt;TinyML&lt;/strong&gt;&lt;/a&gt; de Pete Warden e Daniel Situnayake.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Outro livro interessante para ficar de olho Ã© o prÃ³ximo tÃ­tulo  &lt;strong&gt;â€œ&lt;/strong&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;&lt;strong&gt;Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;â€&lt;/strong&gt; de Jeremy Howard e Sylvain Gugger. O livro tem como objetivo fornecer a base matemÃ¡tica necessÃ¡ria para criar e treinar modelos para abordar tarefas nas Ã¡reas de visÃ£o computacional e NLP.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;menÃ§Ãµes-honrosas-ï¸&quot;&gt;MenÃ§Ãµes honrosas â­ï¸&lt;/h1&gt;

&lt;p&gt;VocÃª pode acessar a NLP Newsletter anterior em PT-BR &lt;a href=&quot;https://dair.ai/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG/&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1909.06576&quot;&gt;&lt;strong&gt;Torchmeta&lt;/strong&gt;&lt;/a&gt; Ã© uma biblioteca para pesquisa em meta-aprendizado. Esta biblioteca Ã© de autoria de Tristan Deleu.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manuel Tonneau escreveu um &lt;a href=&quot;https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter&quot;&gt;&lt;strong&gt;post&lt;/strong&gt;&lt;/a&gt; oferecendo uma visÃ£o mais detalhada em relaÃ§Ã£o ao hardware envolvido em modelagem de linguagem. Alguns tÃ³picos incluem &lt;em&gt;greedy&lt;/em&gt; e &lt;a href=&quot;https://en.wikipedia.org/wiki/Beam_search&quot;&gt;beam search&lt;/a&gt; e &lt;a href=&quot;https://openreview.net/forum?id=rygGQyrFvH&quot;&gt;nucleus sampling&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O MIT &lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;&lt;strong&gt;lanÃ§ou&lt;/strong&gt;&lt;/a&gt; o plano de estudos completo e a programaÃ§Ã£o do curso intitulado â€œIntroduÃ§Ã£o ao Deep Learningâ€, incluindo vÃ­deos das palestras jÃ¡ ministradas. Eles pretendem lanÃ§ar palestras em vÃ­deo e slides uma vez por semana.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aprenda a treinar um modelo para &lt;a href=&quot;https://en.wikipedia.org/wiki/Named-entity_recognition&quot;&gt;reconhecimento de entidade (NER)&lt;/a&gt; usando uma abordagem baseada no Transformer em &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py&quot;&gt;&lt;strong&gt;300 linhas de cÃ³digo&lt;/strong&gt;&lt;/a&gt;. VocÃª pode encontrar o Google Colab em anexo &lt;a href=&quot;https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Se vocÃª tiver datasets, projetos, postagens de blog, tutoriais ou documentos que deseja compartilhar na prÃ³xima ediÃ§Ã£o da NLP Newsletter, entre em contato conosco pelo e-mail ellfae@gmail.com ou via &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;&lt;strong&gt;DM no Twitter&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Assine a NLP Newsletter&lt;/em&gt;&lt;/a&gt; &lt;em&gt;ğŸ”– para receber ediÃ§Ãµes futuras via e-mail.&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/&quot;&gt;NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, Ã‰tica em NLP, Torchmeta,â€¦&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 29, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø®Ø§Ù…Ø³ Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© (NLP) Ø§Ù„Ø¨Ø±ÙŠØ¯ÙŠØ©: GPT-2 Ø§Ù„Ù…Ø´Ø±ÙˆØ­ØŒ ÙÙ‡Ù… Ø§Ù„ØªÙ‚Ø·ÙŠØ±-Ø§Ù„Ø°Ø§ØªÙŠØŒ HaikuØŒ GANILLAØŒ SparkwikiØŒ Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ ÙÙŠ Ø¹Ù„Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ TorchmetaØŒ ÙˆØ§Ù„Ù…Ø²ÙŠØ¯ ...]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter.AR._The_Annotated_GPT-2_And_More/" />
  <id>https://dair.ai/NLP_Newsletter.AR._The_Annotated_GPT-2_And_More</id>
  <published>2020-02-28T00:00:00-06:00</published>
  <updated>2020-02-28T00:00:00-06:00</updated>
  <author>
    <name>Sufian Diraneyya</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ø¨Ø¯Ø§ÙŠØ©ØŒ Ù„Ø§ Ø£Ø³ØªØ·ÙŠØ¹ Ø´ÙƒØ±ÙƒÙ… â¤ï¸ Ø¬Ù…ÙŠØ¹Ø§Ù‹ ÙƒÙØ§ÙŠØ© Ù„Ù„Ø¯Ø¹Ù… Ø§Ù„Ù‡Ø§Ø¦Ù„ ÙˆØ§Ù„ØªØ´Ø¬ÙŠØ¹ Ù„Ù…ÙˆØ§ØµÙ„Ø© Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¨Ø±ÙŠØ¯ÙŠØ©. Ù‡Ø°Ø§ Ø§Ù„Ø¬Ù‡Ø¯ ÙŠØ­ØªØ§Ø¬ Ø¨Ø­Ø« ÙˆØªØ¹Ø¯ÙŠÙ„ Ù…Ø¶Ø¬Ø±ÙŠÙ†ØŒ Ø§Ù„Ø°ÙŠÙ† Ø£Ø±Ø§Ù‡Ù…Ø§ ÙŠØ³ØªØ­Ù‚Ø§Ù†ØŒ ÙˆÙŠØ³Ø§Ø¹Ø¯Ø§Ù† ÙÙŠ ØªØ²ÙˆÙŠØ¯ÙƒÙ… Ø¨Ø£ÙØ¶Ù„ Ù…Ø­ØªÙˆÙ‰. Ø£ØªÙ…Ù†Ù‰ Ø£Ù† ØªØ³ØªÙ…ØªØ¹ÙˆØ§ Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§ØªØŒ Ù„Ø£Ù†Ù†ÙŠ Ø£ÙØ¹Ù„ ğŸ˜‰.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;strong&gt;Ø§Ø´ØªØ±Ùƒ&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;ğŸ”– ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© NLP Ø§Ù„Ø¨Ø±ÙŠØ¯ÙŠØ© Ù„ØªØµÙ„Ùƒ Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù‚Ø§Ø¯Ù…Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ø¹Ù„Ù‰ ØµÙ†Ø¯ÙˆÙ‚ Ø¨Ø±ÙŠØ¯Ùƒ.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª-&quot;&gt;Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª ğŸ“™&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;ÙÙ‡Ù… Ù†Ø¸Ø±ÙŠ Ù„Ù„â€Ø§Ù„ØªÙ‚Ø·ÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠâ€ (Self-Distillation)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
ÙÙŠ Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ (Deep Learning)ØŒ Ø§Ù„ØªÙ‚Ø·ÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠ Ø£Ùˆ Ø§Ù„(&lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;&gt;&lt;strong&gt;Self-Distillation&lt;/strong&gt;&lt;/a&gt;) Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù‡ÙŠ Ø¹Ù…Ù„ÙŠØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ù…Ø¹Ù…Ø§Ø±ÙŠØ© (Architecture) Ù…Ø§ Ø¥Ù„Ù‰ Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø£Ø®Ø±Ù‰ Ù…Ù…Ø§Ø«Ù„Ø©. ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ØµÙ„ÙŠ ØªØªÙ… ØªØºØ°ÙŠØªÙ‡Ø§ ÙƒÙ‚ÙŠÙ… Ù…Ø³ØªÙ‡Ø¯ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¢Ø®Ø± Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨. Ø¨Ø¬Ø§Ù†Ø¨ ÙˆØ¬ÙˆØ¯ Ø¨Ø¹Ø¶ Ù…Ø²Ø§ÙŠØ§ØŒ Ù…Ø«Ù„ ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ØªÙØ¸Ù‡Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ø£Ù† Ù‡Ø°Ø§ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ ÙŠØ¹Ù…Ù„ Ø¬ÙŠØ¯Ø§Ù‹ Ø¹Ù„Ù‰ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙØµÙˆÙ„Ø© (Held-Out Datasets). Ù†Ø´Ø± Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¨Ø§Ø­Ø«ÙŠÙ† ÙˆØ±Ù‚Ø© Ø¨Ø­Ø«ÙŠØ© Ù…Ø¤Ø®Ø±Ø§Ù‹ ØªÙ‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ù†Ø¸Ø±ÙŠØ§Ù‹ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø£Ø¹Ù…Ù‚ Ù„Ù…Ø§ ÙŠØ­Ø¯Ø« ÙÙŠ Ø¹Ù…Ù„ÙŠØ© Ø§Ø³ØªØ®Ø±Ø§Ø¬ (â€œØªÙ‚Ø·ÙŠØ±â€) Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù‡Ø°Ù‡ ÙˆÙ„Ù…Ø§Ø°Ø§ Ù‡ÙŠ ÙØ¹Ø§Ù„Ø©. Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØªØ¸Ù‡Ø± Ø£Ù† Ø¨Ø¶Ø¹ Ø¬ÙˆÙ„Ø§Øª Ù…Ù† â€œØ§Ù„ØªÙ‚Ø·ÙŠØ±â€ ØªØ²ÙŠØ¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ… (Regularization) (Ø¹Ù† Ø·Ø±ÙŠÙ‚ &lt;a href=&quot;https://twitter.com/TheGradient/status/1228132843630387201?s=20&quot;&gt;&lt;strong&gt;Ø§Ù„Ø­Ø¯ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹ Ù…Ù† Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ…Ø«Ù„ Ø§Ù„Ø­Ù„&lt;/strong&gt;&lt;/a&gt;) Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙŠÙ„ Ø¥Ù„Ù‰ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ø§Ø¦Ù…Ø© Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© (Over-Fitting). (Ø§Ù‚Ø±Ø£ Ø§Ù„ÙˆØ±Ù‚Ø© ÙƒØ§Ù…Ù„Ø© (Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©) &lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;&lt;strong&gt;Ù‡Ù†Ø§&lt;/strong&gt;&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ø§Ù„Ø´ÙƒÙ„ 1: Ù…Ø®Ø·Ø· ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ‚Ø·ÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠ Ù„Ø¯ÙˆØ±ØªÙŠÙ†&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;&lt;em&gt;Ø§Ù„Ù…ØµØ¯Ø±&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Ø¹Ù‚Ø¯ 2010 Ø¥Ù„Ù‰ 2019: Ø¹Ù‚Ø¯Ù†Ø§ Ù…Ù† Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ / ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ø¹Ù‚Ø¯ Ø§Ù„Ù‚Ø§Ø¯Ù… 2020 - 2029&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;http://people.idsia.ch/~juergen/&quot;&gt;&lt;strong&gt;JÃ¼rgen Schmidhuber&lt;/strong&gt;&lt;/a&gt;ØŒ Ø±Ø§Ø¦Ø¯ ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ù†Ø´Ø± Ù…Ø¤Ø®Ø±Ø§Ù‹ &lt;a href=&quot;http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html&quot;&gt;&lt;strong&gt;ØªØ¯ÙˆÙŠÙ†Ø© Ø¬Ø¯ÙŠØ¯Ø©&lt;/strong&gt;&lt;/a&gt; Ù…Ø±ÙƒØ²Ø§Ù‹ Ø¹Ù„Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ù†Ø¸Ø±Ø© ØªØ§Ø±ÙŠØ®ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù…Ù† Ø¹Ø§Ù… 2010 ÙˆØ­ØªÙ‰ Ø§Ù„Ø¢Ù†. Ø°ÙƒØ± ÙÙŠÙ‡Ø§ Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ù…Ø«Ù„: ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ù‚ØµÙŠØ±Ø©-Ø§Ù„Ù…Ø¯Ù‰ (LSTMs)ØŒ Ø´Ø¨ÙƒØ§Øª Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© (Feedforward Neural Network)ØŒ Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø®ØµÙˆÙ…Ø© Ø§Ù„Ù…ÙˆÙ„ÙØ¯Ø© (Generative Adversarial Networks - GANs)ØŒ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ù…Ø¹Ø²Ø² Ø§Ù„Ø¹Ù…ÙŠÙ‚ (Deep Reinforcement Learning)ØŒ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„ÙÙˆÙ‚ÙŠ (Meta-Learning)ØŒ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¹Ø§Ù„Ù… (World Models)ØŒ ØªÙ‚Ø·ÙŠØ± Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© (Distilling NNs)ØŒ Ø§Ù„ØªØ¹Ù„Ù… Ø¨Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ (Attention Learning)ØŒ ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹. Ø§Ù„Ù…Ù‚Ø§Ù„Ø© Ø®ØªÙ…Øª Ø¨Ù†Ø¸Ø±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ù‚Ø§Ø¯Ù… Ø¨Ø¯Ø¦Ø§Ù‹ Ù…Ù† Ø¹Ø§Ù… 2020 Ù…Ø´Ø¬Ø¹Ø© Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ù…Ø´Ø§ÙƒÙ„ Ù…Ø«Ù„ Ø§Ù„Ø®ØµÙˆØµÙŠØ© ÙˆØ£Ø³ÙˆØ§Ù‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ù„Ø­Ù„ Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø±ÙŠØ§Ø¶ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ø¨Ø§Ø­Ø«Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ ÙÙŠØ³Ø¨ÙˆÙƒ Ù†Ø´Ø±ÙˆØ§ &lt;a href=&quot;https://arxiv.org/abs/1912.01412&quot;&gt;&lt;strong&gt;ÙˆØ±Ù‚Ø© Ø¨Ø­Ø«ÙŠØ©&lt;/strong&gt;&lt;/a&gt; ØªØ¯Ø¹ÙŠ Ø£Ù† ØªÙ‚Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬Ø§Ù‹ Ù…Ø¯Ø±Ø¨Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø¦Ù„ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© ÙˆÙ…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø­Ù„ÙˆÙ„ Ù„ØªØ¹Ù„Ù… ØªÙˆÙ‚Ø¹ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù…ÙƒÙ†Ø© Ù„Ù…Ù‡Ø§Ù… Ù…Ø«Ù„ Ø­Ù„ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„ØªÙƒØ§Ù…Ù„ÙŠØ©. Ù‡Ø°Ø§ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ (Framework) Ø¬Ø¯ÙŠØ¯ Ù…Ø´Ø§Ø¨Ù‡ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¢Ù„ÙŠØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© (Neural Machine Translation) Ø­ÙŠØ« ØªÙƒÙˆÙ† Ø§Ù„ØªØ¹Ø¨ÙŠØ±Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ù…Ù‚Ø¯Ù…Ø© ÙƒÙ„ØºØ© ÙˆØ§Ù„Ø­Ù„ÙˆÙ„ ØªØªÙ… Ù…Ø¹Ø§Ù…Ù„ØªÙ‡Ø§ ÙƒØ¹Ù…Ù„ÙŠØ© ØªØ±Ø¬Ù…Ø©. ÙˆÙ‡ÙƒØ°Ø§ØŒ Ø¨Ø¯Ù„ Ø£Ù† ÙŠØ®Ø±Ø¬ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØªØ±Ø¬Ù…Ø©ØŒ Ø§Ù„Ù…Ø®Ø±Ø¬ ÙŠÙƒÙˆÙ† Ø­Ù„ Ø§Ù„Ù…Ø³Ø£Ù„Ø©. Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©ØŒ ÙŠØ¯Ø¹ÙŠ Ø§Ù„Ø¨Ø§Ø­Ø«ÙˆÙ† Ø£Ù† Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© Ù„ÙŠØ³Øª Ø¬ÙŠØ¯Ø© ÙÙ‚Ø· ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø±Ù…Ø²ÙŠ (Symbolic Reasoning) Ø¨Ù„ Ø£ÙŠØ¶Ø§Ù‹ ÙÙŠ Ù…Ù‡Ø§Ù… Ø£ÙƒØ«Ø± ØªÙ†ÙˆØ¹Ø§Ù‹.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù…ØºØ°Ø§Ø© ÙƒÙ…Ø¯Ø®Ù„Ø§Øª ÙˆØ§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ø§Ù„Ø°ÙŠ Ø£Ø®Ø±Ø¬Ù‡Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/&quot;&gt;&lt;em&gt;Ø§Ù„Ù…ØµØ¯Ø±&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹-ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹-&quot;&gt;Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ ÙˆØ§Ù„Ù…Ø¬ØªÙ…Ø¹ ğŸ¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„Ø§ÙƒØªØ´Ø§ÙØ§Øª Ø§Ù„Ø¹Ù„Ù…ÙŠØ©&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Mattew Hutson Ù‚Ø¯Ù… &lt;a href=&quot;https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times&quot;&gt;&lt;strong&gt;ØªÙ‚Ø±ÙŠØ±Ø§Ù‹&lt;/strong&gt;&lt;/a&gt; Ø¹Ù† ÙƒÙŠÙ Ø£Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (A.I.) ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ù†ØªØ§Ø¬ Ù…Ø­Ø§ÙƒÙŠØ§Øª Ø°Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù‡Ù… ÙÙŠ Ù†Ù…Ø°Ø¬Ø© Ø¸ÙˆØ§Ù‡Ø± Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…Ø¹Ù‚Ø¯Ø©ØŒ Ø§Ù„ØªÙŠ Ø¨Ø¯ÙˆØ±Ù‡Ø§ Ø£Ø¯Øª Ø¥Ù„Ù‰ Ø£Ù†ÙˆØ§Ø¹ Ø£Ø®Ø±Ù‰ Ù…Ù† Ø§Ù„Ø§ÙƒØªØ´Ø§ÙØ§Øª Ø§Ù„Ø¹Ù„Ù…ÙŠØ©. Ø§Ù„ØªØºÙŠÙŠØ± ÙÙŠ Ø¨Ù†Ø§Ø¡ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø­Ø§ÙƒÙŠØ§Øª Ù‡Ùˆ Ø£Ù†Ù‡Ù… ØºØ§Ù„Ø¨Ø§Ù‹ Ù…Ø§ ÙŠØ­ØªØ§Ø¬ÙˆÙ† Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ù†Ø·Ø§Ù‚ ÙˆØ§Ø³Ø¹ (Large-Scale Data) ÙˆØ§Ø³ØªÙƒØ´Ø§Ù Ù…Ø¹Ø§Ù…Ù„Ø§Øª (Parameter Exploration) Ù…ÙƒØ«Ù. &lt;a href=&quot;https://arxiv.org/abs/2001.08055&quot;&gt;&lt;strong&gt;ÙˆØ±Ù‚Ø© Ø¨Ø­Ø«ÙŠØ© Ø­Ø¯ÙŠØ«Ø©&lt;/strong&gt;&lt;/a&gt; Ø§Ù‚ØªØ±Ø­Øª DENSEØŒ Ø£Ø³Ù„ÙˆØ¨ Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø¨Ø­Ø« Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¹ØµØ¨ÙŠØ© (Neural Architecture Search) Ù„Ø¨Ù†Ø§Ø¡ Ù…Ø­Ø§ÙƒÙŠØ§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ù…Ø¹ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ù‚Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯ Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨. Ø¬Ø±Ø¨ÙˆÙ‡Ø§ Ø¨ØªØ´ØºÙŠÙ„ Ù…Ø­Ø§ÙƒØ§Ø© Ù„Ø­Ø§Ù„Ø§Øª Ù…Ø«Ù„: Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¡ Ø§Ù„ÙÙ„ÙƒÙŠØ©ØŒ Ø¹Ù„Ù… Ø§Ù„Ù…Ù†Ø§Ø®ØŒ Ø·Ø§Ù‚Ø© Ø§Ù„Ø§Ù†Ø¯Ù…Ø§Ø¬ (Fusion)ØŒ ÙˆØºÙŠØ±Ù‡Ø§.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;ØªØ­Ø³ÙŠÙ† ØªØ±Ø¬Ù…Ø© ØµÙˆØ±Ø©-Ø¥Ù„Ù‰-Ø±Ø³Ù…Ø© (Image-to-illustration Translation)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
GANILLA Ù‡Ùˆ Ø£Ø³Ù„ÙˆØ¨ ÙŠÙ‚ØªØ±Ø­ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø®ØµÙˆÙ…Ø© Ø§Ù„Ù…ÙˆÙ„ÙØ¯Ø© (Generative Adversarial Networks - GANs) Ù„ØªØ·ÙˆÙŠØ± ØªØ­ÙˆÙŠÙ„ ÙƒÙ„ Ù…Ù† Ø§Ù„Ù†Ù…Ø· Ø§Ù„ÙÙ†ÙŠ (Style) ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰ ÙÙŠ Ø¹Ù…Ù„ØªÙŠ &lt;a href=&quot;https://paperswithcode.com/task/image-to-image-translation&quot;&gt;&lt;strong&gt;ØªØ±Ø¬Ù…Ø© ØµÙˆØ±Ø©-Ø¥Ù„Ù‰-ØµÙˆØ±Ø©&lt;/strong&gt;&lt;/a&gt; ØºÙŠØ± Ù…ØªØ±Ø§Ø¨Ø·ØªÙŠÙ†. Ø¨Ø§Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù‚ØªØ±Ø­ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„â€ØµÙˆØ±Ø©-Ø¥Ù„Ù‰-Ø±Ø³Ù…Ø©â€ (Ù…Ø¹ Ø´Ø¨ÙƒØ© ØªÙˆÙ„ÙŠØ¯ Ù…Ø­Ø³Ù†Ø©) ÙˆÙ‚ÙŠÙ…Ù‡Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ (Framework) Ø¬Ø¯ÙŠØ¯ Ù„Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙƒÙ…ÙŠ (Quantitative Evaluation) Ø§Ù„Ø°ÙŠ ÙŠØ£Ø®Ø° Ø¨Ø¹ÙŠÙ† Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø± ÙƒÙ„Ø§Ù‹ Ù…Ù† Ø§Ù„Ù†Ù…Ø· Ø§Ù„ÙÙ†ÙŠ ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰. Ù…ÙŠØ²Ø© Ø¹Ù…Ù„Ù‡ Ù‡ÙŠ Ø´Ø¨ÙƒØ© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø© Ø§Ù„ØªÙŠ ØªÙ…Ù„Ùƒ Ù…ÙˆØ§Ø²Ù†Ø© Ø¨ÙŠÙ† Ø§Ù„Ù†Ù…Ø· Ø§Ù„ÙÙ†ÙŠ ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰ ÙØ´Ù„Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ÙÙŠ ØªØ­Ù‚ÙŠÙ‚Ù‡Ø§. Ø§Ù„Ø´ÙØ±Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹ ØªÙ… Ù†Ø´Ø±Ù‡Ø§. Ø§Ù‚Ø±Ø£ ÙƒØ§Ù…Ù„ Ø§Ù„ÙˆØ±Ù‚Ø© (Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©) &lt;a href=&quot;https://arxiv.org/abs/2002.05638&quot;&gt;&lt;strong&gt;Ù‡Ù†Ø§&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ø§Ù„Ø´ÙƒÙ„ 1: Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ù…Ø®Ø±Ø¬Ø§Øª ÙƒÙ„ Ù…Ù† CycleGAN (3) ÙˆDualGAN (4) ÙˆØ·Ø±ÙŠÙ‚ØªÙ†Ø§ (GANILLA) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù†Ù…Ø§Ø· ÙÙ†ÙŠØ© Ù…Ø®ØªÙ„ÙØ©. CycleGAN Ùˆ GANILLAÙŠÙˆÙ„Ø¯Ø§Ù† ØµÙˆØ± Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø±Ø³Ù… Ø§Ù„ÙŠØ¯ÙˆÙŠØŒ Ù„ÙƒÙ† DualGAN ÙŠÙØ´Ù„ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…Ø· Ø§Ù„ÙÙ†ÙŠ. ÙˆØ¹Ù„Ù‰ Ø§Ù„ØµØ¹ÙŠØ¯ Ø§Ù„Ø£Ø®Ø± ÙØ´Ù„ CycleGAN ÙÙŠ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø¯Ø®Ù„Ø©ØŒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ ÙˆØ¶Ø¹ ÙˆØ¬ÙˆÙ‡ Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ù‡ÙˆØ§Ø¡ Ø¨Ø´ÙƒÙ„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ. Ø·Ø±ÙŠÙ‚ØªÙ†Ø§ ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØªØºÙŠÙŠØ± Ø§Ù„Ù†Ù…Ø· ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Andrew Ng ÙŠØªØ­Ø¯Ø« Ø¹Ù† Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø°Ø§ØªÙŠ Ø§Ù„Ø¥Ø´Ø±Ø§Ù (Self-Supervised Learning)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew NgØŒ Ù…Ø¤Ø³Ø³ deeplearning.aiØŒ Ø´Ø§Ø±Ùƒ ÙÙŠ Ø¨ÙˆØ¯ÙƒØ§Ø³Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ &lt;a href=&quot;https://www.youtube.com/watch?v=0jspaMLxBig&quot;&gt;&lt;strong&gt;Ù„ÙŠØªØ­Ø¯Ø«&lt;/strong&gt;&lt;/a&gt; Ø¹Ù† Ù…ÙˆØ§Ø¶ÙŠØ¹ Ù…Ù† Ø¶Ù…Ù†Ù‡Ø§ Ø¨Ø¯Ø§ÙŠØ§ØªÙ‡ Ù…Ø¹ ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©ØŒ Ù…Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØªØ¯Ø±ÙŠØ³ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµØ­ÙŠØ­ Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©ØŒ Ø£Ù‡Ø¯Ø§ÙÙ‡ Ø§Ù„Ø´Ø®ØµÙŠØ© ÙˆØªÙƒÙ†ÙŠÙƒÙŠØ§Øª ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙŠ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø­Ø§Ù„ÙŠ 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew ÙŠØ´Ø±Ø­ Ù„Ù…Ø§Ø°Ø§ Ù‡Ùˆ Ù…ØªØ­Ù…Ø³ Ø¬Ø¯Ø§Ù‹ Ø¨Ø´Ø£Ù† ØªØ¹Ù„Ù… Ø§Ù„Ø¥Ø´Ø±Ø§Ù Ø§Ù„Ø°Ø§ØªÙŠ Ø§Ù„Ù…Ù…Ø«Ù„ (Self-Supervised Representation Learning). &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html&quot;&gt;&lt;strong&gt;Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø°Ø§ØªÙŠ Ø§Ù„Ø¥Ø´Ø±Ø§Ù&lt;/strong&gt;&lt;/a&gt; ÙŠØªØ¶Ù…Ù† ØµÙŠØ§ØºØ© Ù…Ø´ÙƒÙ„Ø© ØªØ¹Ù„Ù… ØªÙ‡Ø¯Ù Ø¥Ù„Ù‰ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ù‚Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù†ÙØ³Ù‡Ø§ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø¯Ø± ÙƒØ¨ÙŠØ± Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØºÙŠØ± Ù…ØµÙ†ÙØ© (Unlabeled) ÙˆÙ‡ÙŠ Ø´Ø§Ø¦Ø¹Ø© Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØ§Ø¶Ø­Ø© Ø§Ù„Ù…ØµÙ†ÙØ© (Labeled). Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª (Representations) Ø°Ø§Øª Ø£Ù‡Ù…ÙŠØ© ÙÙŠ Ù…Ù‚Ø§Ø¨Ù„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù…Ø© ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„Ø­Ù„ Ù…Ù‡Ø§Ù… Ø¨Ø§ØªØ¬Ø§Ù‡ Ø§Ù„ØªÙŠØ§Ø± (Downstream) Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù…Ø§ ÙŠØ³ØªØ®Ø¯Ù… ÙÙŠ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ§Øª Ù…Ø«Ù„ &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert&quot;&gt;&lt;strong&gt;BERT&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ù‡Ù†Ø§Ùƒ Ø£ÙŠØ¶Ø§Ù‹ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø°Ø§ØªÙŠ Ø§Ù„Ø¥Ø´Ø±Ø§Ù Ù„ØªØ¹Ù„Ù… ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…Ø±Ø¦ÙŠØ© Ù…Ø¹Ù…Ù…Ø© Ø§Ù„ØªÙŠ Ø³ØªØ¬Ø¹Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© ÙÙŠ Ø§Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø© Ø§Ù„Ø¯Ù‚Ø©. Ù…Ø«Ù„Ø§Ù‹ØŒ Ø·Ø±ÙŠÙ‚Ø© Ø¬Ø¯ÙŠØ¯Ø© ØªØ³Ù…Ù‰ &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;&lt;strong&gt;SimCLR&lt;/strong&gt;&lt;/a&gt; (ÙƒØªØ¨Øª Ø¨Ø¥Ø´Ø±Ø§ÙGeoffrey Hinton ) ØªÙ‚Ø¯Ù… Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ (Framework) Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ Ø§Ù„Ø¥Ø´Ø±Ø§Ù Ø§Ù„Ù…Ø®ØªÙ„Ù (Contrastive Self-Supervised Learning) Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…Ø±Ø¦ÙŠØ© Ù„ØªØ·ÙˆÙŠØ± Ù†ØªØ§Ø¦Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ± ÙÙŠ Ø¸Ø±ÙˆÙ Ù…Ø®ØªÙ„ÙØ© Ù…Ø«Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø´Ø¨Ù‡ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨ (Semi-Supervised Learning).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ø§Ù„Ø´ÙƒÙ„ 1: Ù…Ù‚ÙŠØ§Ø³ Top-1 Ù…Ù† Ù…ÙƒØªØ¨Ø© ImageNet Ù„Ù„Ù…ØµÙ†ÙÙØ§Øª Ø§Ù„Ø®Ø·ÙŠØ© Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ø¹Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„Ø§Øª ØªÙ… ØªØ¹Ù„Ù…Ù‡Ø§ Ø¨Ø·Ø±Ø§Ø¦Ù‚ Ù…Ø®ØªÙ„ÙØ© Ø®Ø§Ø¶Ø¹Ø© Ù„Ù„Ø¥Ø´Ø±Ø§Ù Ø§Ù„Ø°Ø§ØªÙŠ (Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹ Ø¹Ù„Ù‰ ImageNet). ÙŠØ´ÙŠØ± Ø§Ù„ØµÙ„ÙŠØ¨ Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠ Ø¥Ù„Ù‰ ResNet-50 Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù. Ø·Ø±ÙŠÙ‚ØªÙ†Ø§ SimCLRØŒ Ù…ÙˆØ¶Ø­Ø© Ø¨Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±ÙŠØ¶.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ø¹Ù„Ù‰ Ù…Ø­ÙˆØ± X Ù…Ù‚ÙŠØ§Ø³ Top-1 Ø§Ù„Ø³Ø§Ø¨Ù‚ Ø°ÙƒØ±Ù‡ØŒ ÙˆØ¹Ù„Ù‰ Ù…Ø­ÙˆØ± Y Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨Ø§Ù„Ù…Ù„Ø§ÙŠÙŠÙ†.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;&lt;em&gt;Ø§Ù„Ù…ØµØ¯Ø±&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;Ø£Ø¯ÙˆØ§Øª-ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø§Øª-Ø¨ÙŠØ§Ù†Ø§Øª-ï¸&quot;&gt;Ø£Ø¯ÙˆØ§Øª ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Ù…ÙƒØªØ¨Ø§Øª JAX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/google/jax&quot;&gt;&lt;strong&gt;JAX&lt;/strong&gt;&lt;/a&gt; Ù‡ÙŠ Ù…ÙƒØªØ¨Ø© (Library) Ø¬Ø¯ÙŠØ¯Ø© ØªØ¯Ù…Ø¬ NumPy ÙˆØ§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚ Ø§Ù„Ø¢Ù„ÙŠ (Automatic Differentiation) Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø£Ø¨Ø­Ø§Ø« ØªØ¹Ù„Ù… Ø¢Ù„Ø© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¯Ø§Ø¡. Ù„ØªØ¨Ø³ÙŠØ· Ø®Ø·ÙˆØ· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ (Pipelines) Ù„Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ§Øª Ø¹ØµØ¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… JAXØŒ DeepMind Ø£ØµØ¯Ø±Øª &lt;a href=&quot;https://github.com/deepmind/dm-haiku&quot;&gt;&lt;strong&gt;Haiku&lt;/strong&gt;&lt;/a&gt; Ùˆ&lt;a href=&quot;https://github.com/deepmind/rlax&quot;&gt;&lt;strong&gt;RLax&lt;/strong&gt;&lt;/a&gt;.. RLax ÙŠØ¨Ø³Ø· ØªÙ†ÙÙŠØ° ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ù…Ø¹Ø²Ø² (Reinforcement Learning Agents) ÙˆHaiku ÙŠØ¨Ø³Ø· Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø±Ù…Ø¬Ø© ÙƒØ§Ø¦Ù†ÙŠØ© Ø§Ù„ØªÙˆØ¬Ù‡ (Object-Oriented) Ù…Ø£Ù„ÙˆÙØ©.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Ø£Ø¯Ø§Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/epfl-lts2/sparkwiki&quot;&gt;&lt;strong&gt;Sparkwiki&lt;/strong&gt;&lt;/a&gt; Ù‡ÙŠ Ø£Ø¯Ø§Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§. Ù‡Ø°Ø§ Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ù‡Ùˆ Ø¬Ø²Ø¡ Ù…Ù† Ø¬Ù‡ÙˆØ¯ ÙƒØ«ÙŠØ±Ø© Ù„ØªÙØ¹ÙŠÙ„ Ø¨Ø­ÙˆØ« ØªØ­Ù„ÙŠÙ„ÙŠØ© Ø³Ù„ÙˆÙƒÙŠØ© Ù…Ø«ÙŠØ±Ø© Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù…Ø«Ù„ &lt;a href=&quot;https://arxiv.org/abs/2002.06885&quot;&gt;&lt;strong&gt;Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø±Ø§Ø¦Ø¬Ø© ÙˆØ§Ù†Ø­ÙŠØ§Ø²ÙŠØ§Øª Ø§Ù„Ù„ØºØ§Øª Ù…Ù† Ø¥ØµØ¯Ø§Ø±Ø§Øª ÙˆÙŠÙƒÙŠØ¨Ø¯ÙŠØ§ Ø¨Ù„ØºØ§ØªÙ‡Ø§ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©&lt;/strong&gt;&lt;/a&gt;. Ø§Ù„Ù…Ø¤Ù„Ù Ø§ÙƒØªØ´Ù Ø§Ø³ØªÙ‚Ù„Ø§Ù„ Ø§Ù„Ù„ØºØ§ØªØŒ Ø³Ù„ÙˆÙƒ Ù…Ø³ØªØ®Ø¯Ù…ÙŠ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ ÙÙŠ Ø§Ù„ØªØµÙØ­ ÙŠØ¸Ù‡Ø± Ø£Ù†Ù‡Ù… ØºØ§Ù„Ø¨Ø§Ù‹ Ù…Ø§ ÙŠØ´Ø§Ø±ÙƒÙˆÙ† Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª Ù…Ø´ØªØ±ÙƒØ© Ù„Ù„ØªØµÙ†ÙŠÙØ§Øª Ù…Ø«Ù„ Ø§Ù„Ø£ÙÙ„Ø§Ù…ØŒ Ø§Ù„Ø£ØºØ§Ù†ÙŠØŒ ÙˆØ§Ù„Ø±ÙŠØ§Ø¶Ø§ØªØŒ Ù„ÙƒÙ† Ø§Ù„ÙØ±ÙˆÙ‚ ØªØªØ¶Ø­ ÙÙŠ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ù…Ø­Ù„ÙŠØ© ÙˆØ§Ù„Ø®ØµÙˆØµÙŠØ§Øª Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ©.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Ù…Ø±Ù…Ø²Ø§Øª Rust (Rust Tokenizers)ØŒ Ù‚Ø§Ø¹Ø¯Ø© DistilBERT Ø§Ù„Ù…ØºÙ„ÙØ© (DistilBERT Base Cased)ØŒ Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (Model Cards)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/huggingface/transformers/releases/tag/v2.5.0&quot;&gt;&lt;strong&gt;Ø¥Ø·Ù„Ø§Ù‚ Ù…Ø­ÙˆÙ„Ø§Øª&lt;/strong&gt;&lt;/a&gt; (Transformers) Ø¬Ø¯ÙŠØ¯ Ù…Ù† Ù‚Ø¨Ù„ Hugging Face ÙŠØªØ¶Ù…Ù† Ø¯Ù…Ø¬ Ù…ÙƒØªØ¨Ø© Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø³Ø±ÙŠØ¹Ø© Ø®Ø§ØµØªÙ‡Ù… Ù…Ù…Ø§ ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ØªØ³Ø±ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø«Ù„ BERTØŒ RoBERTaØŒ GPT2ØŒ ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© Ø§Ù„ØµÙ†Ø¹.&lt;/p&gt;

&lt;h1 id=&quot;Ø§Ù„Ø£Ø®Ù„Ø§Ù‚-ÙÙŠ-Ø§Ù„Ø°ÙƒØ§Ø¡-Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ-&quot;&gt;Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ğŸš¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ÙŠØ© Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© (NLP) ÙˆÙ†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© (ML)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
ÙÙŠ &lt;a href=&quot;https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender&quot;&gt;&lt;strong&gt;Ø­Ù„Ù‚Ø©&lt;/strong&gt;&lt;/a&gt; Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ø¨ÙˆØ¯ÙƒØ§Ø³Øª &lt;a href=&quot;https://soundcloud.com/nlp-highlights&quot;&gt;&lt;strong&gt;NLP Highlights&lt;/strong&gt;&lt;/a&gt;ØŒ ØªØ­Ø¯Ø«Øª Emily Bende ÙˆØ§Ù„Ù…Ù‚Ø¯Ù…ÙˆÙ† Ø­ÙˆÙ„ Ø¨Ø¹Ø¶ Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ÙŠØ© Ø¹Ù†Ø¯ ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ§Øª ÙÙŠ Ø³ÙŠØ§Ù‚ ÙƒÙ„ Ù…Ù† Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ§ ÙˆØ§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙØ¹Ù„ÙŠ Ø¹Ù„Ù‰ Ø£Ø±Ø¶ Ø§Ù„ÙˆØ§Ù‚Ø¹. ÙƒØ§Ù† Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ ÙÙŠ Ø§Ù„Ù†Ù‚Ø§Ø´: Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ÙŠØ© Ø¹Ù†Ø¯ ØªØµÙ…ÙŠÙ… Ù…Ù‡Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ Ø£Ø³Ø§Ù„ÙŠØ¨ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ†Ø´Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ù„ÙƒÙ„ Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„Ø£Ø¹Ù„Ù‰ØŒ Ù‚Ù„Ù‚ Ø¯Ø§Ø¦Ù… Ø§Ù„Ù†Ù‚Ø§Ø´ ÙÙŠ Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø§Ù„Ù…ÙØ±Ø· Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ù…Ù‚ÙŠØ§Ø³ØŒ ÙˆÙ‡Ø°Ø§ ÙŠØªØ¹Ø§Ø±Ø¶ Ù…Ø¹ Ø£Ø³Ø§Ø³ Ù…Ø§ ÙŠØ­Ø§ÙˆÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØªØ­Ù‚ÙŠÙ‚Ù‡. Rachel Thomas Ùˆ David Uminsky Ù†Ø§Ù‚Ø´Ø§ Ù…ØªÙ‰ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØµØ¨Ø­ Ù‡Ø°Ø§ Ø®Ø§Ø·Ø¦Ø§Ù‹ Ø¹Ø¨Ø± &lt;a href=&quot;https://arxiv.org/abs/2002.08512&quot;&gt;&lt;strong&gt;ØªØ­Ù„ÙŠÙ„ Ø¯Ù‚ÙŠÙ‚&lt;/strong&gt;&lt;/a&gt; Ù„Ø­Ø§Ù„Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø®ØªÙ„ÙØ©. Ø§Ù‚ØªØ±Ø­Ø§ Ø£ÙŠØ¶Ø§Ù‹ Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ (Framework) Ø¨Ø³ÙŠØ·Ø§Ù‹ Ù„ØªØ®ÙÙŠÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆÙŠØªØ¶Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¯Ù…Ø¬ Ø¹Ø¯Ø© Ù…Ù‚Ø§ÙŠÙŠØ³ØŒ Ù…ØªØ¨ÙˆØ¹Ø§Ù‹ Ø¨Ø¥Ø´Ø±Ø§Ùƒ Ø§Ù„Ù…ØªØ¶Ø±Ø±ÙŠÙ† Ù…Ù† Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø±.&lt;/p&gt;

&lt;h1 id=&quot;Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª-ÙˆØ§Ù„ØªØ¯ÙˆÙŠÙ†Ø§Øª-ï¸&quot;&gt;Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª ÙˆØ§Ù„ØªØ¯ÙˆÙŠÙ†Ø§Øª âœï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;GPT-2 Ø§Ù„Ù…Ø´Ø±ÙˆØ­&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aman Arora Ù†Ø´Ø± Ù…Ø¤Ø®Ø±Ø§Ù‹ ÙÙŠ ØªØ¯ÙˆÙŠÙ†Ø© Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠØ© Ø¨Ø¹Ù†ÙˆØ§Ù† â€œ&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;&lt;strong&gt;The Annotated GPT-2&lt;/strong&gt;&lt;/a&gt;â€ (Ø¨Ù…Ø¹Ù†Ù‰ GPT-2 Ø§Ù„Ù…Ø´Ø±ÙˆØ­) ÙŠØ´Ø±Ø­ ÙÙŠÙ‡Ø§ ØªÙØ§ØµÙŠÙ„ Ø¹Ù…Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø³Ù…Ù‰ GPT-2 Ø§Ù„Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ù…Ø­ÙˆÙ„ (Transformer). Ø£Ø³Ù„ÙˆØ¨Ù‡ Ù…Ø³ØªÙˆØ­Ù‰ Ù…Ù† &lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;The Annotated Transformer&lt;/a&gt; (Ø¨Ù…Ø¹Ù†Ù‰ Ø§Ù„Ù…Ø­ÙˆÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ­) Ø§Ù„Ø°ÙŠ Ø£Ø®Ø° Ù…Ø³Ø¹Ù‰ Ø§Ù„Ø­Ø§Ø´ÙŠØ© ÙˆØ§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù„Ø´Ø±Ø­ Ø£Ø¬Ø²Ø§Ø¡ Ù…Ù‡Ù…Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ø¨Ø± Ø§Ù„Ø´ÙØ±Ø© (â€œØ§Ù„ÙƒÙˆØ¯â€) ÙˆØ´Ø±ÙˆØ­Ø§Øª Ø³Ù‡Ù„Ø© Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©. Ø¨Ø°Ù„ Aman Ø¬Ù‡Ø¯Ø§Ù‹ Ø¹Ø¸ÙŠÙ…Ø§Ù‹ Ù„ÙŠØ¹ÙŠØ¯ ØªÙ†ÙÙŠØ° GPT-2 Ø§Ù„Ø®Ø§Øµ Ø¨ OpenAIØ¨Ø§Ø³ØªØ®Ø¯Ø§Ù… PyTorch ÙˆÙ…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª (Transformers Library) Ø§Ù„Ø®Ø§ØµØ© Ø¨ Hugging Face. Ø¥Ù†Ù‡ Ø¹Ù…Ù„ Ø¹Ø¨Ù‚Ø±ÙŠ!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ù…Ø­ÙˆÙ„ ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± Ø¯Ø§Ø®Ù„ GPT-2&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù„ÙˆØµÙ Ø§Ù„Ù…Ø­ÙˆÙ„ØŒ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ (Attention) Ù‡Ùˆ Ø¯Ø§Ù„Ø© Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… (Q) ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ù…ÙØªØ§Ø­ (K) ÙˆØ£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù‚ÙŠÙ…Ø© (V). Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…ØªØªØ§Ø¨Ø¹Ø§Øª Ø§Ù„Ø£Ø·ÙˆÙ„ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø°Ø§ØªÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³ Ù„Ù„Ù…Ø­ÙˆÙ„ Ù„ÙŠÙ‚Ù„Ù„ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø­Ø¯ Ù…Ù† Ø§Ù„Ù…Ù†ØªØ¬ Ø§Ù„Ù†Ù‚Ø·ÙŠ Ø¨ÙŠÙ† Q ÙˆK ÙÙŠ Ù…Ø¹Ø§Ø¯Ù„Ø© â€œØ§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¨Ù…Ø¹Ù„ÙˆÙ…ÙŠØ© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆØ§Ù„Ù…ÙØªØ§Ø­ ÙˆØ§Ù„Ù‚ÙŠÙ…Ø©â€ Ø§Ù„Ø¸Ø§Ù‡Ø±Ø© ÙÙŠ Ø§Ù„Ø±Ø³Ù….&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;&lt;em&gt;Ø§Ù„Ù…ØµØ¯Ø±&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Ù…Ø§ ÙˆØ±Ø§Ø¡ BERTØŸ&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1&quot;&gt;&lt;strong&gt;Ù…Ù‚Ø§Ù„&lt;/strong&gt;&lt;/a&gt; Ù…Ø«ÙŠØ± Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù…Ù† ÙƒØªØ§Ø¨Ø© Sergi Castella ÙŠØªØ³Ø§Ø¡Ù„ ÙÙŠÙ‡ Ø¹Ù† Ø£Ø³Ø±Ø§Ø± BERT. ÙƒØ§Ù† Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ØŒ ÙƒÙŠÙ Ù…ÙƒÙ†Øª Ù…ÙƒØªØ¨Ø© Ù…Ø­ÙˆÙ„Ø§Øª Hugging Face Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ù„Ù…ÙŠØŒ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø«ÙŠØ±Ø© ØªØ³ØªØ­Ù‚ Ø§Ù„Ù†Ø¸Ø±ØŒ ØªØ´Ø±ÙŠØ­ Ù†Ù…Ø§Ø°Ø¬ØŒ ÙˆØ§Ù„Ù…Ø²ÙŠØ¯.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Ø¹Ù…Ù„ÙŠØ© Ø¶ØºØ· Ø§Ù„Ù…ØµÙÙˆÙØ§Øª (Matrix Compression Operator)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ù…Ø¯ÙˆÙ†Ø© TensorFlow Ù†Ø´Ø±Øª &lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;&lt;strong&gt;ØªØ¯ÙˆÙŠÙ†Ø©&lt;/strong&gt;&lt;/a&gt; ØªØ´Ø±Ø­ ØªÙƒÙ†ÙŠÙƒØ§Øª ÙˆØ§Ù„Ø£Ù‡Ù…ÙŠØ© ÙˆØ±Ø§Ø¡ Ø¶ØºØ· Ø§Ù„Ù…ØµÙÙˆÙØ§Øª ÙÙŠ Ù†Ù…ÙˆØ°Ø¬ Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© Ø¹Ù…ÙŠÙ‚Ø©. Ø¶ØºØ· Ø§Ù„Ù…ØµÙÙˆÙØ§Øª ÙŠÙ…ÙƒÙ†Ù‡ Ø£Ù† ÙŠØ³Ø§Ø¹Ø¯ Ù„Ø¨Ù†Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ ØµØºÙŠØ±Ø© Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ¯Ù…Ø¬ ÙÙŠ Ø£Ø¬Ù‡Ø²Ø© ØµØºÙŠØ±Ø© Ù…Ø«Ù„ Ø§Ù„Ù‡ÙˆØ§ØªÙ ÙˆØ§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø§Øª Ø§Ù„Ù…Ù†Ø²Ù„ÙŠØ© (Home Assistants). Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø¶ØºØ· Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ø¨Ø± Ø·Ø±Ù‚ Ù…Ø«Ù„ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ Ø§Ù„Ù…Ù†Ø®ÙØ¶ Ø§Ù„Ø±ØªØ¨Ø© (Low-Rank Approximation) ÙˆØ§Ù„ØªÙƒÙ…ÙŠÙ… (Quantization) ÙŠØ¹Ù†Ù‰ Ø£Ù†Ù†Ø§ Ù„Ø§ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø£Ù† Ù†Ø¶Ø± Ø¨Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ø§Ù„Ø¶ØºØ· ÙƒØªØ­ÙˆÙ„: Ø±Ø¨Ø· Ø·Ø¨Ù‚Ø© Ø¥Ù„Ù‰ Ø·Ø¨Ù‚ØªÙŠÙ†.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ø¹Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø± Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø£ØµÙ„ÙŠØ©ØŒ ÙˆØ¹Ù„Ù‰ Ø§Ù„ÙŠÙ…ÙŠÙ† Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ù…ØªØ­ÙˆÙ„Ø©.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;&lt;em&gt;Ø§Ù„Ù…ØµØ¯Ø±&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;Ø§Ù„ØªØ¹Ù„ÙŠÙ…-&quot;&gt;Ø§Ù„ØªØ¹Ù„ÙŠÙ… ğŸ“&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Ø£Ø³Ø§Ø³ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© (NLP)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ø£Ù†Ø§ Ù…ØªØ­Ù…Ø³ Ù„Ù†Ø´Ø± Ù…Ø³ÙˆØ¯Ø© Ù…Ù† Ø§Ù„ÙØµÙ„ Ø§Ù„Ø£ÙˆÙ„ Ù…Ù† Ø³Ù„Ø³Ù„ØªÙŠ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„Ù…Ø³Ù…Ø§Ø© &lt;a href=&quot;https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684&quot;&gt;&lt;strong&gt;Fundamentals of NLP&lt;/strong&gt;&lt;/a&gt;(Ø£Ø³Ø§Ø³ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©). Ø¥Ù†Ù‡Ø§ ØªØ¹Ù„Ù… Ù…ÙØ§Ù‡ÙŠÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø¯Ø¦Ø§Ù‹ Ù…Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ§Øª ÙÙŠ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø§ ØªØ´Ø§Ø±Ùƒ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§ØªØŒ Ù…Ø±Ø§Ø¬Ø¹ Ù…Ù‡Ù…Ø©ØŒ Ø£Ø®Ø·Ø§Ø¡ Ø´Ø§Ø¦Ø¹Ø© Ù„ØªØ¬Ù†Ø¨Ù‡Ø§ØŒ ÙˆÙ…Ø§ ÙŠÙ†ØªØ¸Ø±Ù†Ø§ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©. &lt;a href=&quot;https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ&quot;&gt;&lt;strong&gt;Ø¯ÙØªØ±&lt;/strong&gt;&lt;/a&gt; ÙƒÙˆÙ„Ø§Ø¨ (Google Colab) Ù…Ø±ÙÙ‚ ÙˆØ§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø³ÙŠØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡ &lt;a href=&quot;https://github.com/dair-ai/nlp_fundamentals&quot;&gt;&lt;strong&gt;Ù‡Ù†Ø§&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;[Ø¹Ø¨Ø± Ø§Ù„Ø§Ù†ØªØ±Ù†Øª] Ù…Ø±Ø§Ø¬Ø¹Ø©/Ù…Ù†Ø§Ù‚Ø´Ø©: Ø§Ù„Ø¬Ø²Ø¡ 1 Ø¬Ù„Ø³Ø© Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø£Ø³Ø³ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://mml-book.github.io/&quot;&gt;&lt;strong&gt;Mathematics For Machine Learning&lt;/strong&gt;&lt;/a&gt; ØªØ³ØªØ¶ÙŠÙ Ù†Ù‚Ø§Ø´ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ÙØµÙˆÙ„ Ø§Ù„ØªÙŠ ØªÙ…Øª ØªØºØ·ÙŠØªÙ‡Ø§ Ù…Ø¤Ø®Ø±Ø§Ù‹ ÙÙŠ Ø¬Ù„Ø³Ø§ØªÙ‡Ù… Ù„Ù„Ø¯Ø±Ø§Ø³Ø© Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª. Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¯Ø±Ø³Øª Ø³Ø§Ø¨Ù‚Ø§Ù‹ ÙØµÙˆÙ„Ø§Ù‹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙƒØªØ§Ø¨ ÙŠØ¯Ø¹Ù‰ Mathematics For Machine Learning ÙƒØªØ¨Ù‡ Marc Peter Deisenroth ÙˆA Aldo Faisal ÙˆCheng Soon Ong. &lt;a href=&quot;https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/&quot;&gt;&lt;em&gt;Ø§Ù„Ø­Ø¯Ø«&lt;/em&gt;&lt;/a&gt; Ù…Ø®Ø·Ø· Ù„ÙŠÙ‚Ø§Ù… ÙÙŠ Ø§Ù„Ø«Ø§Ù…Ù† Ù…Ù† Ù…Ø§Ø±Ø³ 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„ÙƒØªØ¨&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
ÙÙŠ Ù‚Ø·Ø¹Ø© Ø³Ø§Ø¨Ù‚Ø© Ù†Ø§Ù‚Ø´Ù†Ø§ Ø£Ù‡Ù…ÙŠØ© Ø¶ØºØ· Ø§Ù„Ù…ØµÙÙˆÙØ§Øª Ù„Ø¨Ù†Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„Ù… Ø¢Ù„Ø© ØµØºÙŠØ±Ø©. Ø¥Ø°Ø§ ÙƒÙ†Øª Ù…Ù‡ØªÙ…Ø§Ù‹ Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø²ÙŠØ¯ Ø¹Ù† ÙƒÙŠÙÙŠØ© Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ§Øª Ø¹ØµØ¨ÙŠØ© Ø¹Ù…ÙŠÙ‚Ø© Ø£ØµØºØ± Ù„Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…ØªØ¶Ù…Ù†Ø© (Embedded Systems) Ø®Ø° Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø±Ø§Ø¦Ø¹ Ø¨Ø§Ø³Ù… &lt;a href=&quot;https://tinymlbook.com/?linkId=82595412&quot;&gt;&lt;strong&gt;TinyML&lt;/strong&gt;&lt;/a&gt; (Ø¨Ù…Ø¹Ù†Ù‰: ØªØ¹Ù„Ù… Ø¢Ù„Ø© ØµØºÙŠØ±) Ù…Ù† ØªØ£Ù„ÙŠÙ Pete Warden Ùˆ Daniel Situnayake.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043&quot;&gt;&lt;em&gt;Ø§Ù„Ù…ØµØ¯Ø±&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
ÙƒØªØ§Ø¨ Ø¢Ø®Ø± Ù…Ø«ÙŠØ± Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù„Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ù‡Ùˆ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù‚Ø§Ø¯Ù… â€œ&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;&lt;strong&gt;Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD&lt;/strong&gt;&lt;/a&gt;â€ (Ø¨Ù…Ø¹Ù†Ù‰: Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù…Ø¨Ø±Ù…Ø¬ÙŠÙ† Ø¨ fastai Ùˆ PyTorch: ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø¯ÙˆÙ† Ø´Ù‡Ø§Ø¯Ø© Ø¯ÙƒØªÙˆØ±Ø§Ø©) Ù…Ù† ØªØ£Ù„ÙŠÙ Jeremy Howard Ùˆ Sylvain Gugger. Ø§Ù„ÙƒØªØ§Ø¨ ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠ Ù„Ø¨Ù†Ø§Ø¡ ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ù‡Ø§Ù… ÙÙŠ Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© (Computer Vision) ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© (NLP).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;&lt;em&gt;Ø§Ù„Ù…ØµØ¯Ø±&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;Ø¥Ø¶Ø§ÙØ§Øª-Ø¬Ø¯ÙŠØ±Ø©-Ø¨Ø§Ù„Ø°ÙƒØ±-ï¸&quot;&gt;Ø¥Ø¶Ø§ÙØ§Øª Ø¬Ø¯ÙŠØ±Ø© Ø¨Ø§Ù„Ø°ÙƒØ± â­ï¸&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;
ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø³Ø§Ø¨Ù‚ Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© (NLP) Ø§Ù„Ø¨Ø±ÙŠØ¯ÙŠØ© Ù…Ù† &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-pytorch3d-deepspeed-turing-nlg-question-answering-benchmarks-hydra-sparse-322f018ee096&quot;&gt;&lt;strong&gt;Ù‡Ù†Ø§&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1909.06576&quot;&gt;&lt;strong&gt;Torchmeta&lt;/strong&gt;&lt;/a&gt; Ù‡ÙŠ Ù…ÙƒØªØ¨Ø© (Library) ØªØªÙŠØ­ Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ù…Ø­Ù…Ù„Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Data Loaders) Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ù„Ø£Ø¨Ø­Ø§Ø« Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ÙÙˆÙ‚ÙŠ (Meta-Learning). ÙƒØªØ¨Ù‡Ø§ Tristan Deleu.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manuel Tonneau ÙƒØªØ¨Øª &lt;a href=&quot;https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter&quot;&gt;&lt;strong&gt;Ù…Ù‚Ø§Ù„Ø§Ù‹&lt;/strong&gt;&lt;/a&gt; ÙŠÙ‚Ø¯Ù… Ù†Ø¸Ø±Ø© Ø£Ù‚Ø±Ø¨ Ù„Ø¨Ø¹Ø¶ Ø§Ù„Ø¢Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© (Language Modeling). Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ ØªØ¶Ù…Ù†Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¬Ø´Ø¹ (Greedy Search) ÙˆØ§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¹Ø§Ø¹ÙŠ (Beam Search) ÙˆØ£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ù†ÙˆÙˆÙŠ (Nucleus Sampling).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
MIT &lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;&lt;strong&gt;Ù†Ø´Ø±Øª&lt;/strong&gt;&lt;/a&gt; Ø§Ù„Ù…Ù†Ù‡Ø¬ ÙˆØ§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„ÙŠÙ† Ù„ÙƒÙˆØ±Ø³ â€œIntroduction to Deep Learningâ€ (Ù…Ù‚Ø¯Ù…Ø© Ø¥Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚)ØŒ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù„Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ø§Ù„ØªÙŠ Ø£Ù„Ù‚ÙŠØª Ø¨Ø§Ù„ÙØ¹Ù„. ÙŠÙ‡Ø¯ÙÙˆÙ† Ø¥Ù„Ù‰ Ù†Ø´Ø± Ù…Ù‚Ø§Ø·Ø¹ ÙÙŠØ¯ÙŠÙˆ ÙˆØ´Ø±Ø§Ø¦Ø­ Ø§Ù„Ø¹Ø±Ø¶ Ù„Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ø£Ø³Ø¨ÙˆØ¹ÙŠØ§Ù‹.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
ØªØ¹Ù„Ù… ÙƒÙŠÙ ØªØ¯Ø±Ø¨ Ù†Ù…ÙˆØ°Ø¬Ø§Ù‹ Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³Ù…Ø§Ø© (Named Entity Recognition (NER)) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ Ù…Ø¨Ù†Ù‰ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ÙˆÙ„ (Transformer) ÙÙŠ Ø£Ù‚Ù„ Ù…Ù† &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py&quot;&gt;&lt;strong&gt;300 Ø³Ø·Ø± Ù…Ù† Ø§Ù„Ø´ÙØ±Ø© â€œØ§Ù„ÙƒÙˆØ¯â€&lt;/strong&gt;&lt;/a&gt;. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¬ÙˆØ¬Ù„ ÙƒÙˆÙ„Ø§Ø¨ Ø§Ù„Ù…ØµØ§Ø­Ø¨ &lt;a href=&quot;https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn&quot;&gt;&lt;strong&gt;Ù‡Ù†Ø§&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;
Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ Ø£ÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ù…Ø´Ø§Ø±ÙŠØ¹ØŒ ØªØ¯ÙˆÙŠÙ†Ø§ØªØŒ Ø´Ø±ÙˆØ­Ø§ØªØŒ Ø£Ùˆ Ø£ÙˆØ±Ø§Ù‚ Ø¨Ø­Ø«ÙŠØ© ØªØªÙ…Ù†Ù‰ Ø£Ù† ÙŠØªÙ… Ù†Ø´Ø±Ù‡Ù… ÙÙŠ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ø§Ø¯Ù… Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© (NLP) Ø§Ù„Ø¨Ø±ÙŠØ¯ÙŠØ©ØŒ Ù„Ø§ ØªØªØ±Ø¯ ÙÙŠ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ÙŠ Ø¹Ø¨Ø± Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ellfae@gmail.com Ø£Ùˆ&lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;&lt;strong&gt;Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø®Ø§ØµØ© Ø¹Ù„Ù‰ ØªÙˆÙŠØªØ±&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;strong&gt;Ø§Ø´ØªØ±Ùƒ&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;ğŸ”– ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© NLP Ø§Ù„Ø¨Ø±ÙŠØ¯ÙŠØ© Ù„ØªØµÙ„Ùƒ Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù‚Ø§Ø¯Ù…Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ø¹Ù„Ù‰ ØµÙ†Ø¯ÙˆÙ‚ Ø¨Ø±ÙŠØ¯Ùƒ.&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter.AR._The_Annotated_GPT-2_And_More/&quot;&gt;Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø®Ø§Ù…Ø³ Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© (NLP) Ø§Ù„Ø¨Ø±ÙŠØ¯ÙŠØ©: GPT-2 Ø§Ù„Ù…Ø´Ø±ÙˆØ­ØŒ ÙÙ‡Ù… Ø§Ù„ØªÙ‚Ø·ÙŠØ±-Ø§Ù„Ø°Ø§ØªÙŠØŒ HaikuØŒ GANILLAØŒ SparkwikiØŒ Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ ÙÙŠ Ø¹Ù„Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ TorchmetaØŒ ÙˆØ§Ù„Ù…Ø²ÙŠØ¯ ...&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 28, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,â€¦]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_The_Annotated_GPT-2,_Understanding/" />
  <id>https://dair.ai/NLP_Newsletter_The_Annotated_GPT-2,_Understanding</id>
  <published>2020-02-23T00:00:00-06:00</published>
  <updated>2020-02-23T00:00:00-06:00</updated>
  <author>
    <name>Elvis Saravia</name>
    <uri>https://dair.ai</uri>
    <email>ellfae@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
First of all, I canâ€™t thank â¤ï¸ all of you enough for the incredible support and encouragement to continue with the NLP Newsletter. This effort requires tedious research and editing which I find to be both rewarding and also useful to provide you the best content. Hope you are enjoying them because I do. ğŸ˜‰&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Subscribe&lt;/em&gt;&lt;/a&gt; &lt;em&gt;ğŸ”– to the NLP Newsletter to receive future issues in your inbox.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications ğŸ“™&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A theoretical understanding of self-distillation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In the context of deep learning, &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;&gt;&lt;em&gt;self-distillation&lt;/em&gt;&lt;/a&gt; is the process of transferring knowledge from one architecture to another identical architecture. Predictions of the original model are fed as target values to the other model while training. Besides having desirable properties, such as reducing model size, empirical results demonstrate that this approach works well on held-out datasets. A group of researchers recently published a paper that provides a theoretical analysis with the focus to better understand what is happening in this knowledge distillation process and why it is effective. Results show that a few rounds of self-distillation amplify regularization (by &lt;a href=&quot;https://twitter.com/TheGradient/status/1228132843630387201?s=20&quot;&gt;&lt;em&gt;progressively limiting the number of basis functions that represent the solution&lt;/em&gt;&lt;/a&gt;) which tends to reduce over-fitting. (Read the full paper &lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;The 2010s: Our Decade of Deep Learning / Outlook on the 2020s&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;http://people.idsia.ch/~juergen/&quot;&gt;JÃ¼rgen Schmidhuber,&lt;/a&gt; a pioneer in Artificial Intelligence, recently posted a &lt;a href=&quot;http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html&quot;&gt;&lt;strong&gt;new blog post&lt;/strong&gt;&lt;/a&gt; focusing on providing a historical overview of deep learning since 2010. Some topics include LSTMs, feedforward neural networks, GANs, deep reinforcement learning, meta-learning, world models, distilling NNs, attention learning, etc. The article concludes with an outlook on the 2020s encouraging attention to pressing issues such as privacy and data markets.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Using neural networks to solve advanced mathematics equations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Facebook AI researchers published a &lt;a href=&quot;https://arxiv.org/abs/1912.01412&quot;&gt;&lt;strong&gt;paper&lt;/strong&gt;&lt;/a&gt; that claims to propose a model trained on math problems and matching solutions to learn to predict possible solutions for tasks such as solving integration problems. The approach is based on a novel framework similar to whatâ€™s used in &lt;em&gt;neural machine translation&lt;/em&gt; where mathematical expressions are represented as a kind of language and the solutions being treated as the translation problem. Thus, instead of the model outputting a translation, the output is the solution itself. With this, the researchers claim that deep neural networks are not only good at symbolic reasoning but also for more diverse tasks.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Equations fed as input together with the corresponding solution outputted by the modelâ€Šâ€”&lt;/em&gt;â€Š&lt;a href=&quot;https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;creativity-and-society-&quot;&gt;Creativity and Society ğŸ¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;AI for scientific discovery&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Mattew Hutson &lt;a href=&quot;https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times&quot;&gt;&lt;strong&gt;reports&lt;/strong&gt;&lt;/a&gt; how artificial intelligence (AI) can be used to produce emulators that have an important use in modeling complex natural phenomena that could, in turn, lead to different types of &lt;em&gt;scientific discovery&lt;/em&gt;. The change with building these emulators is that they often require large-scale data and extensive parameter exploration. A recent &lt;a href=&quot;https://arxiv.org/abs/2001.08055&quot;&gt;&lt;strong&gt;paper&lt;/strong&gt;&lt;/a&gt; proposes DENSE an approach based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_architecture_search&quot;&gt;&lt;em&gt;neural architecture search&lt;/em&gt;&lt;/a&gt; to build accurate emulators while only relying on a limited amount of training data. They tested it by running simulations for cases including astrophysics, climate science, and fusion energy, among others.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Improving image-to-illustration translation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
GANILLA is an approach that proposes the use of GANs to improve the transfer of both style and content in the unpaired &lt;a href=&quot;https://paperswithcode.com/task/image-to-image-translation&quot;&gt;&lt;em&gt;image-to-image translation&lt;/em&gt;&lt;/a&gt; task. In particular, a model for image-to-illustration is proposed (with an improved generator network) and evaluated based on a new framework for quantitative evaluation that considers both content and style. The novelty of the work is in the proposed generator network that considers a balance between style and content which previous models fail to achieve. Code and pretrained models are made &lt;a href=&quot;https://github.com/giddyyupp/ganilla&quot;&gt;available&lt;/a&gt;. Read the full paper &lt;a href=&quot;https://arxiv.org/abs/2002.05638&quot;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Andrew Ng talks about interest in self-supervised learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew Ng, the founder of deeplearning.ai, joins the Artificial Intelligence podcast to &lt;a href=&quot;https://www.youtube.com/watch?v=0jspaMLxBig&quot;&gt;&lt;strong&gt;talk&lt;/strong&gt;&lt;/a&gt; about topics including his early days doing ML, the future of AI and AI education, recommendations for proper use of ML, his personal goals and what ML techniques to pay attention to in the 2020s.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew explained why he is very excited about &lt;em&gt;self-supervised representation learning.&lt;/em&gt; &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html&quot;&gt;&lt;strong&gt;Self-supervised learning&lt;/strong&gt;&lt;/a&gt; involves framing a learning problem that aims to obtain supervision from the data itself to make use of large amounts of unlabeled data which is more common than clean labeled data. The representations, as opposed to the performance of the task, are important and can be used to address downstream tasks similar to whatâ€™s being used in language models such as &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert&quot;&gt;BERT&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
There is also a lot of interest to use self-supervised learning for learning generalized visual representations that make models more accurate in low-resource settings. For instance, a recent method called &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;&lt;strong&gt;SimCLR&lt;/strong&gt;&lt;/a&gt; (led by Geoffrey Hinton) proposes a framework for &lt;em&gt;contrastive self-supervised learning&lt;/em&gt; of visual representations for improving image classification results in different settings such as transfer learning and semi-supervised learning.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;tools-and-datasets-ï¸&quot;&gt;Tools and Datasets âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;JAX libraries&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/google/jax&quot;&gt;JAX&lt;/a&gt; is a new library that combines NumPy and automatic differentiation for conducting high-performance ML research. To simplify pipelines for building neural networks using JAX, DeepMind released &lt;a href=&quot;https://github.com/deepmind/dm-haiku&quot;&gt;&lt;strong&gt;Haiku&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/deepmind/rlax&quot;&gt;&lt;strong&gt;RLax&lt;/strong&gt;&lt;/a&gt;. RLax simplifies the implementation of reinforcement learning agents and Haiku simplifies the building of neural networks using &lt;em&gt;familiar object-oriented programming models.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;A tool for processing Wikipedia data&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/epfl-lts2/sparkwiki&quot;&gt;&lt;strong&gt;Sparkwiki&lt;/strong&gt;&lt;/a&gt; is a tool to process Wikipedia data. This release is part of many efforts to enable interesting behavioral analytics research such as &lt;a href=&quot;https://arxiv.org/abs/2002.06885&quot;&gt;capturing trends and language biases across different language editions of Wikipedia&lt;/a&gt;. The authors discovered that independent of language, the browsing behavior of Wikipedia users shows that they tend to share common interests for categories such as movies, music, and sports but differences become more apparent with local events and cultural particularities.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Rust Tokenizers, DistilBERT base cased, Model cards&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A new &lt;a href=&quot;https://github.com/huggingface/transformers/releases/tag/v2.5.0&quot;&gt;&lt;strong&gt;Transformers release&lt;/strong&gt;&lt;/a&gt; by Hugging Face includes the integration of their fast tokenizer library which aims to speed up models like BERT, RoBERTa, GPT2, and other community-built models.&lt;/p&gt;

&lt;h1 id=&quot;ethics-in-ai-&quot;&gt;Ethics in AI ğŸš¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Ethical considerations for NLP and ML models&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In a new &lt;a href=&quot;https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender&quot;&gt;&lt;strong&gt;episode&lt;/strong&gt;&lt;/a&gt; of the &lt;a href=&quot;https://soundcloud.com/nlp-highlights&quot;&gt;NLP Highlights,&lt;/a&gt; Emily Bender and hosts talk about some ethical considerations when developing NLP models and technologies in the context of both academia and real-world usage. Some of the topics in the discussion include ethical considerations when designing NLP tasks, data collection approaches, and eventually publishing results.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In addition to all the considerations above, a concern that is always discussed in the AI community is focusing too much on optimizing a metric, which goes against the foundations of what AI aims to achieve. Rachel Thomas and David Uminsky discuss where this can go wrong through a &lt;a href=&quot;https://arxiv.org/abs/2002.08512&quot;&gt;&lt;strong&gt;thorough analysis&lt;/strong&gt;&lt;/a&gt; of different use cases. They also propose a simple framework for mitigating the problem which involves the use and combination of multiple metrics, followed by the involvement of those affected directly by the technology.&lt;/p&gt;

&lt;h1 id=&quot;articles-and-blog-posts-ï¸&quot;&gt;Articles and Blog posts âœï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The Annotated GPT-2&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aman Arora recently published an exceptional blog post appropriately titled â€œ&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;&lt;strong&gt;The Annotated GPT-2&lt;/strong&gt;&lt;/a&gt;â€ explaining the inner workings of the Transformer based model called GPT-2. His approach was inspired by &lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;The Annotated Transformer&lt;/a&gt; that took an annotation approach to explain the important parts of the model through code and easy-to-follow explanations. Aman went through a great effort to re-implement OpenAIâ€™s GPT-2 using PyTorch and the Transformers library by Hugging Face. Itâ€™s brilliant work!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Beyond BERT?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Interesting &lt;a href=&quot;https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1&quot;&gt;&lt;strong&gt;take&lt;/strong&gt;&lt;/a&gt; by Sergi Castella on what lies beyond BERT. The main topics include improving metrics, how Hugging Faceâ€™s Transformers library empowers research, interesting datasets to look at, unpacking models, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Matrix Compression Operator&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
The TensorFlow Blog published a &lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;&lt;strong&gt;blog post&lt;/strong&gt;&lt;/a&gt; explaining the techniques and importance behind compressing matrices in a deep neural network model. &lt;em&gt;Matrix compression&lt;/em&gt; can help to build more efficient tiny models that can be incorporated into smaller devices such as phones and home assistants. Focusing on the compression of the models via methods like &lt;em&gt;low-rank-approximation&lt;/em&gt; and &lt;em&gt;quantization&lt;/em&gt; means that we donâ€™t need to compromise the quality of the model.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education ğŸ“&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Fundamentals of NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
I am excited to release a draft of Chapter 1 of my new series called &lt;a href=&quot;https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684&quot;&gt;&lt;strong&gt;Fundamentals of NLP&lt;/strong&gt;&lt;/a&gt;. It teaches NLP concepts starting from the basics while sharing best practices, important references, common mistakes to avoid, and what lies ahead in NLP. A Colab &lt;a href=&quot;https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ&quot;&gt;notebook&lt;/a&gt; is included and the project will be maintained &lt;a href=&quot;https://github.com/dair-ai/nlp_fundamentals&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;[Online] Review/Discussion: Part I Mathematical Foundations Reading Session&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Machine Learning Tokyo is hosting a remote online discussion reviewing chapters that were covered in their recent online study sessions. The group had previously studied chapters based on the book called &lt;a href=&quot;https://mml-book.github.io/&quot;&gt;Mathematics For Machine Learning&lt;/a&gt; written by Marc Peter Deisenroth, A Aldo Faisal, and Cheng Soon Ong. The &lt;a href=&quot;https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/&quot;&gt;&lt;strong&gt;event&lt;/strong&gt;&lt;/a&gt; is scheduled for March 8, 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Book recommendations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In a previous segment, we discussed the importance of matrix compression for building tiny ML models. If you are interested to learn more about how to build smaller deep neural networks for embedded systems check out this great book called &lt;a href=&quot;https://tinymlbook.com/?linkId=82595412&quot;&gt;&lt;strong&gt;TinyML&lt;/strong&gt;&lt;/a&gt; by Pete Warden and Daniel Situnayake.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Another interesting book to keep an eye on is the upcoming title &lt;strong&gt;â€œ&lt;/strong&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;&lt;strong&gt;Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;â€&lt;/strong&gt; by Jeremy Howard and Sylvain Gugger. The book aims to provide the necessary mathematical foundation to build and train models to approach tasks in the areas of computer vision and NLP.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;noteworthy-mentions-ï¸&quot;&gt;Noteworthy Mentions â­ï¸&lt;/h1&gt;

&lt;p&gt;You can access the previous issue of the NLP Newsletter &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-pytorch3d-deepspeed-turing-nlg-question-answering-benchmarks-hydra-sparse-322f018ee096&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1909.06576&quot;&gt;&lt;strong&gt;Torchmeta&lt;/strong&gt;&lt;/a&gt; is a library that allows ease of use of related data loaders for meta-learning research. It was authored by Tristan Deleu.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manuel Tonneau wrote a &lt;a href=&quot;https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter&quot;&gt;&lt;strong&gt;piece&lt;/strong&gt;&lt;/a&gt; offering a closer look at some of the machinery involved in language modeling. Some topics include &lt;em&gt;greedy&lt;/em&gt; and &lt;em&gt;beam search&lt;/em&gt; and &lt;em&gt;nucleus sampling&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
MIT &lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;&lt;strong&gt;releases&lt;/strong&gt;&lt;/a&gt; the full syllabus and course schedule for the course titled â€œIntroduction to Deep Learningâ€, including videos for lectures already delivered. They aim to release video lectures and slides every week.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Learn how to train a model for named entity recognition (NER) using a Transformer based approach in under &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py&quot;&gt;&lt;strong&gt;300 lines of code&lt;/strong&gt;&lt;/a&gt;. You can find the accompanying Google Colab &lt;a href=&quot;https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;If you have any datasets, projects, blog posts, tutorials, or papers that you wish to share in the next iteration of the NLP Newsletter, please free to reach out to me at ellfae@gmail.com or &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;&lt;strong&gt;DM on Twitter&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Subscribe&lt;/em&gt;&lt;/a&gt; &lt;em&gt;ğŸ”– to the NLP Newsletter to receive future issues in your inbox.&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_The_Annotated_GPT-2,_Understanding/&quot;&gt;NLP Newsletter #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,â€¦&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 23, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[BoletÃ­n informativo NLP #5: GPT-2 Explicado, Entendiendo â€˜Self-Distillationâ€™, Haiku, GANILLA, Sparkwiki, Ã‰tica en el NLP, Torchmeta,...]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/Bolet%C3%ADn_informativo_NLP_GPT-2_Explicado,_Entendie/" />
  <id>https://dair.ai/BoletÃ­n_informativo_NLP_GPT-2_Explicado,_Entendie</id>
  <published>2020-02-23T00:00:00-06:00</published>
  <updated>2020-02-23T00:00:00-06:00</updated>
  <author>
    <name>Nicolas Araque Volk</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Post Original en Ingles: &lt;a href=&quot;https://dair.ai/NLP_Newsletter_The_Annotated_GPT-2,_Understanding/&quot;&gt;https://dair.ai/NLP_Newsletter_The_Annotated_GPT-2,_Understanding/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Primero que todo, no puedo agradecerles lo suficiente a todos por el increible animo y soporte para continuar con el boletÃ­n informativo de NLP. Este esfuerzo requiere una investigaciÃ³n y ediciÃ³n tediosa, la cual encuentro gratificante y Ãºtil para proveer el mejor contenido. Espero que lo estÃ©n disfrutando tanto como yo.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;SuscrÃ­bete&lt;/a&gt; al BoletÃ­n Informativo de NLP para recibir futuras ediciones en tu email. Este boletin lo desarrolla Elvis Saravia, editor de dair.ai&lt;/p&gt;

&lt;h1 id=&quot;publicaciones&quot;&gt;PublicacionesğŸ“™&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Un entendimiento teÃ³rico del self-distillation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
En el contexto del Deep Learning, self-distillation es el proceso de transferir conocimiento de una arquitectura a otra arquitectura idÃ©ntica. Las predicciones del modelo original son alimentadas como valores objetivo al otro modelo mientras se entrena. AdemÃ¡s de tener propiedades deseables (como reducir el tamaÃ±o del modelo) resultados empÃ­ricos demuestran que esta aproximaciÃ³n trabaja bastante bien en sets de datos del tipo held-out. Un grupo de investigadores recientemente publicÃ³ una investigaciÃ³n que provee un anÃ¡lisis teÃ³rico con foco en entender mejor quÃ© estÃ¡ pasando en el proceso de destilaciÃ³n del conocimiento y por que es efectivo. Los resultados muestran que unas pocas rondas de self-distillation amplifica la regularizaciÃ³n (&lt;a href=&quot;https://twitter.com/TheGradient/status/1228132843630387201?s=20&quot;&gt;limitando progresivamente el nÃºmero de funciones bÃ¡sicas que representan la soluciÃ³n&lt;/a&gt;) lo que tiende a reducir el over-fitting. (Lee la investigaciÃ³n completa &lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;aquÃ­&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;TraducciÃ³n: Figura 1. IlustraciÃ³n esquemÃ¡tica del proceso de self-distillation por dos iteraciones. Fuente: &lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;https://arxiv.org/abs/2002.05715&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Los 2010: La dÃ©cada del Deep Learning / Mirada al 2020&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;http://people.idsia.ch/~juergen/&quot;&gt;JÃ¼rgen Schmidhuber,&lt;/a&gt; un pionero en la inteligencia artificial, publicÃ³ recientemente un &lt;a href=&quot;http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html&quot;&gt;artÃ­culo&lt;/a&gt; titulado â€œFoco en proveer un resumen histÃ³rico del Deep Learning desde el 2010â€. Algunos de los temas incluyen LSTMs, redes neuronales feedforward, GANs, DeepRL, Meta-Learning, World Models, distillings NNs, Attention Learning, etc. El artÃ­culo concluye con una mirada al 2020, donde llama la atenciÃ³n a temas como la privacidad y los mercados de datos.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Utilizando Redes Neuronales para resolver ecuaciones matemÃ¡ticas avanzadas.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Investigadores de Facebook AI publicaron una &lt;a href=&quot;https://arxiv.org/abs/1912.01412&quot;&gt;investigaciÃ³n&lt;/a&gt; que dice proponer un modelo entrenado en problemas matemÃ¡ticos y sus soluciones para aprender a predecir posibles soluciones en problemas cÃ³mo resolver integrales matematicas. El acercamiento estÃ¡ basado en un novedoso framework similar al usado en neural machine translation donde una expresiÃ³n matemÃ¡tica es representada como una especie de lenguaje y la soluciÃ³n es tratada como su traducciÃ³n. Por esto, en vez de que el resultado sea una traducciÃ³n, es la soluciÃ³n al problema matematico. Con esto, los investigadores concluyen que las redes neuronales profundas no solo son buenas para el razonamiento simbÃ³lico sino tambiÃ©n para tareas mÃ¡s diversas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ecuaciones incluidas como entrada con su correspondiente soluciÃ³n. &lt;a href=&quot;https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/&quot;&gt;Fuente&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;creatividad-y-sociedad-&quot;&gt;Creatividad y Sociedad ğŸ¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;IA para el descubrimiento cientÃ­fico&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Mattew Hutson &lt;a href=&quot;https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times&quot;&gt;reporta&lt;/a&gt; cÃ³mo la inteligencia artificial (IA) puede ser usada para producir emuladores con la capacidad de modelar fenÃ³menos naturales complejos. Estos modelos pueden a su vez apuntar a diferentes tipos de descubrimientos cientÃ­ficos. El desafÃ­o al construir estos emuladores es que necesitan una gran cantidad de datos y una bÃºsqueda extensiva de parÃ¡metros. Una &lt;a href=&quot;https://arxiv.org/abs/2001.08055&quot;&gt;investigaciÃ³n&lt;/a&gt; reciente propone una tÃ©cnica llamada DENSE, basada en &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_architecture_search&quot;&gt;neural architecture search&lt;/a&gt; para construir emuladores precisos utilizando una cantidad de datos de entrenamiento limitada. Los investigadores hicieron distintas pruebas construyendo simuladores para caso como astrofÃ­sica, ciencia climÃ¡tica, energÃ­a de fusiÃ³n, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Mejorando la traducciÃ³n imagen-a-ilustraciÃ³n&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
GANILLA es una aproximaciÃ³n que propone el uso de las GANs para mejorar la transferencia de estilo y contenido en la tarea de convertir una imagen en una ilustraciÃ³n. En particular, un modelo para &lt;a href=&quot;https://paperswithcode.com/task/image-to-image-translation&quot;&gt;imagen-a-ilustraciÃ³n&lt;/a&gt; es propuesto (con una red generadora mejorada) y evaluado basado en un nuevo framework para evaluaciÃ³n cuantitativa que considera tanto el contenido como el estilo de la imagen generada. La novedad de esta investigaciÃ³n es en la red generadora propuesta que considera un balance entre estilo y contenido que anteriores intentos fallaron en alcanzar. El cÃ³digo y modelo pre entrenado estÃ¡n disponibles en &lt;a href=&quot;https://github.com/giddyyupp/ganilla&quot;&gt;lÃ­nea&lt;/a&gt;. &lt;a href=&quot;https://arxiv.org/abs/2002.05638&quot;&gt;AquÃ­&lt;/a&gt; se puede encontrar la investigaciÃ³n completa.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;TraducciÃ³n: Ejemplos de salida de CycleGAN, DualGAN, y nuestro mÃ©todo (GANILLA) utilizando diferentes estilos. CycleGAN y GANILLA generan imÃ¡genes en un estilo de ilustraciÃ³n, pero DualGAN falla en transferir el estilo. Sin embargo, CycleGAN falle en preservar el contenido de la imagen original, por ejemplo, aleatoriamente coloca caras de animales en el aire. Nuestro mÃ©todo preserva contenido asÃ­ como tambiÃ©n transfiere el estilo de la imagen original.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Andrew Ng habla sobre su interÃ©s en self-supervised learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew Ng, fundador de &lt;a href=&quot;http://deeplearning.ai/&quot;&gt;deeplearning.ai&lt;/a&gt;, participa en el podcast Artificial Inteligence de Lex Friedman para &lt;a href=&quot;https://www.youtube.com/watch?v=0jspaMLxBig&quot;&gt;hablar&lt;/a&gt; de diferentes temas como sus comienzos en el ML, el futuro del AI y la educaciÃ³n, recomendaciones de uso para ML, sus metas personales y a cuÃ¡les tÃ©cnicas de ML prestar atenciÃ³n en el 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew tambien explico por que estÃ¡ muy emocionado sobre el self-supervised representation learning. &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html&quot;&gt;Self-supervised learning&lt;/a&gt; se trata sobre enfocar un problema de aprendizaje que intenta obtener la supervisiÃ³n de los datos en sÃ­ mismos para hacer uso de grandes cantidades de datos no clasificados, escenario mÃ¡s comÃºn que datos limpios y clasificados. Las representaciones aprendidas, opuesto al rendimiento de la tarea, son importantes y pueden ser usadas en tareas mÃ¡s adelante, similar a lo que estÃ¡ siendo usado en modelos de lenguaje natural como &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert&quot;&gt;BERT&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hay mucho interÃ©s en usar el self-supervised learning para aprender representaciones visuales generales que hagan al modelo mÃ¡s preciso en situaciones de bajos recursos. Por ejemplo, un nuevo mÃ©todo llamado &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;SimCLR&lt;/a&gt; (Dirigido por Geoffrey Hinton) propone un framework para el &lt;em&gt;constrastive self-supervised learning&lt;/em&gt; de representaciones visuales para mejorar el resultado de la clasificaciÃ³n de imÃ¡genes en diferentes contextos como el &lt;em&gt;transfer learning&lt;/em&gt; y el &lt;em&gt;semi-supervised learning&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;TraducciÃ³n: ImageNet top-1 accuracy de clasificadores lineales entrenados en representaciones aprendidas con diferentes mÃ©todos de auto-aprendizaje (pre entrenados en ImageNet). La cruz gris indica un modelo ResNet-50 supervisado. Nuestro mÃ©todo, SimCLR, es mostrado en negrita.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;herramientas-y-set-de-datos-ï¸&quot;&gt;Herramientas y Set de Datos âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;LibrerÃ­as JAX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/google/jax&quot;&gt;JAX&lt;/a&gt; es una nueva librerÃ­a que combina NumPy y diferenciaciÃ³n automÃ¡tica para realizar investigaciÃ³n de alto rendimiento en ML. Para simplificar procesos para construir redes neuronales usando JAX, DeepMind lanzÃ³ &lt;a href=&quot;https://github.com/deepmind/dm-haiku&quot;&gt;Haiku&lt;/a&gt; y &lt;a href=&quot;https://github.com/deepmind/rlax&quot;&gt;RLax&lt;/a&gt;. RLAx simplifica la implementaciÃ³n de agentes de reinforcement learning y Haiku simplifica la construcciÃ³n de redes neuronales utilizando paradigmas familiares de programaciÃ³n orientada a objetos.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Una herramienta para procesar datos de Wikipedia&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/epfl-lts2/sparkwiki&quot;&gt;Spakwiki&lt;/a&gt; es una herramienta para procesar datos de Wikipedia. Este lanzamiento es parte de muchos esfuerzos para permitir anÃ¡lisis interesantes de comportamiento como &lt;a href=&quot;https://arxiv.org/abs/2002.06885&quot;&gt;capturar tendencias y sesgos de lenguaje en ediciones de distinto idioma en Wikipedia&lt;/a&gt;. Los autores descubrieron que independientemente del lenguaje, el comportamiento de bÃºsqueda de los usuarios de Wikipedia es muy parecido en categorÃ­as como pelÃ­culas, mÃºsica y deportes pero que las diferencias se vuelven mÃ¡s aparentes con eventos locales y particularidades de cultura.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Tokenizadores, caso DistilBERT y modelos&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Una &lt;a href=&quot;https://github.com/huggingface/transformers/releases/tag/v2.5.0&quot;&gt;nueva entrega de Transformers&lt;/a&gt; de Hugging Face ahora incluye la integraciÃ³n de su rÃ¡pida librerÃ­a de tokenizaciÃ³n que intenta mejorar el tiempo de ejecuciÃ³n de modelos como BERT, RoBERTa, GPT2, y otro modelos construidos por la comunidad.&lt;/p&gt;

&lt;h1 id=&quot;Ã©tica-en-la-inteligencia-artificial-&quot;&gt;Ã‰tica en la Inteligencia Artificial ğŸš¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Consideraciones Ã©ticas para modelos de NLP y ML.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
En un nuevo &lt;a href=&quot;https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender&quot;&gt;episodio&lt;/a&gt; de &lt;a href=&quot;https://soundcloud.com/nlp-highlights&quot;&gt;NLP Highlights&lt;/a&gt;, Emily Bender habla sobre algunas consideraciones Ã©ticas cuando se desarrollan modelos de Procesamiento de Lenguaje Natural y tecnologÃ­as tanto en el contexto de la academia como de industria. Algunos de los tÃ³picos discutidos incluyen consideraciones Ã©ticas cuando se diseÃ±an tareas, recolecciÃ³n de datos, y eventualmente publicaciÃ³n de resultados.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Adicionalmente a todas las consideraciones mencionadas anteriormente, una preocupaciÃ³n que siempre se discute en la comunidad de AI es enfocarse demasiado en optimizar una mÃ©trica, lo que va en contra de los cimientos de lo que el campo trata de alcanzar. Rachel Thomas y David Uminsky discuten cÃ³mo esto puede salir mal a travÃ©s de un &lt;a href=&quot;https://arxiv.org/abs/2002.08512&quot;&gt;anÃ¡lisis&lt;/a&gt; con diferentes casos de uso. AdemÃ¡s, proponen un framework simple para mitigar el problema que involucra el uso y combinaciÃ³n de mÃºltiples mÃ©tricas, seguido del involucramiento directo de los usuarios afectados por la tecnologÃ­a que se estÃ¡ desarrollando.&lt;/p&gt;

&lt;h1 id=&quot;artÃ­culos-y-blogs-ï¸&quot;&gt;ArtÃ­culos y Blogs âœï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;GPT-2 Explicado&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aman Arora recientemente publicÃ³ un artÃ­culo excepcional titulado â€œ&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;&lt;strong&gt;El GPT-2 Explicado&lt;/strong&gt;&lt;/a&gt;â€ explicando el funcionamiento interno del modelo basado en la tÃ©cnica del Transformer llamado GPT-2. Su aproximaciÃ³n fue inspirada por el artÃ­culo &lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;El Transformer Explicado&lt;/a&gt; que tomÃ³ una aproximaciÃ³n a explicar las partes mÃ¡s importantes del modelo siguiendo ejemplos fÃ¡ciles de seguir y cÃ³digo comentado. Amant realizÃ³ un gran esfuerzo para re implementar GPT-2 de OpenAI utilizando PyTorch y la librerÃ­a de Transformers de Hugging Face. Es un trabajo brillante.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;Fuente&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;MÃ¡s allÃ¡ de BERT?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1&quot;&gt;OpiniÃ³n&lt;/a&gt; interesante de Sergi Castella en que hay mÃ¡s allÃ¡ de BERT. El tema principal incluye mejorar mÃ©tricas, como la librerÃ­a Transformer de Hugging Face empodera la investigaciÃ³n, set de datos interesantes para revisar, utilizaciÃ³n de modelos, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Operador para comprimir Matrices&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
El blog the TensorFlow publico un &lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;articulo&lt;/a&gt; donde explican las tÃ©cnicas para compresiÃ³n de matrices y su importancia en un modelo de red neuronal. La compresiÃ³n de matrices puede ayudar a construir modelos mÃ¡s eficientes y reducidos que pueden ser incorporados en dispositivos mÃ¡s pequeÃ±os como telÃ©fonos y asistentes del hogar. Enfocarse en la comprensiÃ³n por mÃ©todos como low-rank-approximation y quantization significa que no debemos comprometer la calidad del modelo&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;Fuente&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;educaciÃ³n&quot;&gt;EducaciÃ³nğŸ“&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Fundamentos del NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Estoy emocionado de liberar un borrador del capÃ­tulo 1 de mi nueva serie llamado &lt;a href=&quot;https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684&quot;&gt;Fundamentos del NLP&lt;/a&gt;. EnseÃ±a conceptos comenzando desde lo mÃ¡s bÃ¡sico, compartiendo buenas prÃ¡cticas, referencias importantes, errores comunes a evitar, y cuÃ¡l es el futuro del NLP. Un &lt;a href=&quot;https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ&quot;&gt;Colab Notebook&lt;/a&gt; estÃ¡ incluido y el proyecto serÃ¡ mantenido &lt;a href=&quot;https://github.com/dair-ai/nlp_fundamentals&quot;&gt;aquÃ­&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;[En lÃ­nea] RevisiÃ³n/DiscusiÃ³n: Parte 1 SesiÃ³n de lectura de Fundamentos MatemÃ¡ticos&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Machine Learning Tokio estÃ¡ organizando una discusiÃ³n en lÃ­nea para revisar los capÃ­tulos que fueron cubiertos en su mÃ¡s reciente sesiÃ³n de estudio. El grupo ha estudiado previamente capÃ­tulos del libro &lt;a href=&quot;https://mml-book.github.io/&quot;&gt;MatemÃ¡ticas para el Machine Learning&lt;/a&gt; escrito por Marc Peter Deisenroth, A Aldo Faisal, y Cheng Soon Ong. El &lt;a href=&quot;https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/&quot;&gt;evento&lt;/a&gt; estÃ¡ pautado para el 8 de Marzo de 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Libros Recomendados&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
En un segmento previo discutimos la importancia de la compresiÃ³n de matrices para construir modelos de ML compactos. Si estÃ¡s interesado en aprender mÃ¡s sobre como construir redes neuronales mÃ¡s pequeÃ±as para sistemas embebidos te recomiendo este gran libro llamado &lt;a href=&quot;https://tinymlbook.com/?linkId=82595412&quot;&gt;TinyML&lt;/a&gt; por Pete Warden y Daniel Situnayake.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043&quot;&gt;Fuente&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Otro libro interesante a tener en cuenta es â€œ&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;Deep Learning para programadores con fastai y pyTorch: Aplicaciones AI sin un PhD&lt;/a&gt;â€ por Jeremy Howard y Sylvain Gugger. El libro trata de proveer la matemÃ¡tica necesaria para construir y entrenar modelos que resuelvan tareas en el Ã¡rea de visiÃ³n por computador y Entendimiento natural de texto.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;Fuente&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;menciones-a-considerar-ï¸&quot;&gt;Menciones a considerar â­ï¸&lt;/h1&gt;

&lt;p&gt;Puedes acceder a las versiones anteriores de este boletÃ­n &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-pytorch3d-deepspeed-turing-nlg-question-answering-benchmarks-hydra-sparse-322f018ee096&quot;&gt;aquÃ­&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1909.06576&quot;&gt;Torchmeta&lt;/a&gt; es una librerÃ­a que permite el uso de data loaders para la investigaciÃ³n en meta-learning. El creador es Tristan Deleu.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manuel Tonneau escribiÃ³ una &lt;a href=&quot;https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter&quot;&gt;pieza&lt;/a&gt; ofreciendo una mirada mÃ¡s detallada a alguna de la maquinaria involucrada en el modelamiento del lenguaje. Algunos tÃ³picos incluyen bÃºsquedas greedy y beam, asÃ­ como tambiÃ©n nucleus sampling.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
El MIT &lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;liberÃ³&lt;/a&gt; el programa de estudio completo del curso titulado â€œIntroducciÃ³n al Deep Learningâ€, incluye videos de las clases que ya se dictaron. EstÃ¡n apuntando a liberar videos y presentaciones todas las semanas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aprende a entrenar un modelo para reconocimiento de entidades nombradas (NER) utilizando una aproximaciÃ³n vÃ­a Transformers en menos de &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py&quot;&gt;300 lÃ­neas de cÃ³digo&lt;/a&gt;. Puedes encontrar el Google Colab que lo acompaÃ±a &lt;a href=&quot;https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn&quot;&gt;aquÃ­&lt;/a&gt;.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/Bolet%C3%ADn_informativo_NLP_GPT-2_Explicado,_Entendie/&quot;&gt;BoletÃ­n informativo NLP #5: GPT-2 Explicado, Entendiendo â€˜Self-Distillationâ€™, Haiku, GANILLA, Sparkwiki, Ã‰tica en el NLP, Torchmeta,...&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 23, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Fundamentals of NLP (Chapter 1): Tokenization, Lemmatization, Stemming, and Sentence Segmentation]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/Fundamentals-of-NLP-Chapter-1/" />
  <id>https://dair.ai/Fundamentals-of-NLP-Chapter-1</id>
  <published>2020-02-21T00:00:00-06:00</published>
  <updated>2020-02-21T00:00:00-06:00</updated>
  <author>
    <name>Elvis Saravia</name>
    <uri>https://dair.ai</uri>
    <email>ellfae@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/1193/1*Qtz7CEKPnMyGx-62YqHcCg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Natural language processing (NLP) has made substantial advances in the past few years due to the success of &lt;a href=&quot;https://nlpoverview.com/&quot;&gt;modern techniques&lt;/a&gt; that are based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;&gt;deep learning&lt;/a&gt;. With the rise of the popularity of NLP and the availability of different forms of large-scale data, it is now even more imperative to understand the inner workings of NLP techniques and concepts, from first principles, as they find their way into real-world usage and applications that affect society at large. Building intuitions and having a solid grasp of concepts are both important for coming up with innovative techniques, improving research, and building safe, human-centered AI and NLP technologies.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In this first chapter, which is part of a series called &lt;a href=&quot;https://github.com/dair-ai/nlp_fundamentals&quot;&gt;Fundamentals of NLP&lt;/a&gt;, we will learn about some of the most important basic concepts that power NLP techniques used for research and building real-world applications. Some of these techniques include lemmatization, stemming, tokenization, and sentence segmentation. These are all important techniques to train efficient and effective NLP models. Along the way, we will also cover best practices and common mistakes to avoid when training and building NLP models. We also provide some exercises for you to keep practicing and exploring some ideas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In every chapter, we will introduce the theoretical aspect and motivation of each concept covered. Then we will obtain hands-on experience by using bootstrap methods, industry-standard tools, and other open-source libraries to implement the different techniques. Along the way, we will also cover best practices, share important references, point out common mistakes to avoid when training and building NLP models, and discuss what lies ahead.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
The project will be maintained on &lt;a href=&quot;https://github.com/dair-ai/nlp_fundamentals&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Get access to the first chapter at this &lt;a href=&quot;https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ&quot;&gt;Colab notebook&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/640/1*mS5NcoJ_c8hYTjiJsuu_8g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/Fundamentals-of-NLP-Chapter-1/&quot;&gt;Fundamentals of NLP (Chapter 1): Tokenization, Lemmatization, Stemming, and Sentence Segmentation&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 21, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLPç®€æŠ¥ [CH]: Tokenizers, TensorFlow 2.1, TextVectorization, TorchIO, NLP Shortfalls,â€¦]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP%E7%AE%80%E6%8A%A5_Tokenizers,_TensorFlow_2_1,_TextVectorization/" />
  <id>https://dair.ai/NLPç®€æŠ¥_Tokenizers,_TensorFlow_2_1,_TextVectorization</id>
  <published>2020-02-20T00:00:00-06:00</published>
  <updated>2020-02-20T00:00:00-06:00</updated>
  <author>
    <name>Elvis Saravia</name>
    <uri>https://dair.ai</uri>
    <email>ellfae@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2400/1*gLVPodYjYd4YaF9sJbSpjg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;Thanks to&lt;/em&gt; &lt;a href=&quot;https://blog.csdn.net/Kaiyuan_sjtu&quot;&gt;&lt;em&gt;Kaiyuan&lt;/em&gt;&lt;/a&gt; &lt;em&gt;(WeChat: NewBeeNLP) for this great translation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
æ¬¢è¿æ¥åˆ°èˆ¹æ–°æ ç›®&lt;strong&gt;ã€ŒNLP ç®€æŠ¥ã€&lt;/strong&gt;ï¼Œæœ¬æ–°é—»ç®€æŠ¥çš„ç›®çš„æ˜¯è®©ä½ ä¸å¿…èŠ±è´¹å¤ªå¤šæ—¶é—´å°±å¯ä»¥äº†è§£ä¸ NLP å’Œ ML æœ‰å…³çš„ä¸€äº›æœ‰è¶£å’Œæœ€æ–°çš„æ•…äº‹ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;å¦‚æœæƒ³è®©è‡ªå·±æœ‰è¶£çš„ç ”ç©¶/é¡¹ç›®å‡ºç°åœ¨ NLP ç®€æŠ¥ä¸­ï¼Œéšæ—¶åœ¨å…¬ä¼—å·åå°ç•™è¨€è”ç³»æˆ‘&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
ä¸‹é¢æ¥çœ‹çœ‹ç¬¬ä¸€æœŸçš„å†…å®¹&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;1ã€Publications ğŸ“™&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;1.1 ç”¨äºä¹³è…ºç™Œç­›æŸ¥çš„AIç³»ç»Ÿ&lt;/li&gt;
      &lt;li&gt;1.2 ä¿¡æ¯æŠ½å–&lt;/li&gt;
      &lt;li&gt;1.3 Improved recommendation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;2ã€Creativity and Society ğŸ¨&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;2.1 AIå°±ä¸š&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;3ã€Tools and Datasets âš™ï¸&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;3.1 ä¸€ä¸ªæé€Ÿåˆ†è¯å™¨&lt;/li&gt;
      &lt;li&gt;3.2 ç”¨äºæœç´¢çš„ML&amp;amp;NLP&lt;/li&gt;
      &lt;li&gt;3.3 åŒ»å­¦å›¾åƒåˆ†æ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;4ã€Ethics in AI ğŸš¨&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;4.1 MLç¤¾åŒºçš„æ¬ºè¯ˆè¡Œä¸º&lt;/li&gt;
      &lt;li&gt;4.2 æœºå™¨ç¿»è¯‘ä¸­çš„æ€§åˆ«åè§&lt;/li&gt;
      &lt;li&gt;4.3 MLåå·®ä¸å…¬æ­£æ€§&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;5ã€Articles and Blog posts âœï¸&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;5.1 NLP shortfalls&lt;/li&gt;
      &lt;li&gt;5.2 NLPå’ŒML2019å¹´äº®ç‚¹&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;6ã€Education ğŸ“&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;6.1 Democratizing AI education&lt;/li&gt;
      &lt;li&gt;6.2 Top NLP and ML Books&lt;/li&gt;
      &lt;li&gt;6.3 ä½¿ç”¨æ ¸æ–¹æ³•çš„æœºå™¨å­¦ä¹ &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;7ã€Noteworthy Mentions â­ï¸&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;1publications-&quot;&gt;1ã€Publications ğŸ“™&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;1.1 ç”¨äºä¹³è…ºç™Œç­›æŸ¥çš„ AI ç³»ç»Ÿ&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind åœ¨ Nature æ‚å¿—ä¸Šå‘è¡¨äº†ä¸€ç¯‡åä¸ºâ€œInternational evaluation of an AI system for breast cancer screening[1]â€çš„æ–°è®ºæ–‡ã€‚ä½œè€…è¯´ï¼Œè¿™é¡¹å·¥ä½œæ˜¯å…³äºåœ¨ä¹³è…ºç™Œç­›æŸ¥æ–¹é¢è¶…è¶Šäººç±»ä¸“å®¶çš„ AI ç³»ç»Ÿçš„è¯„ä¼°ã€‚å½“å‰çš„ AI ç³»ç»Ÿæ˜¯å¦çœŸçš„å¯ä»¥å®ç°è¿™ä¸€ç‚¹å°šæœ‰äº‰è®®ï¼Œå¹¶ä¸”å¯¹äºè¿™ç§ç±»å‹çš„ç³»ç»ŸåŠå…¶è¯„ä¼°æ–¹å¼ä¸€ç›´å­˜åœ¨æ‰¹è¯„ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.2 ä¿¡æ¯æŠ½å–&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pankaj Gupta å…¬å¼€å‘å¸ƒäº†ä»–çš„åšå£«å­¦ä½è®ºæ–‡ï¼Œé¢˜ç›®ä¸ºâ€œNeural Information Extraction From Natural Language Text[2]â€ã€‚ä¸»è¦è®¨è®ºå¦‚ä½•ä½¿ç”¨åŸºäºç¥ç»çš„æ–¹æ³•æœ‰æ•ˆåœ°ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­æå–è¯­ä¹‰å…³ç³»ï¼Œ æ­¤ç±»ç ”ç©¶å·¥ä½œæ—¨åœ¨ä¿ƒè¿›æ„å»ºç»“æ„åŒ–çš„çŸ¥è¯†åº“ï¼Œè¯¥çŸ¥è¯†åº“å¯ç”¨äºä¸€ç³»åˆ—ä¸‹æ¸¸ NLP åº”ç”¨ç¨‹åºï¼Œä¾‹å¦‚ Web æœç´¢ï¼Œé—®é¢˜è§£ç­”ä»¥åŠå…¶ä»–ä»»åŠ¡ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.3 Improved recommendations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
MIT å’Œ IBM çš„ç ”ç©¶äººå‘˜åŸºäºä¸‰ç§å¹¿æ³›ä½¿ç”¨çš„æ–‡æœ¬åˆ†æå·¥å…·ï¼ˆä¸»é¢˜å»ºæ¨¡ï¼Œå•è¯åµŒå…¥å’Œæœ€ä½³ä¼ è¾“ï¼‰çš„ç»„åˆï¼Œå¼€å‘äº†ä¸€ç§ç”¨äºåˆ†ç±»ï¼Œæ˜¾ç¤ºå’Œæœç´¢ç›¸å…³æ–‡æ¡£çš„æ–¹æ³•ï¼ˆäºå»å¹´åœ¨ NeurIPS ä¸Šå‘å¸ƒï¼‰ã€‚è¯¥æ–¹æ³•è¿˜ä¸ºæ–‡æ¡£æ’åºæä¾›äº†æœ‰å¸Œæœ›çš„ç»“æœã€‚æ­¤ç±»æ–¹æ³•é€‚ç”¨äºéœ€è¦å¯¹å¤§è§„æ¨¡æ•°æ®ï¼ˆä¾‹å¦‚æœç´¢å’Œæ¨èç³»ç»Ÿï¼‰è¿›è¡Œæ”¹è¿›å’Œæ›´å¿«å»ºè®®çš„å„ç§åœºæ™¯å’Œåº”ç”¨ã€‚&lt;/p&gt;

&lt;h1 id=&quot;2creativity-and-society-&quot;&gt;2ã€Creativity and Society ğŸ¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;2.1 AI å°±ä¸š&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Stanford 2019 å¹´ AI æŒ‡æ•°æŠ¥å‘Š[3]è¡¨æ˜ï¼Œå¯¹ AI ä»ä¸šè€…çš„éœ€æ±‚æ›´å¤šã€‚ä½†æ˜¯ï¼Œä¸ AI ç›¸å…³çš„å·¥ä½œæœ‰å¾ˆå¤šæ–¹é¢ï¼Œä¾‹å¦‚èŒä¸šè½¬å˜å’Œé¢è¯•ä»ç„¶æ²¡æœ‰é€‚å½“å®šä¹‰ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
åœ¨åšå®¢â€Shifting Careers to Autonomous Vehicles[4]â€ä¸­ï¼ŒVladimir Iglovivok è¯¦ç»†ä»‹ç»äº†ä»–çš„èŒä¸šç”Ÿæ¶¯å’Œ ML å†’é™©ï¼Œä»æ„å»ºä¼ ç»Ÿçš„æ¨èç³»ç»Ÿåˆ°æ„å»ºå£®è§‚çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹ï¼ˆèµ¢å¾—äº† Kaggle ç«èµ›ï¼‰ã€‚ä»–ç°åœ¨åœ¨ Lyft ä»äº‹è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„å·¥ä½œï¼Œä½†æ˜¯åˆ°è¾¾é‚£é‡Œçš„æ—…ç¨‹å¹¶ä¸å®¹æ˜“ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å¦‚æœ n çœŸçš„å¯¹ AI äº‹ä¸šæ„Ÿå…´è¶£å¹¶ä¸”å¾ˆè®¤çœŸï¼ŒAndrew Ng çš„å…¬å¸ deeplearning.ai æˆç«‹äº† Workeraï¼Œè¯¥å…¬å¸ä¸“é—¨è‡´åŠ›äºå¸®åŠ©æ•°æ®ç§‘å­¦å®¶å’Œæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆä»äº‹ AI äº‹ä¸šã€‚å¯ä»¥åœ¨æ­¤å¤„è·å–å…³äºWorkra çš„å®˜æ–¹æŠ¥å‘Š[5]ã€‚&lt;/p&gt;

&lt;h1 id=&quot;3tools-and-datasets-ï¸&quot;&gt;3ã€Tools and Datasets âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;3.1 ä¸€ä¸ªæé€Ÿåˆ†è¯å™¨&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hugging Face æ˜¯ä¸€å®¶çš„ NLP åˆåˆ›å…¬å¸ï¼Œæ‹¥æœ‰å¼€æºçš„ Tokenizersï¼Œè¿™æ˜¯ä¸€ç§å¯åœ¨ç°ä»£ NLP pipeline ä¸­ä½¿ç”¨çš„è¶…å¿«é€Ÿçš„åˆ†è¯å™¨ï¼Œå¯ä»¥æŸ¥çœ‹Tokenizers GitHub åº“[6]ä»¥è·å–æœ‰å…³å¦‚ä½•ä½¿ç”¨ Tokenizer çš„æ–‡æ¡£ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*BGcXk6Yf9fXGZlEtxz1hcg.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
TensorFlow 2.1 åˆå¹¶äº†ä¸€ä¸ªæ–°çš„TextVectorization å±‚[7]ï¼Œä½ å¯ä»¥è½»æ¾å¤„ç†åŸå§‹å­—ç¬¦ä¸²å¹¶æœ‰æ•ˆåœ°æ‰§è¡Œæ–‡æœ¬ normalizationï¼Œtokenizationï¼Œn-gram ç”Ÿæˆå’Œè¯æ±‡ç´¢å¼•ã€‚ç‚¹å‡»æŸ¥çœ‹Chollet çš„ Colab ç¬”è®°[8]æœ¬ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨è¯¥åŠŸèƒ½è¿›è¡Œç«¯åˆ°ç«¯æ–‡æœ¬åˆ†ç±»ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.2 ç”¨äºæœç´¢çš„ ML&amp;amp;NLP&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å»å¹´ï¼ŒNLP å–å¾—äº†å·¨å¤§è¿›æ­¥ï¼Œå…¶ä¸­ä¸€ä¸ªé¢†åŸŸæ˜¯ä¸€ç³»åˆ—æ”¹è¿›å’Œæ–°çš„ç ”ç©¶æ–¹å‘ã€‚æœç´¢å¯èƒ½æ˜¯å¯èƒ½ä»è¿ç§»å­¦ä¹  NLP ä¸­å—ç›Šçš„é‚£äº›é¢†åŸŸä¹‹ä¸€ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å°½ç®¡æœç´¢å±äºä¿¡æ¯æ£€ç´¢é¢†åŸŸï¼Œä½†ä»æœ‰æœºä¼šæ„å»ºä½¿ç”¨ç°ä»£ NLP æŠ€æœ¯ï¼ˆä¾‹å¦‚æ¥è‡ªåŸºäºBERT[9]çš„åŸºäºå˜å‹å™¨çš„æ¨¡å‹çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼‰æ¥æ”¹è¿›è¯­ä¹‰æœç´¢çš„æœç´¢å¼•æ“ã€‚Google åœ¨å‡ ä¸ªæœˆå‰å‘å¸ƒäº†ä¸€ç¯‡åšå®¢æ–‡ç« ï¼Œè®¨è®ºäº†ä»–ä»¬å¦‚ä½•åˆ©ç”¨ BERT æ¨¡å‹æ¥æ”¹å–„å’Œç†è§£æœç´¢[10]ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å¦‚æœæ‚¨å¯¹å¦‚ä½•å°†ä¸Šä¸‹æ–‡åŒ–è¡¨ç¤ºå½¢å¼åº”ç”¨äºä½¿ç”¨ Elasticsearch å’Œ TensorFlow ç­‰å¼€æ”¾å¼æœç´¢æŠ€æœ¯çš„æœç´¢æ„Ÿåˆ°å¥½å¥‡ï¼Œåˆ™å¯ä»¥æŸ¥çœ‹â€Elasticsearch meets BERTâ€[11]æˆ–â€Building a Search Engine with BERT and TensorFlowâ€[12]ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.3 åŒ»å­¦å›¾åƒåˆ†æ&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
TorchIO[13]æ˜¯åŸºäºæµè¡Œçš„æ·±åº¦å­¦ä¹ åº“ PyTorch çš„ Python è½¯ä»¶åŒ…ã€‚TorchIO æä¾›çš„åŠŸèƒ½å¯è½»æ¾é«˜æ•ˆåœ°è¯»å–å’Œé‡‡æ · 3D åŒ»å­¦å›¾åƒã€‚åŠŸèƒ½åŒ…æ‹¬ç”¨äºæ•°æ®æ‰©å……å’Œé¢„å¤„ç†çš„ç©ºé—´å˜æ¢ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/0*FSPuSC8TK9X-NQ2q.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4ethics-in-ai-&quot;&gt;4ã€Ethics in AI ğŸš¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;4.1 ML ç¤¾åŒºçš„æ¬ºè¯ˆè¡Œä¸º&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Kaggle æ¯”èµ›çš„ç¬¬ä¸€åä¼˜èƒœè€…å› æ¬ºè¯ˆæ´»åŠ¨è€Œè¢«å–æ¶ˆå‚èµ›èµ„æ ¼ï¼Œå…¶é˜Ÿä¼ä½¿ç”¨äº†èªæ˜ä½†ä¸è´Ÿè´£ä»»å’Œä¸å¯æ¥å—çš„ç­–ç•¥æ¥èµ¢å¾—æ¯”èµ›çš„ç¬¬ä¸€åã€‚åŸæ–‡â€PetFinder.my Contest: 1st Place Winner Disqualified[14]â€é‡ç‚¹ä»‹ç»äº†æœºå™¨å­¦ä¹ ç¤¾åŒºæƒ³è¦ç¼“è§£çš„è®¸å¤šä¸¥é‡ä¸”æ— æ³•æ¥å—çš„è¡Œä¸ºï¼Œæ­£ç¡®å’Œé“å¾·åœ°ä½¿ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯æ˜¯å‰è¿›çš„å”¯ä¸€æ–¹æ³•ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;4.2 æœºå™¨ç¿»è¯‘ä¸­çš„æ€§åˆ«åè§&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å…³äºæœºå™¨ç¿»è¯‘ç³»ç»Ÿæ˜¯å¦åæ˜ æ€§åˆ«åè§çš„ä¸»é¢˜ï¼Œä¸€ç»„ç ”ç©¶äººå‘˜å‘è¡¨äº†è¿™ç¯‡å‡ºè‰²çš„è®ºæ–‡ï¼Œâ€Assessing Gender Bias in Machine Translation â€“ A Case Study with Google Translate[15]â€ï¼Œæå‡ºäº†ä½¿ç”¨ Google ç¿»è¯‘çš„æ¡ˆä¾‹ç ”ç©¶ã€‚ä½œè€…å£°ç§°çš„ä¸€é¡¹å‘ç°æ˜¯ï¼ŒGoogle ç¿»è¯‘â€œè¡¨ç°å‡ºå¼ºçƒˆçš„ç”·æ€§è¿çº¦å€¾å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸æ€§åˆ«åˆ†å¸ƒå¤±è¡¡æœ‰å…³çš„é¢†åŸŸï¼Œä¾‹å¦‚ STEM å·¥ä½œã€‚â€&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;4.3 ML åå·®ä¸å…¬æ­£æ€§&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å¦‚æœæ‚¨æƒ³è®©æ‰€æœ‰äººéƒ½äº†è§£ AI ä¼¦ç†å’Œå…¬å¹³ï¼Œé‚£ä¹ˆè¿™æ˜¯ä¸€ä¸ªç”±Timnit Gebru[16]ä¸»æŒï¼Œç”± TWIML ä¸»æŒçš„ä¸é”™çš„æ’­å®¢ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Timnit æ˜¯ ML å…¬å¹³æ€§æ–¹é¢çš„æ°å‡ºç ”ç©¶è€…ï¼Œä»–ä¸ Eun Seo Jo ä¸€èµ·å‘è¡¨äº†ä¸€ç¯‡è®ºæ–‡ï¼Œâ€Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learningâ€[17]ä»–ä»¬ç¡®å®šäº†æ¡£æ¡ˆä¸­æ–‡æ¡£æ”¶é›†å®è·µçš„äº”ç§å…³é”®æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥ä¸ºç¤¾ä¼šæ–‡åŒ– ML ä¸­çš„æ•°æ®æ”¶é›†æä¾›æ›´å¯é çš„æ–¹æ³•ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´è·¨å­¦ç§‘åˆä½œç ”ç©¶è·å¾—æ›´ç³»ç»Ÿçš„æ•°æ®æ”¶é›†æ–¹æ³•ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sina Fazelpour å’Œ Zachary Lipton æœ€è¿‘å‘è¡¨äº†ä¸€ç¯‡è®ºæ–‡ï¼Œâ€fairness-non-ideal-fazelpour-lipton-2020[18]â€ï¼Œä»–ä»¬è®¤ä¸ºï¼Œç”±äºæˆ‘ä»¬éç†æƒ³ä¸–ç•Œçš„äº§ç”Ÿæ–¹å¼çš„æ€§è´¨ï¼ŒåŸºäºç†æƒ³æ€ç»´çš„å…¬å¹³ ML å¯èƒ½ä¼šå¯¼è‡´è¯¯å¯¼æ”¿ç­–å’Œå¹²é¢„æªæ–½ã€‚å®é™…ä¸Šï¼Œä»–ä»¬çš„åˆ†æè¡¨æ˜â€œæå‡ºçš„å…¬å¹³ ML ç®—æ³•çš„ç¼ºç‚¹åæ˜ äº†ç†æƒ³æ–¹æ³•æ‰€é¢ä¸´çš„å¹¿æ³›é—®é¢˜ã€‚â€&lt;/p&gt;

&lt;h1 id=&quot;5articles-and-blog-posts-ï¸&quot;&gt;5ã€Articles and Blog posts âœï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;5.1 NLP shortfalls&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Benjamin Heinzerling åœ¨ The Gradient ä¸­å‘è¡¨äº†ä¸€ç¯‡æœ‰è¶£çš„æ–‡ç« ï¼Œè®¨è®ºäº†NLP ä¸è¶³çš„é¢†åŸŸ[19]ï¼Œä¾‹å¦‚è®ºç‚¹ç†è§£å’Œå¸¸è¯†æ¨ç†ã€‚æœ¬æ°æ˜å‚è€ƒäº† Nivinï¼†Kao çš„æœ€æ–°è®ºæ–‡ï¼Œâ€Probing Neural Network Comprehension of Natural Language Arguments[20]â€ï¼Œè¯¥è®ºæ–‡æŒ‘æˆ˜å’Œè´¨ç–‘äº†è½¬ç§»å­¦ä¹ å’Œè¯­è¨€æ¨¡å‹å¯¹é«˜çº§è‡ªç„¶è¯­è¨€ç†è§£çš„èƒ½åŠ›ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;5.2 NLP å’Œ ML2019 å¹´äº®ç‚¹&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å¯¹äºæ–°çš„ä¸€å¹´ï¼ŒæŠ¥å‘Šâ€NLP Year in Review â€” 2019[21]â€ï¼Œè®°å½•äº† 2019 å¹´çš„ä¸€äº›æœ€æœ‰è¶£çš„ NLP å’Œ ML äº®ç‚¹ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å¡å·´æ–¯è’‚å®‰Â·é²å¾·ï¼ˆSebastian Ruderï¼‰æœ€è¿‘è¿˜å†™äº†ä¸€ç¯‡ç²¾å½©è€Œè¯¦å°½çš„åšå®¢æ–‡ç« [22]ï¼Œä»‹ç»äº†åå¤§å…³äº ML å’Œ NLP çš„ç ”ç©¶æ–¹å‘ï¼Œä»–è®¤ä¸ºè¿™å¾ˆæœ‰å½±å“åŠ› åœ¨ 2019 å¹´ã€‚åˆ—è¡¨ä¸­åŒ…æ‹¬è¯¸å¦‚é€šç”¨æ— ç›‘ç£é¢„è®­ç»ƒï¼Œåº”ç”¨äºç§‘å­¦çš„ ML å’Œ NLPï¼Œå¢å¼ºé¢„è®­ç»ƒæ¨¡å‹ï¼Œé«˜æ•ˆå’Œè¿œç¨‹ Transformers ç­‰ä¸»é¢˜ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI ç ”ç©¶ä¼šå‘å¸ƒä»–ä»¬ä¸€å¹´æ¥è¿›è¡Œçš„ç ”ç©¶çš„æ‘˜è¦ä»¥åŠä»–ä»¬æ­£åœ¨å…³æ³¨çš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œâ€Google Research: Looking Back at 2019, and Forward to 2020 and Beyond[23]â€ã€‚&lt;/p&gt;

&lt;h1 id=&quot;6education-&quot;&gt;6ã€Education ğŸ“&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;6.1 Democratizing AI education&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
ä¸ºäº†ä½¿ AI æ•™è‚²æ°‘ä¸»åŒ–å¹¶å‘å¤§ä¼—æ™®åŠ AI æŠ€æœ¯çš„å½±å“ï¼Œèµ«å°”è¾›åŸºå¤§å­¦ä¸ Reaktor åˆä½œå‘å¸ƒäº†æ¶µç›– AI åŸºç¡€çŸ¥è¯†çš„ç²¾å½©å…è´¹è¯¾ç¨‹ã€‚å—æ¬¢è¿çš„è¯¾ç¨‹ç§°ä¸ºâ€œElements of AI[24]â€ï¼ŒåŒ…æ‹¬è¯¸å¦‚ AI ä¼¦ç†å­¦ï¼ŒAI å“²å­¦ï¼Œç¥ç»ç½‘ç»œï¼Œæœ´ç´ è´å¶æ–¯è§„åˆ™ç­‰ä¸»é¢˜ï¼Œä»¥åŠå…¶ä»–åŸºç¡€ä¸»é¢˜ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Stanford CS224N å†æ¬¡æ¨å‡ºäº†æµè¡Œçš„â€œNatural Language Processing with Deep Learning[25]â€è¯¾ç¨‹ã€‚è¯¥è¯¾ç¨‹äºä»Šå¹´ 1 æœˆ 7 æ—¥æ­£å¼å¼€å§‹ï¼Œå› æ­¤ï¼Œå¦‚æœæ‚¨æƒ³å­¦ä¹ è¯¥è¯¾ç¨‹ï¼Œè¯·è®¿é—®å…¶ç½‘ç«™ä»¥è·å–å®Œæ•´çš„è¯¾ç¨‹æçº²ï¼Œå¹»ç¯ç‰‡ï¼Œè§†é¢‘ï¼Œé˜…è¯»å»ºè®®ç­‰ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.2 Top NLP and ML Books&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å…³äºä¸€äº›NLP å’Œ ML é¢†åŸŸä¹¦ç±æ¨è[26]ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.3 ä½¿ç”¨æ ¸æ–¹æ³•çš„æœºå™¨å­¦ä¹ &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
è¯¸å¦‚ PCA å’Œ K-means ä¹‹ç±»çš„æ ¸æ–¹æ³•å·²ç»å­˜åœ¨äº†å¾ˆé•¿ä¸€æ®µæ—¶é—´ï¼Œè¿™æ˜¯å› ä¸ºå®ƒä»¬å·²æˆåŠŸåº”ç”¨äºå„ç§åº”ç”¨ï¼Œä¾‹å¦‚å›¾å½¢å’Œç”Ÿç‰©åºåˆ—ã€‚æŸ¥çœ‹è¿™å¥—æ¶µç›–äº†å„ç§Kernel Methods[27]åŠå…¶å†…éƒ¨å·¥ä½œåŸç†çš„ç»¼åˆå¹»ç¯ç‰‡ã€‚è¿™ä¹Ÿæ˜¯ä¸€ä¸ªç”± Francis Bach ç»´æŠ¤å¾ˆæ£’çš„åšå®¢ï¼Œâ€Are all kernels cursed?[28]â€ï¼Œè®¨è®ºäº†å†…æ ¸æ–¹æ³•å’Œå…¶ä»–æœºå™¨å­¦ä¹ ä¸»é¢˜ã€‚&lt;/p&gt;

&lt;h1 id=&quot;7notable-mentions-ï¸&quot;&gt;7ã€Notable Mentions â­ï¸&lt;/h1&gt;

&lt;p&gt;å¦å¤–è¿˜æœ‰ä¸€äº›æœ‰è¶£çš„ç ”ç©¶/é¡¹ç›®ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;John Langford çš„åšå®¢[29]ï¼Œè®¨è®ºæœºå™¨å­¦ä¹ ç†è®ºï¼›&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;è®¸å¤šè¡Œä¸šçš„é¢å‘æœºå™¨å­¦ä¹ çš„æŠ€æœ¯å·²ç»ä½¿ç”¨æ¢¯åº¦å¢å¼ºæœºå™¨å¤šå¹´äº†ã€‚çœ‹çœ‹è¿™ç¯‡æ–‡ç« [30]ï¼Œå…¶ä¸­ä»‹ç»äº†ä¸€ä¸ªç”¨äºåº”ç”¨æ¢¯åº¦å¢å¼ºçš„åº“ XGBoostã€‚&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;å¦‚æœä½ å¯¹å­¦ä¹ å¦‚ä½•è®¾è®¡å’Œæ„å»ºåŸºäºæœºå™¨å­¦ä¹ çš„åº”ç”¨ç¨‹åºå¹¶å°†å…¶æŠ•å…¥ç”Ÿäº§æ„Ÿå…´è¶£ï¼Œé‚£ä¹ˆEmmanuel Ameisen[31]çš„ä¹¦ç±å¯ä»¥å¸®åŠ©ä½ ã€‚&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;æœ¬æ–‡å‚è€ƒèµ„æ–™&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[1] &lt;strong&gt;International evaluation of an AI system for breast cancer screening:&lt;/strong&gt; https://www.nature.com/articles/s41586-019-1799-6&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[2] &lt;strong&gt;Neural Information Extraction From Natural Language Text:&lt;/strong&gt; https://www.researchgate.net/publication/336739252_PhD_Thesis_Neural_Information_Extraction_From_Natural_Language_Text&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[3] &lt;strong&gt;Stanford 2019 å¹´ AI æŒ‡æ•°æŠ¥å‘Š:&lt;/strong&gt; https://hai.stanford.edu/sites/g/files/sbiybj10986/f/ai_index_2019_report.pdf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[4] &lt;strong&gt;Shifting Careers to Autonomous Vehicles:&lt;/strong&gt; https://towardsdatascience.com/how-i-found-my-current-job-3fb22e511a1f&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[5] &lt;strong&gt;Workra çš„å®˜æ–¹æŠ¥å‘Š:&lt;/strong&gt; https://workera.ai/candidates/report&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[6] &lt;strong&gt;Tokenizers GitHub åº“:&lt;/strong&gt; https://github.com/huggingface/tokenizers&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[7] &lt;strong&gt;TextVectorization å±‚:&lt;/strong&gt; https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[8] &lt;strong&gt;Chollet çš„ Colab ç¬”è®°:&lt;/strong&gt; https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[9] &lt;strong&gt;BERT:&lt;/strong&gt; https://arxiv.org/abs/1810.04805&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[10] &lt;strong&gt;åˆ©ç”¨ BERT æ¨¡å‹æ¥æ”¹å–„å’Œç†è§£æœç´¢:&lt;/strong&gt; https://www.blog.google/products/search/search-language-understanding-bert/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[11] &lt;strong&gt;Elasticsearch meets BERT:&lt;/strong&gt; https://towardsdatascience.com/elasticsearch-meets-bert-building-search-engine-with-elasticsearch-and-bert-9e74bf5b4cf2&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[12] &lt;strong&gt;Building a Search Engine with BERT and TensorFlow:&lt;/strong&gt; https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[13] &lt;strong&gt;TorchIO:&lt;/strong&gt; https://github.com/fepegar/torchio&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[14] &lt;strong&gt;PetFinder.my Contest: 1st Place Winner Disqualified:&lt;/strong&gt; https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/125436&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[15] &lt;strong&gt;Assessing Gender Bias in Machine Translation â€“ A Case Study with Google Translate:&lt;/strong&gt; https://arxiv.org/abs/1809.02208&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[16] &lt;strong&gt;Timnit Gebru:&lt;/strong&gt; https://twimlai.com/twiml-talk-336-trends-in-fairness-and-ai-ethics-with-timnit-gebru/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[17] &lt;strong&gt;Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning:&lt;/strong&gt; https://arxiv.org/abs/1912.10389&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[18] &lt;strong&gt;fairness-non-ideal-fazelpour-lipton-2020:&lt;/strong&gt; http://zacklipton.com/media/papers/fairness-non-ideal-fazelpour-lipton-2020.pdf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[19] &lt;strong&gt;NLP ä¸è¶³çš„é¢†åŸŸ:&lt;/strong&gt; https://thegradient.pub/nlps-clever-hans-moment-has-arrived/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[20] &lt;strong&gt;Probing Neural Network Comprehension of Natural Language Arguments:&lt;/strong&gt; https://www.aclweb.org/anthology/P19-1459/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[21] &lt;strong&gt;NLP Year in Review â€” 2019:&lt;/strong&gt; https://medium.com/dair-ai/nlp-year-in-review-2019-fb8d523bcb19&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[22] &lt;strong&gt;ç²¾å½©è€Œè¯¦å°½çš„åšå®¢æ–‡ç« :&lt;/strong&gt; https://ruder.io/research-highlights-2019/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[23] &lt;strong&gt;Google Research: Looking Back at 2019, and Forward to 2020 and Beyond:&lt;/strong&gt; https://ai.googleblog.com/2020/01/google-research-looking-back-at-2019.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[24] &lt;strong&gt;Elements of AI:&lt;/strong&gt; https://www.elementsofai.com/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[25] &lt;strong&gt;Natural Language Processing with Deep Learning:&lt;/strong&gt; http://web.stanford.edu/class/cs224n/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[26] &lt;strong&gt;NLP å’Œ ML é¢†åŸŸä¹¦ç±æ¨è:&lt;/strong&gt; https://twitter.com/omarsar0/status/1214547402838986754?s=20&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[27] &lt;strong&gt;Kernel Methods:&lt;/strong&gt; http://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/slides/master2017/master2017.pdf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[28] &lt;strong&gt;Are all kernels cursed?:&lt;/strong&gt; https://francisbach.com/cursed-kernels/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[29] &lt;strong&gt;John Langford çš„åšå®¢:&lt;/strong&gt; https://hunch.net/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[30] &lt;strong&gt;è¿™ç¯‡æ–‡ç« :&lt;/strong&gt; https://opendatascience.com/xgboost-enhancement-over-gradient-boosting-machines/?utm_campaign=Learning%20Posts&amp;amp;utm_content=111061559&amp;amp;utm_medium=social&amp;amp;utm_source=twitter&amp;amp;hss_channel=tw-3018841323&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[31] &lt;strong&gt;Emmanuel Ameisen:&lt;/strong&gt; https://www.amazon.com/Building-Machine-Learning-Powered-Applications/dp/149204511X/&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP%E7%AE%80%E6%8A%A5_Tokenizers,_TensorFlow_2_1,_TextVectorization/&quot;&gt;NLPç®€æŠ¥ [CH]: Tokenizers, TensorFlow 2.1, TextVectorization, TorchIO, NLP Shortfalls,â€¦&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 20, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLPç®€æŠ¥ [CH]: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,â€¦]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP%E7%AE%80%E6%8A%A5_ISSUE_4_PyTorch3D,_DeepSpeed,_Turing-NLG/" />
  <id>https://dair.ai/NLPç®€æŠ¥_ISSUE_4_PyTorch3D,_DeepSpeed,_Turing-NLG</id>
  <published>2020-02-17T00:00:00-06:00</published>
  <updated>2020-02-17T00:00:00-06:00</updated>
  <author>
    <name>Elvis Saravia</name>
    <uri>https://dair.ai</uri>
    <email>ellfae@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*3vNKhz6K-oGQ8aLi3mo84Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;Thanks to&lt;/em&gt; &lt;a href=&quot;https://blog.csdn.net/Kaiyuan_sjtu&quot;&gt;&lt;em&gt;Kaiyuan&lt;/em&gt;&lt;/a&gt; &lt;em&gt;(WeChat: NewBeeNLP) for this great translation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
æ¬¢è¿æ¥åˆ° NLP æ—¶äº‹ç®€æŠ¥ï¼å…¨æ–‡è¾ƒé•¿ï¼Œå»ºè®®æ”¶è—ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å¦å¤–åŠ äº†ç›®å½•æ–¹ä¾¿ç›´æ¥ç´¢å¼•åˆ°è‡ªå·±æ„Ÿå…´è¶£çš„éƒ¨åˆ†ã€‚enjoy&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;1ã€Publications ğŸ“™&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;1.1 Turing-NLG:&lt;/li&gt;
      &lt;li&gt;1.2 Neural based Dependency Parsing&lt;/li&gt;
      &lt;li&gt;1.3 End-to-end Cloud-based Information Extraction with BERT&lt;/li&gt;
      &lt;li&gt;1.4 Question Answering Benchmark&lt;/li&gt;
      &lt;li&gt;1.5 Radioactive data: tracing through training&lt;/li&gt;
      &lt;li&gt;1.6 REALM: Retrieval-Augmented Language Model Pre-Training&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;2ã€Creativity and Society ğŸ¨&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;2.1 å…è®¸åœ¨ç§‘å­¦ä¼šè®®ä¸Šè¿›è¡Œè¿œç¨‹è®ºæ–‡å’Œæµ·æŠ¥å±•ç¤º&lt;/li&gt;
      &lt;li&gt;2.2 Abstraction and Reasoning Challenge&lt;/li&gt;
      &lt;li&gt;2.3 ML and NLP Publications in 2019&lt;/li&gt;
      &lt;li&gt;2.4 Growing Neural Cellular Automata&lt;/li&gt;
      &lt;li&gt;2.5 Transformer attentionå¯è§†åŒ–&lt;/li&gt;
      &lt;li&gt;2.6 SketchTransfer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;3ã€Tools and Datasets âš™ï¸&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;3.1 DeepSpeed + ZeRO&lt;/li&gt;
      &lt;li&gt;3.2 ä¸€ä¸ªç”¨äºè¿›è¡Œå¿«é€Ÿæœ‰æ•ˆçš„3Dæ·±åº¦å­¦ä¹ ç ”ç©¶çš„åº“&lt;/li&gt;
      &lt;li&gt;3.3 ç®¡ç†ä½ çš„MLé¡¹ç›®é…ç½®&lt;/li&gt;
      &lt;li&gt;3.4 è´å¶æ–¯ç½‘ç»œå› æœæ¨ç†å·¥å…·åŒ…&lt;/li&gt;
      &lt;li&gt;3.5 TyDié—®ç­”ï¼šå¤šè¯­è¨€é—®ç­”åŸºå‡†&lt;/li&gt;
      &lt;li&gt;3.6 Question Answering for Node.js&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;4ã€Ethics in AI ğŸš¨&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;4.1 è¯†åˆ«æ–‡æœ¬ä¸­çš„subjective bias&lt;/li&gt;
      &lt;li&gt;4.2 Artificial Intelligence, Values and Alignment&lt;/li&gt;
      &lt;li&gt;4.3 å…³äºå®¡æ ¸AIç³»ç»Ÿ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;5ã€Articles and Blog posts âœï¸&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;5.1 ç”¨äºNLPç³»ç»Ÿçš„æ¨¡å‹è’¸é¦&lt;/li&gt;
      &lt;li&gt;5.2 å•è¯çš„ä¸Šä¸‹æ–‡è¡¨ç¤º&lt;/li&gt;
      &lt;li&gt;5.3 ç¥ç»ç½‘ç»œä¸­çš„ç¨€ç–æ€§&lt;/li&gt;
      &lt;li&gt;5.4 è®­ç»ƒä½ è‡ªå·±çš„è¯­è¨€æ¨¡å‹&lt;/li&gt;
      &lt;li&gt;5.5 åˆ†è¯å™¨Tokenizer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;6ã€Education ğŸ“&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;6.1 é˜¿å§†æ–¯ç‰¹ä¸¹è‡ªç”±å¤§å­¦æœºå™¨å­¦ä¹ è¯¾ç¨‹&lt;/li&gt;
      &lt;li&gt;6.2 æœºå™¨å­¦ä¹ æ•°å­¦èµ„æº&lt;/li&gt;
      &lt;li&gt;6.3 æ·±åº¦å­¦ä¹ å…¥é—¨&lt;/li&gt;
      &lt;li&gt;6.4 Pytorchæ·±åº¦å­¦ä¹ &lt;/li&gt;
      &lt;li&gt;6.5 Missing Semester of Your CS&lt;/li&gt;
      &lt;li&gt;6.6 æ·±åº¦å­¦ä¹ è¿›é˜¶&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;7ã€Noteworthy Mentions â­ï¸&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;1publications-&quot;&gt;1ã€Publications ğŸ“™&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;1.1 Turing-NLG: A 17-billion-parameter language model by Microsoft&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å›¾çµè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆT-NLGï¼‰[1]æ˜¯ç”± Microsoft AI ç ”ç©¶äººå‘˜æå‡ºçš„ 170 äº¿å‚æ•°è¯­è¨€æ¨¡å‹ã€‚é™¤äº†æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å·²çŸ¥è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ä¸‹å›¾æ‰€ç¤ºï¼‰ä¹‹å¤–ï¼ŒT-NLG æ˜¯åŸºäº 78 å±‚ Transformer çš„è¯­è¨€æ¨¡å‹ï¼Œå…¶åœ¨ WikiText-103 ä¸Šçš„å›°æƒ‘åº¦æ€§èƒ½ä¼˜äºä¹‹å‰çš„æœ€æ–°æŠ€æœ¯æˆæœï¼ˆç”±NVIDIA Megatron-LM[2]æŒæœ‰ï¼‰ ã€‚T-NLG åœ¨å„ç§ä»»åŠ¡ï¼ˆä¾‹å¦‚é—®é¢˜å›ç­”å’ŒæŠ½è±¡æ‘˜è¦ï¼‰ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼ŒåŒæ—¶åˆ†åˆ«æ˜¾ç¤ºäº†æ¨¡å‹çš„å¥½å¤„ï¼Œä¾‹å¦‚é›¶ç®€çŸ­é—®é¢˜åŠŸèƒ½å’Œæœ€å°åŒ–ç›‘ç£ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å¾—ç›Šäº DeepSpeed åº“ï¼ˆä¸ PyTorch å…¼å®¹ï¼‰å’Œ ZeRO ä¼˜åŒ–å™¨ï¼Œè¿™ä¸¤è€…ä¹Ÿä¼šåœ¨æœ¬æœŸç®€æŠ¥ä¸­å…·ä½“ä»‹ç»ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*CAZm7uj8EaupnvnJ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;è¯­è¨€æ¨¡å‹å‚æ•°å¤§å°&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.2 Neural based Dependency Parsing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Miryam de Lhoneux å…¬å¸ƒäº†å…¶åšå£«å­¦ä½è®ºæ–‡â€œLinguistically Informed Neural Dependency Parsing for Typologically Diverse Languages[3]â€ã€‚è¿™é¡¹å·¥ä½œæ˜¯å…³äºä½¿ç”¨ç¥ç»å­¦æ–¹æ³•ä»¥ç±»å‹å¤šæ ·çš„è¯­è¨€ï¼ˆå³ä»¥ç»“æ„ä¸Šä¸åŒçš„æ–¹å¼æ„é€ å’Œè¡¨è¾¾å«ä¹‰çš„è¯­è¨€ï¼‰è¿›è¡Œä¾èµ–å…³ç³»è§£æ[4]ã€‚è®ºæ–‡æŒ‡å‡º RNN å’Œé€’å½’å±‚å¯èƒ½æœ‰åŠ©äºåˆå¹¶åˆ°è§£æå™¨ä¸­ï¼Œå› ä¸ºå®ƒä»¬æœ‰åŠ©äºå‘ŠçŸ¥å…·æœ‰è§£ææ‰€éœ€çš„é‡è¦è¯­è¨€çŸ¥è¯†çš„æ¨¡å‹ã€‚æ›´å¤š ideas åŒ…æ‹¬ä½¿ç”¨å¤šè¯­è¨€è§£æå’Œå‚æ•°å…±äº«ç­–ç•¥æ¥è§£æç›¸å…³å’Œä¸ç›¸å…³è¯­è¨€ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.3 End-to-end Cloud-based Information Extraction with BERT&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
ç ”ç©¶äººå‘˜å‘è¡¨äº†ä¸€ç¯‡è®ºæ–‡â€Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documentsâ€[5]ï¼Œæè¿°äº† BERT ç­‰ Transformer æ¨¡å‹å¦‚ä½•å¸®åŠ©ç‰¹å®šé¢†åŸŸçš„ä¸šåŠ¡æ–‡æ¡£ï¼ˆä¾‹å¦‚ç›‘ç®¡æ–‡ä»¶å’Œç‰©ä¸šç§Ÿèµåè®®ï¼‰ä¸­çš„ç«¯åˆ°ç«¯ä¿¡æ¯æå–ã€‚è¿™ç§ç±»å‹çš„å·¥ä½œä¸ä»…å¯ä»¥å¸®åŠ©ä¼˜åŒ–ä¸šåŠ¡è¿è¥ï¼Œè€Œä¸”è¿˜æ˜¾ç¤ºäº†åŸºäº BERT çš„æ¨¡å‹åœ¨å¸¦æ³¨é‡Šæ•°æ®å¾ˆå°‘çš„æƒ…å†µä¸‹çš„é€‚ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚åŒæ—¶æå‡ºå¹¶è®¨è®ºäº†åœ¨äº‘ä¸Šè¿è¡Œçš„åº”ç”¨ç¨‹åºåŠå…¶å®ç°ç»†èŠ‚ï¼ˆè¯·å‚è§ä¸‹å›¾ï¼‰ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*KqViSLhP0otleDY-XFy3Bg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.4 Question Answering Benchmark&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Wolfson ç­‰å‘å¸ƒäº†ä¸€ä¸ªquestion understanding benchmark[6]ï¼Œä»¥åŠä¸€ç§ç”¨äºåˆ†è§£è®¡ç®—é€‚å½“ç­”æ¡ˆæ‰€å¿…éœ€çš„é—®é¢˜çš„æ–¹æ³•ã€‚ä»–ä»¬åˆ©ç”¨ä¼—åŒ…æ¥æ³¨é‡Šåˆ†è§£é—®é¢˜æ‰€éœ€çš„å¿…è¦æ­¥éª¤ï¼Œ ä¸ºäº†å±•ç¤ºè¯¥æ–¹æ³•çš„å¯è¡Œæ€§å’Œé€‚ç”¨æ€§ï¼Œä»–ä»¬æ”¹è¿›äº†ä½¿ç”¨ HotPotQA æ•°æ®é›†çš„å¼€æ”¾åŸŸé—®ç­”ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*AztG-Inqt6LGQ87lSufRcw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;ä¸åŒæ¥æºçš„é—®é¢˜å…·æœ‰ç›¸ä¼¼çš„ç»„æˆç»“æ„ã€‚&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.5 Radioactive data: tracing through training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Facebook AI ç ”ç©¶äººå‘˜æœ€è¿‘å‘è¡¨äº†ä¸€é¡¹æœ‰è¶£çš„å·¥ä½œ[7]ï¼Œæ—¨åœ¨æ ‡è®°å›¾åƒï¼ˆç§°ä¸º&lt;strong&gt;ã€Œradioactive dataã€&lt;/strong&gt;ï¼‰ï¼Œä»¥éªŒè¯è¯¥ç‰¹å®šæ•°æ®é›†æ˜¯å¦ç”¨äºè®­ç»ƒ ML æ¨¡å‹ã€‚ä»–ä»¬å‘ç°ï¼Œå¯ä»¥ä½¿ç”¨å·§å¦™çš„æ ‡è®°å°†ç‰¹å¾ç§»å‘æŸä¸ªæ–¹å‘ï¼Œå³ä½¿åªæœ‰ 1ï¼…çš„è®­ç»ƒæ•°æ®æ˜¯ radioactivï¼Œæ¨¡å‹ä¹Ÿå¯ä»¥ä½¿ç”¨è¯¥æ ‡è®°å¸®åŠ©æ£€æµ‹â€˜radioactive dataâ€™çš„ä½¿ç”¨æƒ…å†µã€‚è¿™æå…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ•°æ®ä¸­çš„ä»»ä½•æ›´æ”¹éƒ½å¯èƒ½ä¼šé™ä½æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ä½œè€…è¯´ï¼Œè¿™é¡¹å·¥ä½œå¯ä»¥â€œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆè·Ÿè¸ªç”¨äºè®­ç»ƒæ¨¡å‹çš„æ•°æ®é›†ï¼Œä»¥ä¾¿ä»–ä»¬å¯ä»¥æ›´å¥½åœ°äº†è§£å„ç§æ•°æ®é›†å¦‚ä½•å½±å“ä¸åŒç¥ç»ç½‘ç»œçš„æ€§èƒ½â€ï¼Œåœ¨å…³é”®ä»»åŠ¡ ML åº”ç”¨ç¨‹åºä¸­ï¼Œè¿™ä¼¼ä¹æ˜¯ä¸€ç§é‡è¦çš„æ–¹æ³•ã€‚å¦‚æœæ„Ÿå…´è¶£å¯ä»¥æŸ¥çœ‹å®Œæ•´è®ºæ–‡[8]äº†è§£å…·ä½“ä¿¡æ¯ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.6 REALM: Retrieval-Augmented Language Model Pre-Training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
REALM [9] æ˜¯åŸºäºç¥ç»çš„å¤§è§„æ¨¡æ£€ç´¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ–‡æœ¬çŸ¥è¯†çš„è¯­æ–™åº“ä»¥æ— ç›‘ç£çš„æ–¹å¼é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚è¯¥æ–¹æ³•çš„ä¸»è¦ç›®çš„æ˜¯ä»¥ä¸€ç§æ›´å¯è§£é‡Šçš„æ–¹å¼æ•è·çŸ¥è¯†ï¼Œå…·ä½“è€Œè¨€æ˜¯åˆ©ç”¨ä¸–ç•ŒçŸ¥è¯†å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹ã€‚ä½¿ç”¨ REALM å¤„ç†å’Œè¯„ä¼°çš„ä»»åŠ¡åŒ…æ‹¬å¼€æ”¾åŸŸé—®ç­”åŸºå‡†ï¼Œ é™¤äº†æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å¤–ï¼Œå…¶ä»–å¥½å¤„è¿˜åŒ…æ‹¬æ¨¡å—åŒ–å’Œå¯è§£é‡Šæ€§ç»„ä»¶ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*MJO-yzCwsB5ydKGz7hKHVA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2creativity-and-society-&quot;&gt;2ã€Creativity and Society ğŸ¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;2.1 å…è®¸åœ¨ç§‘å­¦ä¼šè®®ä¸Šè¿›è¡Œè¿œç¨‹è®ºæ–‡å’Œæµ·æŠ¥å±•ç¤º&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
è¿‡å»ä¸€å‘¨æœ‰è¯·æ„¿ä¹¦æ•£å‘ï¼Œä»¥ä¾¿åœ¨ä¸ ML ç›¸å…³çš„ç§‘å­¦ä¼šè®®ä¸Šè¿›è¡Œè¿œç¨‹è®ºæ–‡å’Œæµ·æŠ¥å±•ç¤ºï¼Œå¯ä»¥åœ¨change.org[10]ä¸Šé˜…è¯»æœ‰å…³å®ƒçš„æ›´å¤šä¿¡æ¯ã€‚æ·±åº¦å­¦ä¹ çš„å…ˆé©± Yoshua Bengio ä¼¼ä¹åœ¨å€¡å¯¼äººä»¬å»ç­¾ç½²è¯·æ„¿ä¹¦ï¼Œ å¹¶åœ¨ä»–çš„æ–°åšå®¢[11]ä¸­é˜æ˜äº†è¿™ä¸€ç‚¹ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;2.2 Abstraction and Reasoning Challenge&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
FranÃ§oisChollet æœ€è¿‘å‘å¸ƒäº†ä¸€ä¸ª Kaggle ç«èµ›ï¼Œä»–å‘å¸ƒäº†æŠ½è±¡æ¨ç†è¯­æ–™åº“ï¼ˆARCï¼‰[12]ï¼Œæ—¨åœ¨é¼“åŠ±ç”¨æˆ·åˆ›å»ºå¯ä»¥è§£å†³ä»æœªæ¥è§¦è¿‡çš„æ¨ç†ä»»åŠ¡çš„ AI ç³»ç»Ÿã€‚å¸Œæœ›èƒ½å¤Ÿå¼€å§‹æ„å»ºæ›´å¼ºå¤§çš„ AI ç³»ç»Ÿï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½ï¼Œå¿«é€Ÿåœ°è‡ªè¡Œè§£å†³æ–°é—®é¢˜ï¼Œè¿™å¯èƒ½æœ‰åŠ©äºè§£å†³æ›´å…·æŒ‘æˆ˜æ€§çš„ç°å®åº”ç”¨ï¼Œä¾‹å¦‚æ”¹å–„åœ¨æç«¯å’Œå¤šæ ·åŒ–ç¯å¢ƒä¸­è¿è¡Œçš„è‡ªåŠ¨é©¾é©¶æ±½è½¦ ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;2.3 ML and NLP Publications in 2019&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Marek Rei å‘å¸ƒæœºå™¨å­¦ä¹ å’Œ NLP é¢†åŸŸ 2019 å¹´å‡ºç‰ˆæ•°æ®ç»Ÿè®¡[13]ï¼Œåˆ†æä¸­åŒ…æ‹¬çš„ä¼šè®®æ˜¯ ACLï¼ŒEMNLPï¼ŒNAACLï¼ŒEACLï¼ŒCOLINGï¼ŒTACLï¼ŒCLï¼ŒCoNLLï¼ŒNeurIPSï¼ŒICMLï¼ŒICLR å’Œ AAAIã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;2.4 Growing Neural Cellular Automata&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Morphogenesis&lt;/code&gt;æ˜¯ä¸€ä¸ªè‡ªç»„ç»‡è¿‡ç¨‹ï¼Œé€šè¿‡è¯¥è¿‡ç¨‹ï¼Œè¾èˆç­‰æŸäº›ç”Ÿç‰©å¯ä»¥å†ç”Ÿæˆ–ä¿®å¤èº«ä½“æŸä¼¤ã€‚è¯¥è¿‡ç¨‹å¯¹äºæ‰°åŠ¨æ˜¯é²æ£’çš„ï¼Œå¹¶ä¸”æœ¬è´¨ä¸Šæ˜¯è‡ªé€‚åº”çš„ã€‚å—è¿™ç§ç”Ÿç‰©å­¦ç°è±¡çš„å¯å‘ï¼Œéœ€è¦æ›´å¥½åœ°äº†è§£è¿™ä¸€è¿‡ç¨‹ï¼Œç ”ç©¶äººå‘˜å‘è¡¨äº†ä¸€ç¯‡é¢˜ä¸ºâ€Growing Neural Cellular Automataâ€[14]çš„è®ºæ–‡ï¼Œè¯¥è®ºæ–‡é‡‡ç”¨äº†å¯åŒºåˆ†çš„å½¢æ€å‘ç”Ÿæ¨¡å‹ï¼Œæ—¨åœ¨å¤åˆ¶è‡ªæˆ‘ä¿®å¤ç³»ç»Ÿçš„è¡Œä¸ºå’Œç‰¹æ€§ï¼Œå¹¶å¸Œæœ›èƒ½å¤Ÿåˆ¶é€ å‡ºå…·æœ‰ä¸ç”Ÿç‰©ç”Ÿå‘½ç›¸åŒçš„åšå›ºæ€§å’Œå¯å¡‘æ€§çš„è‡ªæˆ‘ä¿®å¤ machinesã€‚æ­¤å¤–ï¼Œè¿™å°†æœ‰åŠ©äºæ›´å¥½åœ°äº†è§£å†ç”Ÿè¿‡ç¨‹æœ¬èº«ï¼Œ è®¸å¤šåº”ç”¨æ³¨å…¥å†ç”ŸåŒ»å­¦ä»¥åŠç¤¾ä¼šå’Œç”Ÿç‰©ç³»ç»Ÿçš„å»ºæ¨¡ç­‰éƒ½å°†å—ç›Šã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2p62h1RaHD6d11LX8olnTA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;2.5 Transformer attention å¯è§†åŒ–&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hendrik Strobelt å¼€æºé¡¹ç›®æ˜¾ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Hugging Face åº“å’Œ d3.js é€šè¿‡ Web åº”ç”¨ç¨‹åºå¿«é€Ÿæ„å»ºç®€å•çš„äº¤äº’å¼ Transformer æ³¨æ„å¯è§†åŒ–[15]ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*lMaZGDRJUI1Qcv7T5AdhlQ.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;2.6 SketchTransfer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
SketchTransfer[16]æå‡ºäº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼Œä»¥æµ‹è¯•æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å­˜åœ¨/ä¸å­˜åœ¨ç»†èŠ‚æ—¶æ”¯æŒä¸å˜æ€§çš„èƒ½åŠ›ã€‚é•¿æœŸä»¥æ¥ä¸€ç›´äº‰è®ºçš„æ˜¯ï¼Œæ·±åº¦ç½‘ç»œæ— æ³•æ¨å¹¿åˆ°è®­ç»ƒæœŸé—´å°šæœªå‘ç°çš„å˜åŒ–ï¼Œäººç±»å¯ä»¥ç›¸å¯¹è½»æ¾åœ°å®ŒæˆæŸäº›äº‹æƒ…ï¼Œä¾‹å¦‚åœ¨è§‚çœ‹åŠ¨ç”»ç‰‡æ—¶å¤„ç†ä¸¢å¤±çš„è§†è§‰ç»†èŠ‚ã€‚æœ¬æ–‡è®¨è®ºå¹¶å‘å¸ƒäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œä»¥é€šè¿‡æä¾›æœªæ ‡è®°çš„è‰å›¾å›¾åƒå’Œæ ‡è®°çš„çœŸå®å›¾åƒç¤ºä¾‹æ¥å¸®åŠ©ç ”ç©¶äººå‘˜ä»”ç»†ç ”ç©¶â€œç»†èŠ‚ä¸å˜æ€§â€é—®é¢˜ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*jdYuMoHiu2yya5rHzZyjwQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3tools-and-datasets-ï¸&quot;&gt;3ã€Tools and Datasets âš™ï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;3.1 DeepSpeed + ZeRO&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Microsoft å¼€æºäº†ä¸€ä¸ªç§°ä¸º DeepSpeed çš„è®­ç»ƒä¼˜åŒ–åº“ï¼Œè¯¥åº“ä¸ PyTorch å…¼å®¹ï¼Œå¹¶ä¸”å¯ä»¥è®­ç»ƒ 1000 äº¿ä¸ªå‚æ•°çš„æ¨¡å‹ã€‚è¯¥åº“ä¾§é‡äºè®­ç»ƒæ¨¡å‹çš„å››ä¸ªé‡è¦æ–¹é¢ï¼šè§„æ¨¡ï¼Œé€Ÿåº¦ï¼Œæˆæœ¬å’Œå¯ç”¨æ€§ã€‚DeepSpeed ä¸ ZeRO[18]ä¸€èµ·å‘å¸ƒï¼ŒZeRO æ˜¯ä¸€ç§å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œå¯åœ¨å½“å‰ GPU æŠ€æœ¯ä¸­å®ç°å¤§è§„æ¨¡åˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ ï¼ŒåŒæ—¶å°†ååé‡æé«˜åˆ°å½“å‰æœ€ä½³ç³»ç»Ÿçš„ä¸‰åˆ°äº”å€ã€‚ZeRO å…è®¸è®­ç»ƒä»»æ„å¤§å°çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥æ ¹æ®å…±äº«æ¨¡å‹çŠ¶æ€é€‚åˆèšåˆçš„å¯ç”¨å†…å­˜ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*MXDI1f3cSBrY5w2g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.2 ä¸€ä¸ªç”¨äºè¿›è¡Œå¿«é€Ÿæœ‰æ•ˆçš„ 3D æ·±åº¦å­¦ä¹ ç ”ç©¶çš„åº“&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
PyTorch3D[19]æ˜¯ç”¨äºåŸºäº 3D çš„æ·±åº¦å­¦ä¹ ç ”ç©¶çš„å¼€æºå·¥å…·åŒ…ã€‚è¿™ä¸ª PyTorch åº“æ—¨åœ¨å¸®åŠ©æ·±åº¦å­¦ä¹ ç³»ç»Ÿä¸­çš„ 3D æ•°æ®æ”¯æŒå’Œç†è§£ï¼Œè¯¥åº“ç”±å¸¸ç”¨ 3D è¿ç®—ç¬¦å’ŒæŸå¤±å‡½æ•°çš„å¿«é€Ÿä¼˜åŒ–å®ç°ç»„æˆï¼Œ å®ƒè¿˜å¸¦æœ‰æ¨¡å—åŒ–çš„å¯åŒºåˆ†æ¸²æŸ“å™¨ï¼Œå¯å¸®åŠ©è¿›è¡Œå¤æ‚ 3D è¾“å…¥çš„ç ”ç©¶å¹¶æ”¯æŒé«˜è´¨é‡ 3D é¢„æµ‹ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*VbspKMmPBUsgpdnIkd5jYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.3 ç®¡ç†ä½ çš„ ML é¡¹ç›®é…ç½®&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hydra[20]æ˜¯åŸºäº Python çš„é…ç½®å·¥å…·ï¼Œç”¨äºæ›´æœ‰æ•ˆåœ°ç®¡ç†å¤æ‚çš„ ML é¡¹ç›®ã€‚å®ƒæ—¨åœ¨é€šè¿‡ä¸º ML é¡¹ç›®æä¾›åŠŸèƒ½çš„é…ç½®é‡ç”¨æ¥å¸®åŠ© PyTorch ç ”ç©¶äººå‘˜ã€‚å®ƒæä¾›çš„ä¸»è¦å¥½å¤„æ˜¯å®ƒå…è®¸ç¨‹åºå‘˜åƒç¼–å†™ä»£ç ä¸€æ ·ç¼–å†™é…ç½®ï¼Œè¿™æ„å‘³ç€å¯ä»¥è½»æ¾åœ°è¦†ç›–é…ç½®æ–‡ä»¶ã€‚Hydra è¿˜å¯ä»¥å¸®åŠ©è‡ªåŠ¨ç®¡ç† ML é¡¹ç›®è¾“å‡ºçš„å·¥ä½œç›®å½•ï¼Œè¿™åœ¨éœ€è¦ä¿å­˜å’Œè®¿é—®å¤šä¸ªä½œä¸šçš„å¤šä¸ªå®éªŒç»“æœæ—¶éå¸¸æœ‰ç”¨ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.4 è´å¶æ–¯ç½‘ç»œå› æœæ¨ç†å·¥å…·åŒ…&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
CausalNex[21]æ˜¯ç”¨äºâ€œä¸è´å¶æ–¯ç½‘ç»œè¿›è¡Œå› æœæ¨ç†â€çš„å·¥å…·åŒ…ã€‚è¯¥å·¥å…·æ—¨åœ¨å°†æœºå™¨å­¦ä¹ å’Œå› æœæ¨ç†ç›¸ç»“åˆï¼Œä»¥å‘ç°æ•°æ®ä¸­çš„ç»“æ„å…³ç³»ã€‚ä½œè€…è¿˜å‡†å¤‡äº†å…³äºä½¿ç”¨ Python åº“æ¨æ–­è´å¶æ–¯ç½‘ç»œåŸå› å’ŒåŸå› çš„å…¥é—¨æŒ‡å—ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*EYwKhdnscR7ZLuNkTqCS2Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.5 TyDi é—®ç­”ï¼šå¤šè¯­è¨€é—®ç­”åŸºå‡†&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI å‘å¸ƒäº†TyDi QA[22]ï¼Œå®ƒæ˜¯ä¸€ç§å¤šè¯­è¨€çš„æ•°æ®é›†ï¼Œå¯ä»¥é¼“åŠ±ç ”ç©¶äººå‘˜å¯¹æ›´å¤šç±»å‹å¤šæ ·çš„è¯­è¨€è¿›è¡Œé—®ç­”ï¼Œè¿™äº›è¯­è¨€ä»¥ä¸åŒçš„æ–¹å¼æ„å»ºå’Œè¡¨è¾¾å«ä¹‰ã€‚è¿™æ ·åšçš„ç›®çš„æ˜¯æ¿€åŠ±ç ”ç©¶äººå‘˜åœ¨ç±»å‹ä¸Šè·ç¦»é¥è¿œçš„è¯­è¨€ï¼ˆä¾‹å¦‚é˜¿æ‹‰ä¼¯è¯­ï¼Œå­ŸåŠ æ‹‰è¯­ï¼ŒéŸ©è¯­ï¼Œä¿„è¯­ï¼Œæ³°å¢å›ºè¯­å’Œæ³°è¯­ï¼‰ä¸Šå»ºç«‹æ›´å¼ºå¤§çš„æ¨¡å‹ï¼Œä»¥ä¾¿å°†å…¶æ¨å¹¿åˆ°æ›´å¤šç§è¯­è¨€ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*1dZv5you3jigdrQ2uAKzUw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.6 Question Answering for Node.js&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hugging Face å‘å¸ƒäº†åŸºäº DistilBERT çš„é—®ç­”åº“[23]ï¼Œå¹¶ç»§ç»­ä½¿ NLP æ›´åŠ æ˜“äºè®¿é—®ã€‚è¯¥æ¨¡å‹å¯ä»¥ä½¿ç”¨ Node.js åœ¨ç”Ÿäº§ä¸­è¿è¡Œï¼Œåªéœ€ 3 è¡Œä»£ç ã€‚è¯¥æ¨¡å‹åˆ©ç”¨äº†ç”± Hugging Face å’Œ TensorFlow.jsï¼ˆç”¨äºå°†æœºå™¨å­¦ä¹ æ¨¡å‹ä¸ Javascript ç»“åˆä½¿ç”¨çš„æµè¡Œåº“ï¼‰æ„å»ºçš„ Tokenizer çš„å¿«é€Ÿå®ç°ã€‚&lt;/p&gt;

&lt;h1 id=&quot;4ethics-in-ai-&quot;&gt;4ã€Ethics in AI ğŸš¨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;4.1 è¯†åˆ«æ–‡æœ¬ä¸­çš„ subjective bias&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
è®¡ç®—ç¤¾ä¼šç§‘å­¦çš„ç ”ç©¶å‘˜ Diyi Yang è®¨è®ºäº† AI ç³»ç»Ÿå¦‚ä½•å¸®åŠ©è¯†åˆ«æ–‡æœ¬ä¿¡æ¯ä¸­çš„ä¸»è§‚åè§[25]ã€‚è¿™æ˜¯æ¶‰åŠ AI ç³»ç»Ÿå’Œ NLP çš„é‡è¦ç ”ç©¶é¢†åŸŸï¼Œå°¤å…¶æ˜¯å½“æˆ‘ä»¬è®¨è®ºè¯¸å¦‚æ–°é—»æ ‡é¢˜ä¹‹ç±»çš„æ–‡æœ¬åª’ä½“çš„æ¶ˆè´¹æ—¶ï¼Œå¾ˆå®¹æ˜“å°†å…¶æ„æ¶æˆåå‘æ¶ˆè´¹è€…ï¼Œè€Œå®é™…ä¸Šä»–ä»¬åº”è¯¥è¿½æ±‚æ›´åŠ å®¢è§‚ã€‚ä»åº”ç”¨ç¨‹åºçš„è§’åº¦æ¥çœ‹ï¼Œè‡ªåŠ¨è¯†åˆ«æ–‡æœ¬åª’ä½“ä¸­å­˜åœ¨çš„ä¸»è§‚åè§ä»¥å¸®åŠ©æ¶ˆè´¹è€…æ›´åŠ äº†è§£ä»–ä»¬æ­£åœ¨æ¶ˆè´¹çš„å†…å®¹å˜å¾—è‡³å…³é‡è¦ã€‚æ­¤å¤–è¿˜è®¨è®ºäº† AI å¦‚ä½•ä¹Ÿå¯ä»¥ä¿æŒåè§ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;4.2 Artificial Intelligence, Values and Alignment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å…´èµ·ä»¥åŠå®ƒä»¬å¦‚ä½•ä¸äººç±»ä»·å€¼è§‚ä¿æŒä¸€è‡´æ˜¯æ¶‰åŠäººå·¥æ™ºèƒ½ç³»ç»Ÿä¼¦ç†å­¦çš„æ´»è·ƒç ”ç©¶é¢†åŸŸã€‚DeepMind[26]æœ€è¿‘å‘å¸ƒäº†ä¸€ç¯‡è®ºæ–‡ï¼Œæ·±å…¥æ¢è®¨äº†å›´ç»• AI å¯¹é½çš„å“²å­¦é—®é¢˜ã€‚è¯¥æŠ¥å‘Šé‡ç‚¹è®¨è®ºäº†ä¸¤ä¸ªéƒ¨åˆ†ï¼Œå³æŠ€æœ¯éƒ¨åˆ†ï¼ˆå³å¦‚ä½•å¯¹ä» AI ä»£ç†è·å¾—å¯é ç»“æœçš„å€¼è¿›è¡Œç¼–ç ï¼‰å’Œè§„èŒƒæ€§ï¼ˆåœ¨ AI ä¸­è¿›è¡Œç¼–ç çš„åŸåˆ™æ˜¯æ­£ç¡®çš„ï¼‰ä»¥åŠå®ƒä»¬ä¹‹é—´çš„è”ç³»ä»¥åŠå¯ä»¥ç¡®ä¿çš„éƒ¨åˆ†ã€‚æœ¬æ–‡ä¸»å¼ é‡‡ç”¨ä¸€ç§åŸºäºåŸåˆ™çš„ AI å¯¹é½æ–¹æ³•ï¼Œå¹¶åœ¨ä¿¡å¿µå’Œè§‚ç‚¹å­˜åœ¨å·®å¼‚çš„æƒ…å†µä¸‹ä¿æŒå…¬å¹³å¾…é‡ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;4.3 å…³äºå®¡æ ¸ AI ç³»ç»Ÿ&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
VentureBeat æŠ¥å‘Šç§°ï¼ŒGoogle ç ”ç©¶äººå‘˜ä¸å…¶ä»–å°ç»„åˆä½œåˆ›å»ºäº†ä¸€ä¸ªåä¸º SMACTR çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿å·¥ç¨‹å¸ˆå¯ä»¥å®¡æ ¸ AI ç³»ç»Ÿã€‚è¿›è¡Œè¿™é¡¹å·¥ä½œçš„åŸå› æ˜¯ä¸ºäº†è§£å†³ç›®å‰è¢«æ¶ˆè´¹è€…å¹¿æ³›ä½¿ç”¨ä»¥ä¾›ä½¿ç”¨çš„ AI ç³»ç»Ÿå­˜åœ¨çš„é—®è´£åˆ¶å·®è·ã€‚åœ¨è¿™é‡Œé˜…è¯»å®Œæ•´çš„æŠ¥å‘Š[27]ï¼Œåœ¨è¿™é‡Œé˜…è¯»å®Œæ•´çš„è®ºæ–‡[28]ã€‚&lt;/p&gt;

&lt;h1 id=&quot;5articles-and-blog-posts-ï¸&quot;&gt;5ã€Articles and Blog posts âœï¸&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;5.1 ç”¨äº NLP ç³»ç»Ÿçš„æ¨¡å‹è’¸é¦&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
åœ¨NLP Highlights[29]æ’­å®¢çš„æ–°å‰§é›†ä¸­ï¼ŒThomas Wolf å’Œ Victor Sanh è®¨è®ºäº†æ¨¡å‹è’¸é¦ï¼Œä»¥åŠå¦‚ä½•å°†å…¶ç”¨ä½œå‹ç¼©å¤§å‹æ¨¡å‹ï¼ˆå¦‚ BERTï¼‰ä»¥ç”¨äºå¯æ‰©å±•çš„å®é™… NLP åº”ç”¨ç¨‹åºçš„å¯è¡Œæ–¹æ³•ã€‚ä»–ä»¬åœ¨ä»–ä»¬æå‡ºçš„ç§°ä¸ºDistilBERT[30]çš„æ–¹æ³•ä¸­å¯¹æ­¤æ¦‚å¿µè¿›è¡Œäº†è¿›ä¸€æ­¥çš„è®¨è®ºï¼Œåœ¨è¯¥æ–¹æ³•ä¸­ï¼Œä»–ä»¬æ„å»ºè¾ƒå°çš„æ¨¡å‹ï¼ˆåŸºäºè¾ƒå¤§æ¨¡å‹çš„ç›¸åŒä½“ç³»ç»“æ„ï¼‰ï¼Œä»¥æ ¹æ®è¯¥æ¨¡å‹çš„è¾“å‡ºæ¥æ¨¡ä»¿è¾ƒå¤§æ¨¡å‹çš„è¡Œä¸ºã€‚æœ¬è´¨ä¸Šï¼Œè¾ƒå°çš„æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰ä¼šå°è¯•æ ¹æ®å…¶è¾“å‡ºåˆ†å¸ƒæ¥æ‹Ÿåˆæ•™å¸ˆçš„æ¦‚ç‡åˆ†å¸ƒã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;5.2 å•è¯çš„ä¸Šä¸‹æ–‡è¡¨ç¤º&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
æœ€è¿‘ï¼Œå…³äºè¯¸å¦‚ BERT çš„ä¸Šä¸‹æ–‡åŒ–æ–¹æ³•æˆåŠŸç”¨äºå¤„ç†å„ç§å¤æ‚çš„ NLP ä»»åŠ¡çš„è®¨è®ºå¾ˆå¤šã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼ŒKawin Ethayarajh è¯•å›¾å›ç­”ä»¥ä¸‹é—®é¢˜ï¼šè¯¸å¦‚ BERTï¼ŒELMo å’Œ GPT-2 ä¹‹ç±»çš„ä¸Šä¸‹æ–‡æ¨¡å‹åŠå…¶ä¸Šä¸‹æ–‡åŒ–çš„è¯è¡¨ç¤ºå½¢å¼æ˜¯ä»€ä¹ˆ[31]ï¼Ÿä¸»é¢˜åŒ…æ‹¬è¯­å¢ƒæ€§ï¼Œè¯­å¢ƒç‰¹å®šæ€§çš„åº¦é‡ä»¥åŠé™æ€åµŒå…¥ä¸è¯­å¢ƒåŒ–è¡¨ç¤ºä¹‹é—´çš„æ¯”è¾ƒã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*70aIv1Fkkz4rnHgQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;5.3 ç¥ç»ç½‘ç»œä¸­çš„ç¨€ç–æ€§&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
ML ç ”ç©¶äººå‘˜ FranÃ§oisLagunas å†™äº†è¿™ç¯‡å¾ˆæ£’çš„æ–‡ç« ï¼ŒIs the future of Neural Networks Sparse?[32] è®¨è®ºäº†ä»–å¯¹åœ¨ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­é‡‡ç”¨ç¨€ç–å¼ é‡çš„ä¹è§‚æ€åº¦ã€‚å¸Œæœ›é‡‡ç”¨æŸç§å½¢å¼çš„ç¨€ç–æ€§æ¥å‡å°å½“å‰æ¨¡å‹çš„å¤§å°ï¼Œè¿™äº›æ¨¡å‹ç”±äºå…¶å¤§å°å’Œé€Ÿåº¦è€Œåœ¨æŸäº›æ—¶å€™å˜å¾—ä¸åˆ‡å®é™…ã€‚ç”±äºå½“å‰æ¨¡å‹ï¼ˆä¾‹å¦‚ Transformerï¼‰çš„åºå¤§è§„æ¨¡ï¼ˆé€šå¸¸ä¾èµ–æ•°åäº¿ä¸ªå‚æ•°ï¼‰ï¼Œå› æ­¤åœ¨ ML ä¸­å¯èƒ½å€¼å¾—æ¢è®¨è¿™ä¸€æ¦‚å¿µã€‚ä½†æ˜¯ï¼Œä»å¼€å‘äººå‘˜å·¥å…·çš„è§’åº¦æ¥çœ‹ï¼Œåœ¨ GPU ä¸Šçš„ç¥ç»ç½‘ç»œä¸­æ”¯æŒæœ‰æ•ˆç¨€ç–æ€§çš„å®ç°ç»†èŠ‚å°šä¸æ¸…æ¥šï¼Œè¿™æ˜¯æœºå™¨å­¦ä¹ ç¤¾åŒºæ­£åœ¨åŠªåŠ›çš„äº‹æƒ…ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;5.4 è®­ç»ƒä½ è‡ªå·±çš„è¯­è¨€æ¨¡å‹&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å¦‚æœä½ æƒ³å­¦ä¹ å¦‚ä½•ä»é›¶å¼€å§‹è®­ç»ƒè¯­è¨€æ¨¡å‹[33]ï¼Œè¯·æŸ¥çœ‹ Hugging Face çš„è¿™ä»½ä»¤äººå°è±¡æ·±åˆ»ä¸”å…¨é¢çš„æ•™ç¨‹ã€‚ä»–ä»¬æ˜¾ç„¶åˆ©ç”¨äº†è‡ªå·±çš„åº“ Transformers å’Œ Tokenizers æ¥è®­ç»ƒæ¨¡å‹ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;5.5 åˆ†è¯å™¨ Tokenizer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cathal Horan å‘è¡¨äº†ä¸€ç¯‡ä»¤äººå°è±¡æ·±åˆ»ä¸”éå¸¸è¯¦ç»†çš„åšå®¢æ–‡ç« ï¼ŒTokenizers: How machines read[34]ï¼Œå†…å®¹æ¶‰åŠæœ€æ–°çš„ NLP æ¨¡å‹å¦‚ä½•ä»¥åŠä½¿ç”¨å“ªç§ç±»å‹çš„ tokenizer æ¥å¸®åŠ©æœºå™¨å­¦ä¹ ç®—æ³•ä»æ–‡æœ¬ä¿¡æ¯ä¸­å­¦ä¹ ã€‚ä»–è¿˜è®¨è®ºå¹¶æ¿€å‘äº†ä¸ºä»€ä¹ˆ tokenizer æ˜¯æ¿€åŠ¨äººå¿ƒä¸”é‡è¦çš„ç ”ç©¶æ´»è·ƒé¢†åŸŸçš„åŸå› ã€‚æ–‡ç« ç”šè‡³è¿˜å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ SentencePiece å’Œ WordPiece ç­‰æ ‡è®°åŒ–æ–¹æ³•æ¥è®­ç»ƒè‡ªå·±çš„æ ‡è®°åŒ–å·¥å…·ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Vkjw5n9Sz0Was43haVNJMg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;6education-&quot;&gt;6ã€Education ğŸ“&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;6.1 é˜¿å§†æ–¯ç‰¹ä¸¹è‡ªç”±å¤§å­¦æœºå™¨å­¦ä¹ è¯¾ç¨‹&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
ç°åœ¨ï¼Œä½ å¯ä»¥åœ¨çº¿å­¦ä¹  2020 MLVU æœºå™¨å­¦ä¹ è¯¾ç¨‹[35]ï¼Œå…¶ä¸­åŒ…æ‹¬å…¨å¥—å¹»ç¯ç‰‡ï¼Œè§†é¢‘å’Œæ•™å­¦å¤§çº²ã€‚å®ƒæ—¨åœ¨ä½œä¸º ML çš„å…¥é—¨ï¼Œä½†å®ƒä¹ŸåŒ…å«å…¶ä»–ä¸æ·±åº¦å­¦ä¹ ç›¸å…³çš„ä¸»é¢˜ï¼Œä¾‹å¦‚ VAE å’Œ GANã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.2 æœºå™¨å­¦ä¹ æ•°å­¦èµ„æº&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
SuzanaIliÄ‡ å’Œä¸œäº¬æœºå™¨å­¦ä¹ ï¼ˆMLTï¼‰åœ¨ä½¿ ML æ•™è‚²æ°‘ä¸»åŒ–æ–¹é¢ä¸€ç›´åšç€æƒŠäººçš„å·¥ä½œã€‚ä¾‹å¦‚ï¼Œè¯¥åº“Machine-Learning-Tokyo/Math_resources[36]å±•ç¤ºäº†å…è´¹çš„åœ¨çº¿èµ„æºé›†åˆï¼Œç”¨äºå­¦ä¹  ML ä¸­ä½¿ç”¨çš„æ•°å­¦æ¦‚å¿µçš„åŸºç¡€ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.3 æ·±åº¦å­¦ä¹ å…¥é—¨&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
MIT çš„â€œæ·±åº¦å­¦ä¹ å…¥é—¨â€è¯¾ç¨‹[37]ï¼Œ æ¯å‘¨éƒ½ä¼šå‘å¸ƒæ–°çš„è®²åº§ï¼Œæ‰€æœ‰æ–¹é¢å’Œè§†é¢‘ï¼ˆåŒ…æ‹¬ç¼–ç å®éªŒå®¤ï¼‰éƒ½å°†å‘å¸ƒã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.4 Pytorch æ·±åº¦å­¦ä¹ &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Alfredo Canziani å‘å¸ƒäº†PyTorch æ·±åº¦å­¦ä¹ [38]å¾®å‹è¯¾ç¨‹çš„å¹»ç¯ç‰‡å’Œç¬”è®°æœ¬ï¼Œè¯¥èµ„æºåº“è¿˜åŒ…å«ä¸€ä¸ªé…å¥—ç½‘ç«™ï¼Œå…¶ä¸­åŒ…å«å¯¹æœ¬è¯¾ç¨‹ä¸­æ‰€æ•™æˆæ¦‚å¿µçš„æ–‡å­—æè¿°ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.5 Missing Semester of Your CS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
ç”±éº»çœç†å·¥å­¦é™¢çš„æ•™å‘˜å‘å¸ƒçš„â€œMissing Semesterâ€[39]æ˜¯ä¸€é—¨å¾ˆæ£’çš„åœ¨çº¿è¯¾ç¨‹ï¼Œå…¶ä¸­åŒ…å«å¯¹éå¼€å‘èƒŒæ™¯çš„æ•°æ®ç§‘å­¦å®¶è€Œè¨€å¯èƒ½æœ‰ç”¨çš„ææ–™ã€‚å®ƒåŒ…æ‹¬è¯¸å¦‚ Shell å·¥å…·å’Œè„šæœ¬ä»¥åŠç‰ˆæœ¬æ§åˆ¶ä¹‹ç±»çš„ä¸»é¢˜ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*weUnTXxmHxYf-B2DDaslvw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.6 æ·±åº¦å­¦ä¹ è¿›é˜¶&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
CMU å‘å¸ƒäº†â€œAdvanced Deep Learningâ€[40]è¯¾ç¨‹çš„å¹»ç¯ç‰‡å’Œæ•™å­¦å¤§çº²ï¼Œå…¶ä¸­åŒ…æ‹¬è¯¸å¦‚è‡ªåŠ¨å›å½’æ¨¡å‹ï¼Œç”Ÿæˆæ¨¡å‹ä»¥åŠè‡ªæˆ‘ç›‘ç£/é¢„æµ‹å­¦ä¹ ç­‰ä¸»é¢˜ã€‚è¯¥è¯¾ç¨‹é€‚ç”¨äº MS æˆ– Ph.Dï¼Œ å…·æœ‰ ML é«˜çº§èƒŒæ™¯çš„å­¦ç”Ÿã€‚&lt;/p&gt;

&lt;h1 id=&quot;7noteworthy-mentions-ï¸&quot;&gt;7ã€Noteworthy Mentions â­ï¸&lt;/h1&gt;

&lt;p&gt;BERT-of-Theseus[41]æå‡ºäº†ä¸€ç§é€šè¿‡å°† BERT æ¨¡å‹åˆ’åˆ†ä¸ºåŸå§‹ç»„ä»¶æ¥é€æ­¥æ›¿æ¢å’Œå‹ç¼© BERT æ¨¡å‹çš„æ–¹æ³•ã€‚é€šè¿‡é€æ­¥æ›¿æ¢å’Œè®­ç»ƒï¼Œè¿˜å…·æœ‰å°†æ¨¡å‹çš„åŸå§‹ç»„ä»¶å’Œå‹ç¼©ç‰ˆæœ¬ç»„åˆåœ¨ä¸€èµ·çš„ä¼˜åŠ¿ã€‚æ‰€æå‡ºçš„æ¨¡å‹ä¼˜äº GLUE åŸºå‡†ä¸Šçš„å…¶ä»–çŸ¥è¯†æç‚¼æ–¹æ³•ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
è¿™å„¿æ‰¾åˆ°ä¸€ä»½æœ‰è¶£çš„è¯¾ç¨‹ï¼Œç§°ä¸ºâ€œæœºå™¨å­¦ä¹ å…¥é—¨â€[42]ï¼Œæ¶µç›–äº† ML åŸºç¡€çŸ¥è¯†ï¼Œç›‘ç£å›å½’ï¼Œéšæœºæ£®æ—ï¼Œå‚æ•°è°ƒæ•´ä»¥åŠè®¸å¤šå…¶ä»–åŸºæœ¬çš„ ML ä¸»é¢˜ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
å¸Œè…Šè¯­ BERTï¼ˆGreekBERTï¼‰[43]æ¨¡å‹ç°åœ¨å¯é€šè¿‡ Hugging Face Transformers åº“ä½¿ç”¨ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jeremy Howard[44]å‘è¡¨äº†ä¸€ç¯‡è®ºæ–‡ï¼Œæè¿°äº† fastai æ·±åº¦å­¦ä¹ åº“ï¼Œè¯¥åº“è¢«å¹¿æ³›ç”¨äºç ”ç©¶å¹¶æ•™æˆå…¶æ·±åº¦å­¦ä¹ å¼€æ”¾è¯¾ç¨‹ï¼Œ æ¨èç»™è‡´åŠ›äºæ„å»ºå’Œæ”¹è¿›æ·±åº¦å­¦ä¹ å’Œ ML åº“çš„è½¯ä»¶å¼€å‘äººå‘˜ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Deeplearning.ai å®Œæˆäº† TensorFlow æ‰€æœ‰å››ä¸ªè¯¾ç¨‹çš„å‘å¸ƒï¼šTensorFlow: Data and Deployment Specialization[45]ã€‚è¯¥ä¸“ä¸šçš„ä¸»è¦ç›®çš„æ˜¯æ•™è‚²å¼€å‘äººå‘˜å¦‚ä½•åœ¨ä¸åŒçš„åœºæ™¯ä¸­æœ‰æ•ˆåœ°éƒ¨ç½²æ¨¡å‹ï¼Œä»¥åŠåœ¨è®­ç»ƒæ¨¡å‹æ—¶ä»¥æœ‰è¶£ä¸”æœ‰æ•ˆçš„æ–¹å¼ä½¿ç”¨æ•°æ®ã€‚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Raschka æœ€è¿‘å‘è¡¨äº†ä¸€ç¯‡é¢˜ä¸ºã€ŠPython ä¸­çš„æœºå™¨å­¦ä¹ ï¼šæ•°æ®ç§‘å­¦ï¼Œæœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½çš„ä¸»è¦å‘å±•å’ŒæŠ€æœ¯è¶‹åŠ¿[46]ã€‹çš„è®ºæ–‡ã€‚æœ¬æ–‡æ˜¯å¯¹æœºå™¨å­¦ä¹ å·¥å…·å‰æ™¯çš„å…¨é¢å›é¡¾ã€‚å¯¹äºç†è§£ ML å·¥ç¨‹ä¸­ä½¿ç”¨çš„æŸäº›åº“å’Œæ¦‚å¿µçš„å„ç§ä¼˜ç‚¹è€Œè¨€ï¼Œè¿™æ˜¯ä¸€ä»½æå¥½çš„æŠ¥å‘Šã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†æœ‰å…³åŸºäº Python çš„æœºå™¨å­¦ä¹ åº“çš„æœªæ¥çš„ä¿¡æ¯ã€‚&lt;/p&gt;

&lt;h1 id=&quot;æœ¬æ–‡å‚è€ƒèµ„æ–™&quot;&gt;æœ¬æ–‡å‚è€ƒèµ„æ–™&lt;/h1&gt;

&lt;p&gt;[1] &lt;strong&gt;å›¾çµè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆT-NLGï¼‰:&lt;/strong&gt; https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[2] &lt;strong&gt;NVIDIA Megatron-LM:&lt;/strong&gt; https://github.com/NVIDIA/Megatron-LM&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[3] &lt;strong&gt;Linguistically Informed Neural Dependency Parsing for Typologically Diverse Languages:&lt;/strong&gt; http://uu.diva-portal.org/smash/record.jsf?pid=diva2:1357373&amp;amp;dswid=7905&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[4] &lt;strong&gt;ä¾èµ–å…³ç³»è§£æ:&lt;/strong&gt; http://nlpprogress.com/english/dependency_parsing.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[5] &lt;strong&gt;Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents:&lt;/strong&gt; https://arxiv.org/abs/2002.01861&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[6] &lt;strong&gt;question understanding benchmark:&lt;/strong&gt; https://arxiv.org/abs/2001.11770v1&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[7] &lt;strong&gt;æœ‰è¶£çš„å·¥ä½œ:&lt;/strong&gt; https://ai.facebook.com/blog/using-radioactive-data-to-detect-if-a-data-set-was-used-for-training/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[8] &lt;strong&gt;å®Œæ•´è®ºæ–‡:&lt;/strong&gt; https://arxiv.org/pdf/2002.00937.pdf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[9] &lt;strong&gt;REALM: Retrieval-Augmented Language Model Pre-Training:&lt;/strong&gt; https://kentonl.com/pub/gltpc.2020.pdf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[10] &lt;strong&gt;change.org:&lt;/strong&gt; https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences[11]&lt;strong&gt;æ–°åšå®¢:&lt;/strong&gt; https://yoshuabengio.org/2020/02/10/fusce-risus/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[12] &lt;strong&gt;æŠ½è±¡æ¨ç†è¯­æ–™åº“ï¼ˆARCï¼‰:&lt;/strong&gt; https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[13] &lt;strong&gt;æœºå™¨å­¦ä¹ å’Œ NLP é¢†åŸŸ 2019 å¹´å‡ºç‰ˆæ•°æ®ç»Ÿè®¡:&lt;/strong&gt; https://www.marekrei.com/blog/ml-and-nlp-publications-in-2019/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[14] &lt;strong&gt;Growing Neural Cellular Automata:&lt;/strong&gt; https://distill.pub/2020/growing-ca/[15]&lt;strong&gt;äº¤äº’å¼ Transformer æ³¨æ„å¯è§†åŒ–:&lt;/strong&gt; https://github.com/SIDN-IAP/attnvis&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[16] &lt;strong&gt;SketchTransfer:&lt;/strong&gt; https://arxiv.org/pdf/1912.11570.pdf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[18] &lt;strong&gt;DeepSpeed ä¸ ZeRO:&lt;/strong&gt; https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[19] &lt;strong&gt;PyTorch3D:&lt;/strong&gt; https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[20] &lt;strong&gt;Hydra:&lt;/strong&gt; https://hydra.cc/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[21] &lt;strong&gt;CausalNex:&lt;/strong&gt; https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[22] &lt;strong&gt;TyDi QA:&lt;/strong&gt; https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[23] &lt;strong&gt;é—®ç­”åº“:&lt;/strong&gt; https://github.com/huggingface/node-question-answering&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[25] &lt;strong&gt;è¯†åˆ«æ–‡æœ¬ä¿¡æ¯ä¸­çš„ä¸»è§‚åè§:&lt;/strong&gt; https://podcasts.apple.com/us/podcast/will-ai-help-identify-bias-or-perpetuate-it-with-diyi-yang/id1435564422?i=1000464141922&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[26] &lt;strong&gt;DeepMind:&lt;/strong&gt; https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment[27]&lt;strong&gt;å®Œæ•´çš„æŠ¥å‘Š:&lt;/strong&gt; https://venturebeat.com/2020/01/30/google-researchers-release-audit-framework-to-close-ai-accountability-gap/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[28] &lt;strong&gt;å®Œæ•´çš„è®ºæ–‡:&lt;/strong&gt; https://dl.acm.org/doi/abs/10.1145/3351095.3372873&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[29] &lt;strong&gt;NLP Highlights:&lt;/strong&gt; https://soundcloud.com/nlp-highlights/104-model-distillation-with-victor-sanh-and-thomas-wolf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[30] &lt;strong&gt;DistilBERT:&lt;/strong&gt; https://arxiv.org/abs/1910.01108&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[31] &lt;strong&gt;è¯¸å¦‚ BERTï¼ŒELMo å’Œ GPT-2 ä¹‹ç±»çš„ä¸Šä¸‹æ–‡æ¨¡å‹åŠå…¶ä¸Šä¸‹æ–‡åŒ–çš„è¯è¡¨ç¤ºå½¢å¼æ˜¯ä»€ä¹ˆ:&lt;/strong&gt; https://kawine.github.io/blog/nlp/2020/02/03/contextual.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[32] &lt;strong&gt;Is the future of Neural Networks Sparse?:&lt;/strong&gt; https://medium.com/huggingface/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[33] &lt;strong&gt;ä»é›¶å¼€å§‹è®­ç»ƒè¯­è¨€æ¨¡å‹:&lt;/strong&gt; https://huggingface.co/blog/how-to-train&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[34] &lt;strong&gt;Tokenizers: How machines read:&lt;/strong&gt; https://blog.floydhub.com/tokenization-nlp/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[35] &lt;strong&gt;MLVU æœºå™¨å­¦ä¹ è¯¾ç¨‹:&lt;/strong&gt; https://mlvu.github.io/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[36] &lt;strong&gt;Machine-Learning-Tokyo/Math_resources:&lt;/strong&gt; https://github.com/Machine-Learning-Tokyo/Math_resources&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[37] &lt;strong&gt;MIT çš„â€œæ·±åº¦å­¦ä¹ å…¥é—¨â€è¯¾ç¨‹:&lt;/strong&gt; http://introtodeeplearning.com/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[38] &lt;strong&gt;PyTorch æ·±åº¦å­¦ä¹ :&lt;/strong&gt; https://atcold.github.io/pytorch-Deep-Learning-Minicourse/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[39] &lt;strong&gt;â€œMissing Semesterâ€:&lt;/strong&gt; https://missing.csail.mit.edu/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[40] &lt;strong&gt;â€œAdvanced Deep Learningâ€:&lt;/strong&gt; https://andrejristeski.github.io/10707-S20/syllabus.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[41] &lt;strong&gt;BERT-of-Theseus:&lt;/strong&gt; http://xxx.itp.ac.cn/abs/2002.02925&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[42] &lt;strong&gt;â€œæœºå™¨å­¦ä¹ å…¥é—¨â€:&lt;/strong&gt; https://compstat-lmu.github.io/lecture_i2ml/index.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[43] &lt;strong&gt;å¸Œè…Šè¯­ BERTï¼ˆGreekBERTï¼‰:&lt;/strong&gt; https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[44] &lt;strong&gt;Jeremy Howard:&lt;/strong&gt; https://arxiv.org/abs/2002.04688&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[45] &lt;strong&gt;TensorFlow: Data and Deployment Specialization:&lt;/strong&gt; https://www.coursera.org/specializations/tensorflow-data-and-deployment&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[46] &lt;strong&gt;Python ä¸­çš„æœºå™¨å­¦ä¹ ï¼šæ•°æ®ç§‘å­¦ï¼Œæœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½çš„ä¸»è¦å‘å±•å’ŒæŠ€æœ¯è¶‹åŠ¿:&lt;/strong&gt; https://arxiv.org/abs/2002.04803&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP%E7%AE%80%E6%8A%A5_ISSUE_4_PyTorch3D,_DeepSpeed,_Turing-NLG/&quot;&gt;NLPç®€æŠ¥ [CH]: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,â€¦&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 17, 2020.&lt;/p&gt;
  </content>
</entry>

</feed>
