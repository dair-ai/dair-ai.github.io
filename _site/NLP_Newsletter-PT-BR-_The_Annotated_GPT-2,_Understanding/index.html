<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->

<head>
<meta charset="utf-8">
<title>NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, √âtica em NLP, Torchmeta,‚Ä¶ &#8211; dair.ai</title>
<meta name="description" content="Esta edi√ß√£o abrange t√≥picos como a compreens√£o de self-distillation, tradu√ß√£o de imagem para ilustra√ß√£o, considera√ß√µes √©ticas para modelos de NLP, etc.">
<meta name="keywords" content="nlp_newsletter">


<!-- Twitter Cards -->
<meta name="twitter:title" content="NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, √âtica em NLP, Torchmeta,‚Ä¶">
<meta name="twitter:description" content="Esta edi√ß√£o abrange t√≥picos como a compreens√£o de self-distillation, tradu√ß√£o de imagem para ilustra√ß√£o, considera√ß√µes √©ticas para modelos de NLP, etc.">
<meta name="twitter:site" content="@dair_ai">
<meta name="twitter:creator" content="@flavioclesio">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:4000/images/nlp_newsletter_5.png">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, √âtica em NLP, Torchmeta,‚Ä¶">
<meta property="og:description" content="Esta edi√ß√£o abrange t√≥picos como a compreens√£o de self-distillation, tradu√ß√£o de imagem para ilustra√ß√£o, considera√ß√µes √©ticas para modelos de NLP, etc.">
<meta property="og:url" content="http://localhost:4000/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/">
<meta property="og:site_name" content="dair.ai">

<meta property="og:image" content="http://localhost:4000/images/nlp_newsletter_5.png">







<link rel="canonical" href="http://localhost:4000/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="dair.ai Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="http://localhost:4000/assets/js/vendor/html5shiv.min.js"></script>
	<script src="http://localhost:4000/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body class="post">

<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.8&appId=1537934899816329";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<!-- Go to www.addthis.com/dashboard to customize your tools -->
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-4e43ef4f23bf37b0"></script>

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="http://localhost:4000/">dair.ai</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				    
				    <li><a href="http://localhost:4000/posts/" >Blog ‚úçÔ∏è</a></li>
				
				    
				    <li><a href="http://localhost:4000/about/" >About ‚ÑπÔ∏è</a></li>
				
				    
				    <li><a href="http://localhost:4000/newsletter/" >NLP Newsletter üóûÔ∏è</a></li>
				
				    
				    <li><a href="http://localhost:4000/projects/" >Projects üí°</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai" target="_blank">GitHub üìÅ</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai/dair-ai.github.io/contribute" target="_blank">Contribute ‚ú®</a></li>
				
				    
				    <li><a href="https://medium.com/dair-ai" target="_blank">Medium üì∞</a></li>
				
				    
				    <li><a href="https://nlpoverview.com/" target="_blank">NLP Overview üìò</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai/nlp_highlights" target="_blank">2019 NLP Highlights (PDF) üî•</a></li>
				
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->



<div id="main" role="main">
  <div class="article-author-side">
    

<div itemscope itemtype="http://schema.org/Person">


	<img src="http://localhost:4000/images/flavio.png" class="bio-photo" alt="Flavio Clesio bio photo">


  <h3 itemprop="name">Flavio Clesio</h3>
  <p>Machine Learning Engineer (NLP, CV, Marketplace RecSys)</p>

  <a href="http://twitter.com/flavioclesio" class="author-social" target="_blank"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a>
  
  
  
  
  <a href="http://instagram.com/flavioclesio" class="author-social" target="_blank"><i class="fa fa-fw fa-instagram"></i> Instagram</a>
  
  <a href="http://github.com/fclesio" class="author-social" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a>
  
  
  
  
  
  
  
  
  
  
</div>

  </div>
  <article class="post">
    <div class="headline-wrap">
      
        
          <h1><a href="http://localhost:4000/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/" rel="bookmark" title="NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, √âtica em NLP, Torchmeta,‚Ä¶">NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, √âtica em NLP, Torchmeta,‚Ä¶</a></h1>
        
      
    </div><!--/ .headline-wrap -->

    
    <div class="article-wrap">
      <p><img src="https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png" alt="" /></p>

<p><br />
Antes de tudo, gostaria de agradecer de ‚ù§Ô∏è a todos voc√™s pelo incr√≠vel apoio e incentivo para continuar com a NLP Newsletter. Esse esfor√ßo requer pesquisa, edi√ß√£o, e tradu√ß√£o tediosas, mas que considero gratificantes e √∫teis para fornecer o melhor conte√∫do. Espero que voc√™ esteja gostando deste conte√∫do. üòâ</p>

<p><br />
<a href="https://dair.ai/newsletter/"><em>Assine a NLP Newsletter</em></a> <em>üîñ para receber edi√ß√µes futuras via e-mail.</em></p>

<h1 id="publica√ß√µes-">Publica√ß√µes üìô</h1>

<p><strong><em>Um entendimento te√≥rico do self-distillation</em></strong></p>

<p><br />
No contexto de Deep Learning, <a href="https://arxiv.org/pdf/1503.02531.pdf"><em>self-distillation</em></a> (<em>NT: auto-destila√ß√£o</em>) √© o processo de transfer√™ncia de conhecimento de uma arquitetura para outra. As previs√µes do modelo original s√£o alimentadas como valores de destino para o outro modelo durante o treinamento. Al√©m de ter propriedades desej√°veis como a redu√ß√£o do tamanho dos modelos, os resultados emp√≠ricos mostram que essa abordagem funciona bem em conjuntos de dados n√£o vistos anteriormente pelo modelo (NT: amostras <em>held out</em>). Um grupo de pesquisadores publicou recentemente um artigo que fornece uma an√°lise te√≥rica com o foco em um melhor entendimento sobre o que est√° acontecendo neste processo de <em>destila√ß√£o do conhecimento</em> e o porque ele √© eficaz. Os resultados mostram que alguns poucos ciclos de destila√ß√£o amplificam a regulariza√ß√£o (devido ao fato que a t√©cnica <a href="https://twitter.com/TheGradient/status/1228132843630387201?s=20"><em>progressivamente ajuda a limitar o n√∫mero de fun√ß√µes base que representam a solu√ß√£o</em></a>) as quais tendem a reduzir o over-fitting. (Leia o paper <a href="https://arxiv.org/abs/2002.05715"><strong>aqui</strong></a>)</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png" alt="" /></p>

<p><a href="https://arxiv.org/abs/2002.05715"><em>Fonte</em></a></p>

<p><br />
<strong><em>Os anos 2010s: Nossa d√©cada de Deep Learning / Perspectivas para os 2020s</em></strong></p>

<p><br />
<a href="http://people.idsia.ch/~juergen/">J√ºrgen Schmidhuber,</a> um dos pioneiros em Intelig√™ncia Artificial,  <a href="http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html"><strong>postou recentemente em seu blog</strong></a> uma vis√£o hist√≥rica sobre Deep Learning desde o ano de 2010. Alguns t√≥picos incluem <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTMs</a>, <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">feedforward neural networks</a>, <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">GANs</a>, <a href="https://en.wikipedia.org/wiki/Deep_reinforcement_learning">deep reinforcement learning</a>, <a href="https://en.wikipedia.org/wiki/Meta_learning_(computer_science)">meta-learning</a>, world models, <a href="https://arxiv.org/abs/1503.02531">distilling NNs</a>, <a href="https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f">attention learning</a>, etc. O artigo traz algumas perspectivas futuras para os anos 2020 chamando aten√ß√£o para quest√µes como privacidade e mercado de dados.</p>

<p><br />
<strong><em>Usando Redes Neurais para a resolu√ß√£o de equa√ß√µes matem√°ticas</em></strong></p>

<p><br />
Pesquisadores do Facebook AI publicaram um <a href="https://arxiv.org/abs/1912.01412"><strong>paper</strong></a> em que apresentam um modelo treinado em problemas de matem√°tica para prever poss√≠veis solu√ß√µes para in√∫meras tarefas como, por exemplo, problemas de integra√ß√£o. A abordagem √© baseada em uma nova estrutura semelhante √† usada na <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> (<em>NT: tradu√ß√£o autom√°tica neural</em>), em que express√µes matem√°ticas s√£o representadas como um tipo de linguagem e as solu√ß√µes tratadas como um problema de tradu√ß√£o. Assim, ao inv√©s do modelo produzir uma tradu√ß√£o, a sa√≠da desta tradu√ß√£o √© a pr√≥pria solu√ß√£o do problema. Com isso, os pesquisadores afirmam que as redes Deep Learning n√£o s√£o apenas boas em racioc√≠nio simb√≥lico, mas podem ser usadas tamb√©m para tarefas mais diversas.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png" alt="" /></p>

<p><em>Equa√ß√µes sendo usadas como entrada, juntamente com a solu√ß√£o correspondente gerada pelo modelo‚Äî</em>‚Ää<a href="https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/"><em>fonte</em></a></p>

<h1 id="criatividade-e-sociedade-">Criatividade e Sociedade üé®</h1>

<p><strong><em>Intelig√™ncia Artificial para descobertas cient√≠ficas</em></strong></p>

<p><br />
Mattew Hutson <a href="https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times"><strong>informa</strong></a> como a intelig√™ncia artificial (IA) pode ser utilizada para produzir emuladores que t√™m um uso importante na modelagem de fen√¥menos naturais complexos e que, por sua vez, podem levar a diferentes tipos de <em>descobertas cient√≠ficas</em>. A mudan√ßa na constru√ß√£o desses emuladores acontece devido ao fato de que estes modelos geralmente exigem dados em larga escala e uma vasta explora√ß√£o de par√¢metros. Um <a href="https://arxiv.org/abs/2001.08055"><strong>paper recente</strong></a> prop√µe um m√©todo chamado DENSE que √© uma abordagem baseada em <a href="https://en.wikipedia.org/wiki/Neural_architecture_search"><em>neural architecture search (NAS)</em></a> (NT: Explora√ß√£o e busca de arquitetura de Redes Neurais) para criar emuladores precisos, contando apenas com uma quantidade limitada de dados de treinamento. Eles o testaram executando simula√ß√µes para casos que incluem astrof√≠sica, ci√™ncia clim√°tica e energia de fus√£o, entre outros.</p>

<p><br />
<strong><em>Melhorando a tradu√ß√£o de imagem para imagem</em></strong></p>

<p><br />
<a href="https://arxiv.org/abs/2002.05638">GANILLA</a> √© uma abordagem que prop√µe o uso de <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">GANs</a> para melhorar a transfer√™ncia de estilo e conte√∫do em pares para tarefas de tradu√ß√£o <a href="https://paperswithcode.com/task/image-to-image-translation"><em>image-to-image</em></a> (NT: imagem para imagem). A abordagem prop≈çe um modelo de imagem para imagem (com uma rede de geradores aprimorada) e este modelo √© avaliado com base em uma nova estrutura de avalia√ß√£o quantitativa que considera tanto o conte√∫do quanto o estilo. A novidade do trabalho est√° na rede de geradores proposta, que considera um equil√≠brio entre estilo e conte√∫do que os modelos anteriores n√£o conseguem. O c√≥digo e os modelos pr√©-treinados est√£o <a href="https://github.com/giddyyupp/ganilla">dispon√≠veis</a>. Leia o artigo completo <a href="https://arxiv.org/abs/2002.05638"><strong>aqui</strong></a>.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png" alt="" /></p>

<p><br />
<strong><em>Andrew Ng fala sobre o interesse em aprendizagem auto-supervisionada</em></strong></p>

<p><br />
Andrew Ng, o fundador do <a href="deeplearning.ai">deeplearning.ai</a>, falou no <a href="https://www.youtube.com/watch?v=0jspaMLxBig"><strong>podcast de Intelig√™ncia Artificial do Lex Friedman</strong></a> sobre os seguintes t√≥picos: seus primeiros anos em ML, o futuro da IA, educa√ß√£o em IA, recomenda√ß√µes para o uso adequado da ML, seus objetivos pessoais e quais t√©cnicas de ML que devemos prestar aten√ß√£o nesta d√©cada de 2020.</p>

<p><br />
Andrew explicou o motivo da sua anima√ß√£o em rela√ß√£o ao <em>self-supervised representation learning.</em> <a href="https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html"><strong>Self-supervised learning</strong></a> (NT: aprendizado de representa√ß√£o auto-supervisionado) envolve a estrutura√ß√£o de um problema de aprendizagem que visa obter supervis√£o dos pr√≥prios dados para fazer uso de grandes quantidades de dados n√£o rotulados, o que √© mais comum que os dados rotulados limpos. As representa√ß√µes s√£o importantes e podem ser usadas para lidar com tarefas posteriores, semelhantes √†s usadas em modelos de linguagem como o <a href="https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert">BERT</a>.</p>

<p><br />
Tamb√©m h√° muito interesse em usar o aprendizado auto-supervisionado para treinamento de representa√ß√µes visuais generalizadas que tornam os modelos mais precisos em ambientes com poucos recursos. Por exemplo, um m√©todo recente chamado <a href="https://arxiv.org/abs/2002.05709"><strong>SimCLR</strong></a> (liderado por <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">Geoffrey Hinton</a>) prop√µe uma estrutura para <em>aprendizagem auto-supervisionada contrastante</em> (<em>NT: contrastive self-supervised learning</em>) de representa√ß√µes visuais para melhorar a classifica√ß√£o de imagens em diferentes configura√ß√µes, como transfer√™ncia de aprendizado (NT: <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>) e aprendizado semi-supervisionado.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png" alt="" /></p>

<p><a href="https://arxiv.org/abs/2002.05709"><em>fonte</em></a></p>

<h1 id="ferramentas-e-datasets-Ô∏è">Ferramentas e Datasets ‚öôÔ∏è</h1>

<p><strong><em>Bibliotecas JAX</em></strong></p>

<p><br />
<a href="https://github.com/google/jax">JAX</a> √© uma nova biblioteca que combina o NumPy e <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">diferencia√ß√£o autom√°tica</a> para realizar pesquisas de ML de alto desempenho. Para simplificar os pipelines para a constru√ß√£o de redes neurais usando JAX, a <a href="https://deepmind.com/">DeepMind</a> lan√ßou o <a href="https://github.com/deepmind/dm-haiku"><strong>Haiku</strong></a> e <a href="https://github.com/deepmind/rlax"><strong>RLax</strong></a>. O RLax simplifica a implementa√ß√£o de agentes de aprendizado por refor√ßo e o Haiku simplifica a constru√ß√£o de redes neurais usando <em>modelos familiares com o paradigma de programa√ß√£o orientada a objetos.</em></p>

<p><br />
<strong><em>Uma ferramenta para processar dados da Wikip√©dia</em></strong></p>

<p><br />
<a href="https://github.com/epfl-lts2/sparkwiki"><strong>Sparkwiki</strong></a> √© uma ferramenta para processar dados da Wikip√©dia. Esta vers√£o faz parte de muitos esfor√ßos para permitir pesquisas interessantes de an√°lise comportamental, como <a href="https://arxiv.org/abs/2002.06885">a captura de tend√™ncias e preconceitos em diferentes idiomas na Wikip√©dia</a>. Os autores descobriram que, independentemente do idioma, o comportamento de navega√ß√£o dos usu√°rios da Wikip√©dia mostra que eles tendem a compartilhar interesses comuns por categorias como filmes, m√∫sica e esportes, mas as diferen√ßas se tornam mais aparentes com eventos locais e particularidades culturais.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg" alt="" /></p>

<p><br />
<strong><em>Tokenizers em Rust, DistilBERT e outros</em></strong></p>

<p><br />
Um novo release dos <a href="https://github.com/huggingface/transformers/releases/tag/v2.5.0"><strong>Transformers</strong></a> da Hugging Face inclui a integra√ß√£o de sua biblioteca de tokeniza√ß√£o r√°pida, que visa acelerar modelos como o <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">BERT</a>, <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a>, <a href="https://openai.com/blog/better-language-models/">GPT-2</a> e outros modelos criados pela comunidade.</p>

<h1 id="√âtica-em-intelig√™ncia-artificial-">√âtica em Intelig√™ncia Artificial üö®</h1>

<p><strong><em>Considera√ß√µes √©ticas para modelos de NLP (Processamento de Linguagem Natural) e Machine Learning</em></strong></p>

<p><br />
Em um novo <a href="https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender"><strong>epis√≥dio</strong></a> do postcast <a href="https://soundcloud.com/nlp-highlights">NLP Highlights,</a> <a href="https://twitter.com/emilymbender">Emily Bender</a> e os hosts conversaram sobre algumas considera√ß√µes √©ticas no desenvolvimento de modelos e tecnologias de NLP no contexto da academia e do seu uso no mundo real. Alguns dos t√≥picos da discuss√£o incluem considera√ß√µes √©ticas nas tarefas de NLP, abordagens sobre coleta de dados e eventualmente considera√ß√µes na publica√ß√£o de resultados.</p>

<p><br />
Al√©m de todas as considera√ß√µes acima, uma preocupa√ß√£o discutida √© que a comunidade de IA est√° se concentrando demais na otimiza√ß√£o de m√©tricas espec√≠ficas, o que contraria os objetivos que a IA pretende alcan√ßar. Rachel Thomas e David Uminsky discutem os problemas dessa abordagem atrav√©s de uma <a href="https://arxiv.org/abs/2002.08512"><strong>an√°lise completa</strong></a> de diferentes casos de uso. Eles tamb√©m prop√µem uma estrutura simples para mitigar este problema, que envolve o uso e a combina√ß√£o de v√°rias m√©tricas, seguidas pelo envolvimento das pessoas afetadas diretamente pela tecnologia.</p>

<h1 id="artigos-e-blog-posts-Ô∏è">Artigos e Blog posts ‚úçÔ∏è</h1>

<p><strong>‚ÄúThe Annotated GPT-2‚Äù</strong></p>

<p><br />
Aman Arora publicou recentemente uma postagem no blog excepcionalmente intitulada <a href="https://amaarora.github.io/2020/02/18/annotatedGPT2.html">‚ÄúThe Annotated GPT-2‚Äú</a> explicando o funcionamento interno do modelo baseado em <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">Transformer</a> chamado <a href="https://openai.com/blog/better-language-models/">GPT-2</a>. Sua abordagem foi inspirada em <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> que adotou uma abordagem de anota√ß√£o para explicar as partes importantes do modelo. Aman fez um grande esfor√ßo para reimplementar o <a href="https://openai.com/blog/better-language-models/">GPT-2</a> da <a href="https://openai.com/">OpenAI</a> usando o <a href="https://pytorch.org/">PyTorch</a> e a biblioteca <a href="https://huggingface.co/transformers/">Transformers da Hugging Face</a>. √â um trabalho brilhante!</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png" alt="" /></p>

<p><a href="https://amaarora.github.io/2020/02/18/annotatedGPT2.html"><em>fonte</em></a></p>

<p><br />
<strong><em>Al√©m do BERT?</em></strong></p>

<p><br />
<a href="https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1"><strong>Um ponto interessante foi levantado</strong></a> por Sergi Castella sobre o que est√° al√©m do <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">BERT</a>. Os principais t√≥picos incluem o aprimoramento das m√©tricas, uma reflex√£o de como a biblioteca <a href="https://huggingface.co/transformers/">Transformers da Hugging Face</a> ajuda na pesquisa, alguns conjuntos de dados interessantes para an√°lise, etc.</p>

<p><br />
<strong><em>Operador de Compress√£o de Matrizes</em></strong></p>

<p><br />
O Blog do TensorFlow publicou um <a href="https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016"><strong>post</strong></a> explicando as t√©cnicas e a import√¢ncia por tr√°s da compress√£o matrizes em um modelo de Deep Learning. <em>A compacta√ß√£o matricial</em> (<em>NT: Matrix compression</em>) pode ajudar a criar modelos menores e mais eficientes que podem ser incorporados a dispositivos menores, como telefones e assistentes dom√©sticos. Concentrar-se na compress√£o dos modelos por meio de m√©todos como <a href="https://en.wikipedia.org/wiki/Low-rank_approximation">low-rank-approximation</a> e <a href="https://en.wikipedia.org/wiki/Quantization_(signal_processing)">quantiza√ß√£o</a> significa que n√£o precisamos comprometer a qualidade do modelo.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png" alt="" /></p>

<p><a href="https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016"><em>fonte</em></a></p>

<h1 id="educa√ß√£o-">Educa√ß√£o üéì</h1>

<p><strong><em>Fundamentos de NLP</em></strong></p>

<p><br />
Estou animado por lan√ßado um rascunho do Cap√≠tulo 1 da minha nova s√©rie chamado <a href="https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-senten√ßa-segmenta√ß√£o-b362c5d07684"><strong>Fundamentos de NLP</strong></a>. Esta s√©rie ensina conceitos de NLP a partir do b√°sico, compartilhando boas pr√°ticas, refer√™ncias importantes, erros comuns a serem evitados e o que est√° por vir no que se refere a NLP. Um <a href="https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ">notebook no Colab</a> foi inclu√≠do e o projeto ser√° mantido <a href="https://github.com/dair-ai/nlp_fundamentals">aqui</a>.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif" alt="" /></p>

<p><br />
<strong><em>Revis√£o/Discuss√£o Online: Parte I sess√£o de leitura para fundamentos da matem√°tica</em></strong></p>

<p><br />
O time do <a href="https://www.meetup.com/Machine-Learning-Tokyo/">Meetup ‚ÄúMachine Learning Tokyo‚Äù</a> est√° hospedando uma discuss√£o on-line remota, revisando cap√≠tulos que foram abordados em suas recentes sess√µes de estudo on-line. O grupo j√° havia estudado cap√≠tulos com base no livro <a href="https://mml-book.github.io/">Mathematics For Machine Learning</a> escrito por Marc Peter Deisenroth, A Aldo Faisal e Cheng Soon Ong. O <a href="https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/"><strong>evento</strong></a> est√° programado para 8 de mar√ßo de 2020.</p>

<p><br />
<strong><em>Recomenda√ß√µes de livros</em></strong></p>

<p><br />
Em um segmento anterior, discutimos a import√¢ncia da compress√£o de matriz para a constru√ß√£o de modelos pequenos (em termos de espa√ßo) de ML. Se voc√™ estiver interessado em aprender mais sobre como construir redes neurais profundas menores para sistemas embarcados, confira este √≥timo livro chamado <a href="https://tinymlbook.com/?linkId=82595412"><strong>TinyML</strong></a> de Pete Warden e Daniel Situnayake.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg" alt="" /></p>

<p><a href="https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043"><em>fonte</em></a></p>

<p><br />
Outro livro interessante para ficar de olho √© o pr√≥ximo t√≠tulo  <strong>‚Äú</strong><a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527"><strong>Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD</strong></a><strong>‚Äù</strong> de Jeremy Howard e Sylvain Gugger. O livro tem como objetivo fornecer a base matem√°tica necess√°ria para criar e treinar modelos para abordar tarefas nas √°reas de vis√£o computacional e NLP.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg" alt="" /></p>

<p><a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527"><em>source</em></a></p>

<h1 id="men√ß√µes-honrosas-Ô∏è">Men√ß√µes honrosas ‚≠êÔ∏è</h1>

<p>Voc√™ pode acessar a NLP Newsletter anterior em PT-BR <a href="https://dair.ai/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG/">aqui</a>.</p>

<p><br />
<a href="https://arxiv.org/abs/1909.06576"><strong>Torchmeta</strong></a> √© uma biblioteca para pesquisa em meta-aprendizado. Esta biblioteca √© de autoria de Tristan Deleu.</p>

<p><br />
Manuel Tonneau escreveu um <a href="https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter"><strong>post</strong></a> oferecendo uma vis√£o mais detalhada em rela√ß√£o ao hardware envolvido em modelagem de linguagem. Alguns t√≥picos incluem <em>greedy</em> e <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a> e <a href="https://openreview.net/forum?id=rygGQyrFvH">nucleus sampling</a>.</p>

<p><br />
O MIT <a href="http://introtodeeplearning.com/"><strong>lan√ßou</strong></a> o plano de estudos completo e a programa√ß√£o do curso intitulado ‚ÄúIntrodu√ß√£o ao Deep Learning‚Äù, incluindo v√≠deos das palestras j√° ministradas. Eles pretendem lan√ßar palestras em v√≠deo e slides uma vez por semana.</p>

<p><br />
Aprenda a treinar um modelo para <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">reconhecimento de entidade (NER)</a> usando uma abordagem baseada no Transformer em <a href="https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py"><strong>300 linhas de c√≥digo</strong></a>. Voc√™ pode encontrar o Google Colab em anexo <a href="https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn">aqui</a>.</p>

<hr />

<p>Se voc√™ tiver datasets, projetos, postagens de blog, tutoriais ou documentos que deseja compartilhar na pr√≥xima edi√ß√£o da NLP Newsletter, entre em contato conosco pelo e-mail ellfae@gmail.com ou via <a href="https://twitter.com/omarsar0"><strong>DM no Twitter</strong></a>.</p>

<p><br />
<a href="https://dair.ai/newsletter/"><em>Assine a NLP Newsletter</em></a> <em>üîñ para receber edi√ß√µes futuras via e-mail.</em></p>

      <hr />
      <footer role="contentinfo">
        <div class="social-share">
  <!-- Go to www.addthis.com/dashboard to customize your tools --> 
  <div class="addthis_inline_share_toolbox"></div>
  <!--
  <h4>Share on</h4>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/" class="twitter" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/" class="facebook" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=http://localhost:4000/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/" class="google-plus" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
  </ul>-->
</div><!-- /.social-share -->
        <p class="byline"><strong>NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, √âtica em NLP, Torchmeta,‚Ä¶</strong> was published on <time datetime="2020-02-29T00:00:00+01:00">February 29, 2020</time>.</p>
        
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dair-ai'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<!--
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->

      </footer>
    </div><!-- /.article-wrap -->
  
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
  
  </article>
</div><!-- /#main -->

<div class="footer-wrap">
  
  <div class="related-articles">
  <h4>You might also enjoy <small class="pull-right">(<a href="http://localhost:4000/posts/">View all posts</a>)</small></h4>
    <ul>
    
      <li><a href="http://localhost:4000/NLP_Newsletter_NLP_7-ZH-.md/" title="NLP ÁÆÄÊä•ÔºàIssue#7Ôºâ: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶">NLP ÁÆÄÊä•ÔºàIssue#7Ôºâ: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶</a></li>
    
      <li><a href="http://localhost:4000/NLP_Newsletter_NLP_7/" title="NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶">NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶</a></li>
    
      <li><a href="http://localhost:4000/NLP_Newsletter_-7_-FR/" title="NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶">NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶</a></li>
    
    </ul>
    <hr />
  </div><!-- /.related-articles -->
  
  <footer>
    

<span>&copy; 2020 dair.ai. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  var _gaq = _gaq || [];
  var pluginUrl =
 '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
  _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
  _gaq.push(['_setAccount', 'UA-158959084-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>


  
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dair-ai'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<!--
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->




</body>
</html>
