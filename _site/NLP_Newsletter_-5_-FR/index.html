<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->

<head>
<meta charset="utf-8">
<title>NLP Newsletter [FR] #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,‚Ä¶ &#8211; dair.ai</title>
<meta name="description" content="">
<meta name="keywords" content="nlp_newsletter">


<!-- Twitter Cards -->
<meta name="twitter:title" content="NLP Newsletter [FR] #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,‚Ä¶">
<meta name="twitter:description" content="">
<meta name="twitter:site" content="@dair_ai">


<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://dair.ai/images/nlp_newsletter_5.png">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP Newsletter [FR] #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,‚Ä¶">
<meta property="og:description" content="">
<meta property="og:url" content="https://dair.ai/NLP_Newsletter_-5_-FR/">
<meta property="og:site_name" content="dair.ai">

<meta property="og:image" content="https://dair.ai/images/nlp_newsletter_5.png">







<link rel="canonical" href="https://dair.ai/NLP_Newsletter_-5_-FR/">
<link href="https://dair.ai/feed.xml" type="application/atom+xml" rel="alternate" title="dair.ai Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://dair.ai/assets/css/main.css">

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="https://dair.ai/assets/js/vendor/html5shiv.min.js"></script>
	<script src="https://dair.ai/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="https://dair.ai/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://dair.ai/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://dair.ai/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://dair.ai/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://dair.ai/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://dair.ai/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://dair.ai/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body class="post">

<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.8&appId=1537934899816329";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<!-- Go to www.addthis.com/dashboard to customize your tools -->
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-4e43ef4f23bf37b0"></script>

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="https://dair.ai/">dair.ai</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				    
				    <li><a href="https://dair.ai/posts/" >Blog ‚úçÔ∏è</a></li>
				
				    
				    <li><a href="https://dair.ai/about/" >About ‚ÑπÔ∏è</a></li>
				
				    
				    <li><a href="https://dair.ai/newsletter/" >NLP Newsletter üóûÔ∏è</a></li>
				
				    
				    <li><a href="https://dair.ai/projects/" >Projects üí°</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai" target="_blank">GitHub üìÅ</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai/dair-ai.github.io/contribute" target="_blank">Contribute ‚ú®</a></li>
				
				    
				    <li><a href="https://medium.com/dair-ai" target="_blank">Medium üì∞</a></li>
				
				    
				    <li><a href="https://nlpoverview.com/" target="_blank">NLP Overview üìò</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai/nlp_highlights" target="_blank">2019 NLP Highlights (PDF) üî•</a></li>
				
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->



<div id="main" role="main">
  <div class="article-author-side">
    

<div itemscope itemtype="http://schema.org/Person">


	<img src="https://dair.ai/images/lbourdois.png" class="bio-photo" alt="Lo√Øck BOURDOIS bio photo">


  <h3 itemprop="name">Lo√Øck BOURDOIS</h3>
  <p>Data Scientist working at the Bordeaux Population Health Research Centre of INSERM University of Bordeaux.</p>

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
</div>

  </div>
  <article class="post">
    <div class="headline-wrap">
      
        
          <h1><a href="https://dair.ai/NLP_Newsletter_-5_-FR/" rel="bookmark" title="NLP Newsletter [FR] #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,‚Ä¶">NLP Newsletter [FR] #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,‚Ä¶</a></h1>
        
      
    </div><!--/ .headline-wrap -->

    
    <div class="article-wrap">
      <p><img src="https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png" alt="" /></p>

<h1 id="avant-propos">Avant-propos</h1>
<p>Tout d‚Äôabord, je ne saurais trop tous vous remercier pour le soutien et les encouragements incroyables que vous avez apport√©s √† cette newsletter. Son √©laboration n√©cessite des recherches et une r√©daction fastidieuse que je trouve √† la fois enrichissantes et utiles, afin de vous fournir le meilleur contenu. J‚Äôesp√®re que vous les appr√©ciez, car c‚Äôest le cas pour moi. üòâ</p>

<h1 id="publications-">Publications üìô</h1>

<p><strong><em>Une compr√©hension th√©orique de l‚Äôautodistillation</em></strong></p>

<p><br />
L‚Äô<a href="https://arxiv.org/pdf/1503.02531.pdf">autodistillation</a> est le processus de transfert de connaissances d‚Äôune architecture √† une seconde qui est identique. Les pr√©dictions du mod√®le original sont transmises comme valeurs cibles au second mod√®le pendant la phase d‚Äôentra√Ænement. Outre les propri√©t√©s souhaitables, comme la r√©duction de la taille du mod√®le, les r√©sultats empiriques montrent que cette approche fonctionne bien sur des ensembles de donn√©es maintenus. Un groupe de chercheurs a r√©cemment publi√© un article qui fournit une analyse th√©orique visant √† mieux comprendre ce qui se passe dans ce processus et pourquoi il est efficace. Les r√©sultats montrent que quelques cycles d‚Äôautodistillation amplifient la r√©gularisation (<a href="https://twitter.com/TheGradient/status/1228132843630387201?s=20">en limitant progressivement le nombre de fonctions de base qui repr√©sentent la solution</a>), ce qui tend √† r√©duire le sur-apprentissage. (Lire l‚Äôarticle complet <a href="https://arxiv.org/abs/2002.05715">ici</a>)</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png" alt="" /></p>

<p><a href="https://arxiv.org/abs/2002.05715"><em>source</em></a></p>

<p><br />
<strong><em>Les ann√©es 2010 : Une d√©cennie d‚Äôapprentissage approfondi / Perspectives pour les ann√©es 2020</em></strong></p>

<p><br />
<a href="http://people.idsia.ch/~juergen/">J√ºrgen Schmidhuber</a>, un pionnier de l‚Äôintelligence artificielle, a r√©cemment publi√© un <a href="http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html">article sur son blog</a> qui se concentre sur un aper√ßu historique de l‚Äôapprentissage profond depuis 2010. Parmi les sujets abord√©s, citons les LSTM, les feedforward NN, les GAN, l‚Äôapprentissage par renforcement, le m√©ta-apprentissage, la distillation, l‚Äôapprentissage par l‚Äôattention, etc. L‚Äôarticle se termine par une perspective sur les ann√©es 2020, encourageant l‚Äôattention sur des questions urgentes telles que la vie priv√©e et les march√©s des donn√©es.</p>

<p><br />
<strong><em>Utilisation des r√©seaux de neurones pour r√©soudre des √©quations math√©matiques</em></strong></p>

<p><br />
Les chercheurs du FAIR de Facebook ont publi√© un <a href="https://arxiv.org/abs/1912.01412">article</a> qui propose un mod√®le entra√Æn√© sur des probl√®mes math√©matiques ainsi que leurs solutions associ√©es, afin d‚Äôapprendre √† pr√©dire les solutions possibles pour des t√¢ches telles que la r√©solution de probl√®mes d‚Äôint√©gration. L‚Äôapproche est bas√©e sur une approche similaire √† celle utilis√©e dans la traduction automatique o√π les expressions math√©matiques sont repr√©sent√©es comme une sorte de langage et les solutions sont trait√©es comme le probl√®me de traduction. Ainsi, au lieu que le mod√®le produise une traduction, le r√©sultat est la solution elle-m√™me. Les chercheurs affirment ainsi que les r√©seaux neuronaux profonds ne sont pas seulement bons pour le raisonnement symbolique, mais aussi pour des t√¢ches plus diverses.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png" alt="" /></p>

<p><em>√âquations fournies en entr√©e avec la solution correspondante fournie par le mod√®le ‚Äì</em> <a href="https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/"><em>source</em></a></p>

<h1 id="cr√©ativit√©-et-soci√©t√©-">Cr√©ativit√© et soci√©t√© üé®</h1>

<p><strong><em>L‚ÄôIA au service de la d√©couverte scientifique</em></strong></p>

<p><br />
Mattew Hutson <a href="https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times">rapporte</a> comment l‚ÄôIA peut √™tre utilis√©e pour produire des √©mulateurs qui ont une utilit√© importante dans la mod√©lisation de ph√©nom√®nes naturels complexes qui pourraient, √† leur tour, conduire √† diff√©rents types de d√©couvertes scientifiques. Le d√©fi avec la construction de ces √©mulateurs est qu‚Äôils n√©cessitent souvent d‚Äôimportantes donn√©es et une exploration approfondie des param√®tres. Un <a href="https://arxiv.org/abs/2001.08055">article</a> r√©cent propose DENSE, une approche bas√©e sur la <a href="https://en.wikipedia.org/wiki/Neural_architecture_search">recherche d‚Äôarchitecture neurale</a> pour construire des √©mulateurs pr√©cis tout en ne s‚Äôappuyant que sur une quantit√© limit√©e de donn√©es d‚Äôentra√Ænement. Ils l‚Äôont test√©e en effectuant des simulations pour des cas tels que l‚Äôastrophysique, la climatologie et la fusion, entre autres.</p>

<p><br />
<strong><em>Am√©liorer la ¬´ traduction ¬ª de l‚Äôimage √† l‚Äôillustration</em></strong></p>

<p><br />
GANILLA est une approche qui propose l‚Äôutilisation de GAN pour am√©liorer le transfert √† la fois du style et du contenu dans la <a href="https://paperswithcode.com/task/image-to-image-translation">t√¢che de traduction d‚Äôimage √† image</a> non appari√©e. En particulier, un mod√®le d‚Äôillustration d‚Äôimage √† image est propos√© (avec un r√©seau g√©n√©rateur am√©lior√©) et √©valu√© sur la base d‚Äôun nouveau cadre d‚Äô√©valuation quantitative qui prend en compte √† la fois le contenu et le style. La nouveaut√© de ce travail r√©side dans le r√©seau g√©n√©rateur propos√© qui tient compte d‚Äôun √©quilibre entre le style et le contenu, ce que les mod√®les pr√©c√©dents n‚Äôont pas r√©ussi √† atteindre. Des codes et mod√®les pr√©-entrain√©s sont mis √† <a href="https://github.com/giddyyupp/ganilla">disposition</a>. Lisez le document complet <a href="https://arxiv.org/abs/2002.05638">ici</a>.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png" alt="" /></p>

<p><br />
<strong><em>Andrew Ng parle de l‚Äôint√©r√™t pour l‚Äôauto-apprentissage</em></strong></p>

<p><br />
Andrew Ng, le fondateur de deeplearning.ai, est intervenu dans un podcast sur l‚Äôintelligence artificielle pour <a href="https://www.youtube.com/watch?v=0jspaMLxBig">parler</a> de sujets tels que ses d√©buts dans le ML, l‚Äôavenir de l‚ÄôIA, l‚Äôenseignement de l‚ÄôIA, ses recommandations pour une bonne utilisation du ML, ses objectifs personnels et les techniques de ML auxquelles il faudra pr√™ter attention dans les ann√©es 2020.</p>

<p><br />
Andrew a expliqu√© pourquoi il est tr√®s enthousiaste √† l‚Äôid√©e de s‚Äôinitier √† l‚Äôauto-apprentissage. <a href="https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html">L‚Äôauto-apprentissage supervis√©</a> consiste √† formuler un probl√®me d‚Äôapprentissage dont le but est d‚Äôobtenir une supervision √† partir des donn√©es elles-m√™mes. L‚Äôint√©r√™t est d‚Äôutiliser de grandes quantit√©s de donn√©es non lab√©lis√©es, ce qui est plus disponibles en plus grande quantit√© que les donn√©es lab√©lis√©es. Les repr√©sentations, par opposition √† l‚Äôex√©cution de la t√¢che, sont importantes et peuvent √™tre utilis√©es pour traiter des t√¢ches en aval, comme c‚Äôest le cas dans les mod√®les linguistiques tels que le <a href="https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert">BERT</a>.</p>

<p><br />
Il y a √©galement beaucoup d‚Äôint√©r√™t √† utiliser l‚Äôauto-apprentissage supervis√©](pour apprendre des repr√©sentations visuelles g√©n√©ralis√©es qui rendent les mod√®les plus pr√©cis dans des environnements √† faibles ressources. Par exemple, une m√©thode r√©cente appel√©e <a href="https://arxiv.org/abs/2002.05709">SimCLR</a> (dirig√©e par Geoffrey Hinton) propose un cadre pour am√©liorer les r√©sultats de la classification des images dans diff√©rents contextes tels que l‚Äôapprentissage par transfert et l‚Äôapprentissage semi-supervis√©.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png" alt="" /></p>

<p><a href="https://arxiv.org/abs/2002.05709"><em>source</em></a></p>

<h1 id="outils-et-jeux-de-donn√©es-Ô∏è">Outils et jeux de donn√©es ‚öôÔ∏è</h1>

<p><strong><em>Les libraries li√©es √† JAX</em></strong></p>

<p><br />
<a href="https://github.com/google/jax">JAX</a> est une nouvelle biblioth√®que qui combine NumPy et la diff√©renciation automatique pour mener des recherches de ML de haut niveau. Afin de simplifier les pipelines utilisant JAX, DeepMind a publi√© <a href="https://github.com/deepmind/dm-haiku">Haiku</a> et <a href="https://github.com/deepmind/rlax">RLax</a>. RLax simplifie l‚Äôimpl√©mentation de mod√®les bas√©s sur l‚Äôapprentissage par renforcement et Haiku simplifie la construction de r√©seaux neuronaux en utilisant des mod√®les de programmation orient√©s objet.</p>

<p><br />
<strong><em>Un outil de traitement des donn√©es Wikip√©dia</em></strong></p>

<p><br />
<a href="https://github.com/epfl-lts2/sparkwiki">Sparkwiki</a> est un outil permettant de traiter les donn√©es de Wikip√©dia. Cette version s‚Äôinscrit dans le cadre de nombreux efforts visant √† permettre des recherches int√©ressantes en mati√®re d‚Äôanalyse comportementale, telles que la <a href="https://arxiv.org/abs/2002.06885">capture des tendances et des biais linguistiques dans les diff√©rentes √©ditions linguistiques de Wikip√©dia</a>. Les auteurs ont d√©couvert qu‚Äôind√©pendamment de la langue, le comportement de navigation des utilisateurs de Wikip√©dia montre qu‚Äôils ont tendance √† partager des int√©r√™ts communs comme par exemples les films, la musique et le sport, mais que les diff√©rences deviennent plus apparentes avec les √©v√©nements locaux et les particularit√©s culturelles.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg" alt="" /></p>

<p><br />
<strong><em>Mise √† jour de la librairie Transformers</em></strong></p>

<p><br />
Une <a href="https://github.com/huggingface/transformers/releases/tag/v2.5.0">nouvelle version</a> de la librairie Transformers d‚ÄôHugging Face est disponible. Elle comprend l‚Äôint√©gration de leur librairie Tokenizer qui vise √† acc√©l√©rer des mod√®les comme BERT, RoBERTa, GPT2, et d‚Äôautres mod√®les construits par la communaut√©.</p>

<h1 id="ethique-en-ia-">Ethique en IA üö®</h1>

<p><strong><em>Consid√©rations √©thiques pour les mod√®les de NLP et de ML</em></strong></p>

<p><br />
Dans un nouvel <a href="https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender">√©pisode</a> des <a href="https://soundcloud.com/nlp-highlights">NLP Highlights</a>, Emily Bender et les intervenants parlent de certaines consid√©rations √©thiques qui peuvent se poser lors du d√©veloppement de mod√®les de NLP dans un contexte d‚Äôutilisation universitaire et grand public. Parmi les sujets abord√©s, citons les consid√©rations √©thiques lors de la conception des t√¢ches de NLP, les approches de collecte de donn√©es et, finalement, la publication des r√©sultats.
En plus de toutes les consid√©rations ci-dessus, une pr√©occupation qui est toujours discut√©e dans la communaut√© de l‚ÄôIA est de se concentrer trop sur l‚Äôoptimisation d‚Äôune mesure, ce qui va √† l‚Äôencontre des fondements de ce que l‚ÄôIA vise √† atteindre (c√†d une IA g√©n√©rale). Rachel Thomas et David Uminsky discutent des erreurs possibles en <a href="https://arxiv.org/abs/2002.08512">analysant de mani√®re approfondie</a> diff√©rents cas d‚Äôutilisation. Ils proposent √©galement un cadre simple pour att√©nuer le probl√®me, qui implique l‚Äôutilisation et la combinaison de plusieurs mesures, suivies par l‚Äôimplication des personnes directement concern√©es par la technologie.</p>

<h1 id="articles-et-blog-Ô∏è">Articles et Blog ‚úçÔ∏è</h1>

<p><strong><em>Le GPT2 annot√©</em></strong></p>

<p><br />
Aman Arora a r√©cemment publi√© un article sur son blog, intitul√© le ‚Äú<a href="https://amaarora.github.io/2020/02/18/annotatedGPT2.html">The Annotated GPT-2</a>‚Äù, qui explique le fonctionnement interne du GPT-2. Son approche s‚Äôinspire de ‚Äú<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a>‚Äù qui a adopt√© une approche d‚Äôannotation pour expliquer les parties importantes du mod√®le par le biais de code et d‚Äôexplications faciles √† suivre. Aman a fait de gros efforts pour r√©impl√©menter le GPT-2 d‚ÄôOpenAI en utilisant PyTorch et la biblioth√®que Transformers de Hugging Face.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png" alt="" /></p>

<p><a href="https://amaarora.github.io/2020/02/18/annotatedGPT2.html"><em>source</em></a></p>

<p><br />
<strong><em>Au-del√† de BERT ?</em></strong></p>

<p><br />
Sergi Castella expose son <a href="https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1">point de vue</a> sur ce qui se trouve au-del√† de BERT. Les principaux sujets abord√©s sont l‚Äôam√©lioration des mesures, la fa√ßon dont la librairie Transformers d‚ÄôHuggingfFace permet de faire des recherches, les jeux de donn√©es int√©ressants √† consulter, etc‚Ä¶</p>

<p><br />
<strong><em>Op√©rateur de compression matricielle</em></strong></p>

<p><br />
TensorFlow blog a publi√© un <a href="https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016">article</a> expliquant les techniques et l‚Äôimportance de la compression des matrices dans un mod√®le de r√©seau neuronal profond. La compression des matrices peut aider √† construire des mod√®les petits plus efficaces qui peuvent √™tre incorpor√©s dans des appareils tels que les t√©l√©phones et les assistants vocaux. En se concentrant sur la compression des mod√®les par des m√©thodes telles que la low-rank-approximation et la quantization, nous n‚Äôavons pas besoin de compromettre la qualit√© du mod√®le.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png" alt="" /></p>

<p><a href="https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016"><em>source</em></a></p>

<h1 id="education-">Education üéì</h1>

<p><strong><em>Les bases du NLP</em></strong></p>

<p><br />
Elvis a publi√© une √©bauche du chapitre 1 de sa nouvelle s√©rie intitul√©e ‚Äú<a href="https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684">Les bases du NLP</a>‚Äù. Il enseigne les concepts du NLP en partant des bases tout en partageant les meilleures pratiques, les r√©f√©rences importantes, et les erreurs courantes √† √©viter. Un <a href="https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ">Google Colab</a> est disponible et le projet sera maintenu <a href="https://github.com/dair-ai/nlp_fundamentals">ici</a>.</p>

<p><br />
<strong><em>[Online] Review/Discussion: Part I Mathematical Foundations Reading Session</em></strong></p>

<p><br />
Machine Learning Tokyo organise une discussion en ligne sur les chapitres qui ont √©t√© couverts lors de leurs r√©centes sessions d‚Äô√©tude en ligne. Le groupe avait auparavant √©tudi√© des chapitres bas√©s sur le livre intitul√© <a href="https://mml-book.github.io/">Mathematics For Machine Learning</a>, √©crit par Marc Peter Deisenroth, A Aldo Faisal et Cheng Soon Ong. L‚Äô√©v√©nement est pr√©vu pour le 8 mars 2020.</p>

<p><br />
<strong><em>Recommandations de livres</em></strong></p>

<p><br />
Dans une partie pr√©c√©dente, nous avons discut√© de l‚Äôimportance de la compression matricielle pour la construction de petits mod√®les de ML. Si vous souhaitez en savoir plus sur la fa√ßon de construire des r√©seaux neuronaux profonds plus petits pour les syst√®mes embarqu√©s, consultez cet excellent livre intitul√© <a href="https://tinymlbook.com/?linkId=82595412">TinyML</a>, √©crit par Pete Warden et Daniel Situnayake.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg" alt="" /></p>

<p><a href="https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043"><em>source</em></a></p>

<p><br />
Un autre livre int√©ressant √† surveiller et qui est √† para√Ætre est ‚Äú<a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527">Deep Learning for Coders with fastai and PyTorch‚Äù : AI Applications Without a PhD</a>‚Äù de Jeremy Howard et Sylvain Gugger. Ce livre vise √† fournir les bases math√©matiques n√©cessaires pour construire et former des mod√®les permettant d‚Äôaborder des t√¢ches dans les domaines de computer vision et du NLP.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg" alt="" /></p>

<p><a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527"><em>source</em></a></p>

<h1 id="mentions-sp√©ciales-Ô∏è">Mentions sp√©ciales ‚≠êÔ∏è</h1>

<p><a href="https://arxiv.org/abs/1909.06576">Torchmeta</a> est une librairie qui permet d‚Äôutiliser facilement des chargeurs de donn√©es connexes pour la recherche sur le m√©ta-apprentissage. Elle a √©t√© r√©dig√©e par Tristan Deleu.</p>

<p><br />
Manuel Tonneau a √©crit un <a href="https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter">article</a> offrant un regard plus approfondi sur certains des m√©canismes impliqu√©s dans la mod√©lisation du langage. Parmi les sujets abord√©s, citons la greedy recherche, la beam recherche et l‚Äô√©chantillonnage de noyaux.</p>

<p><br />
Le MIT <a href="http://introtodeeplearning.com/">publie</a> le programme complet et le calendrier du cours intitul√© ‚ÄúIntroduction to Deep Learning‚Äù. L‚Äôobjectif est de publier chaque semaine des vid√©os et des diapositives.</p>

<p><br />
Apprenez comment entra√Æner un mod√®le de reconnaissance d‚Äôentit√©s nomm√©es (NER) en utilisant une approche bas√©e sur <a href="https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py">Transformers</a> en moins de 300 lignes de code. Vous pouvez trouver le programme Google Colab qui l‚Äôaccompagne <a href="https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn">ici</a>.</p>

<hr />

<p>Vous pouvez retrouver la pr√©c√©dente newsletter <a href="https://dair.ai/NLP_Newsletter_-4_-FR/">ici</a></p>

<p><br />
Si vous avez des jeux de donn√©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine √©dition de la newletter, n‚Äôh√©sitez pas √† me contacter √† ellfae@gmail.com ou par message sur <a href="https://twitter.com/omarsar0">Twitter</a>.</p>

<p><br />
<a href="https://dair.ai/newsletter/">Abonnez-vous</a> pour recevoir les prochains num√©ros dans votre bo√Æte mail.</p>

      <hr />
      <footer role="contentinfo">
        <div class="social-share">
  <!-- Go to www.addthis.com/dashboard to customize your tools --> 
  <div class="addthis_inline_share_toolbox"></div>
  <!--
  <h4>Share on</h4>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=https://dair.ai/NLP_Newsletter_-5_-FR/" class="twitter" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=https://dair.ai/NLP_Newsletter_-5_-FR/" class="facebook" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=https://dair.ai/NLP_Newsletter_-5_-FR/" class="google-plus" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
  </ul>-->
</div><!-- /.social-share -->
        <p class="byline"><strong>NLP Newsletter [FR] #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,‚Ä¶</strong> was published on <time datetime="2020-03-09T00:00:00-05:00">March 09, 2020</time>.</p>
        
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dair-ai'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<!--
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->

      </footer>
    </div><!-- /.article-wrap -->
  
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
  
  </article>
</div><!-- /#main -->

<div class="footer-wrap">
  
  <div class="related-articles">
  <h4>You might also enjoy <small class="pull-right">(<a href="https://dair.ai/posts/">View all posts</a>)</small></h4>
    <ul>
    
      <li><a href="https://dair.ai/NLP_Newsletter_NLP_7-ZH-.md/" title="NLP ÁÆÄÊä•ÔºàIssue#7Ôºâ: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶">NLP ÁÆÄÊä•ÔºàIssue#7Ôºâ: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶</a></li>
    
      <li><a href="https://dair.ai/NLP_Newsletter_NLP_7/" title="NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶">NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶</a></li>
    
      <li><a href="https://dair.ai/NLP_Newsletter_-7_-FR/" title="NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶">NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶</a></li>
    
    </ul>
    <hr />
  </div><!-- /.related-articles -->
  
  <footer>
    

<span>&copy; 2020 dair.ai. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://dair.ai/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://dair.ai/assets/js/scripts.min.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  var _gaq = _gaq || [];
  var pluginUrl =
 '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
  _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
  _gaq.push(['_setAccount', 'UA-158959084-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>


  
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dair-ai'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<!--
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->




</body>
</html>
