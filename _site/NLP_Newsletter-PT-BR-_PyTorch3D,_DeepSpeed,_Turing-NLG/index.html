<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->

<head>
<meta charset="utf-8">
<title>NLP Newsletter [PT-BR]: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,‚Ä¶ &#8211; dair.ai</title>
<meta name="description" content="Essa edi√ß√£o cobre t√≥picos como modelos de linguagem maiores, avan√ßos na pesquisa de Deep Learning em 3D, benchmarks para question answering multi-idiomas, auditoria de sistemas de IA e muito mais">
<meta name="keywords" content="nlp_newsletter">


<!-- Twitter Cards -->
<meta name="twitter:title" content="NLP Newsletter [PT-BR]: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,‚Ä¶">
<meta name="twitter:description" content="Essa edi√ß√£o cobre t√≥picos como modelos de linguagem maiores, avan√ßos na pesquisa de Deep Learning em 3D, benchmarks para question answering multi-idiomas, auditoria de sistemas de IA e muito mais">
<meta name="twitter:site" content="@dair_ai">
<meta name="twitter:creator" content="@vic_garritano">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:4000/images/nlp_newsletter_4.png">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP Newsletter [PT-BR]: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,‚Ä¶">
<meta property="og:description" content="Essa edi√ß√£o cobre t√≥picos como modelos de linguagem maiores, avan√ßos na pesquisa de Deep Learning em 3D, benchmarks para question answering multi-idiomas, auditoria de sistemas de IA e muito mais">
<meta property="og:url" content="http://localhost:4000/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG/">
<meta property="og:site_name" content="dair.ai">

<meta property="og:image" content="http://localhost:4000/images/nlp_newsletter_4.png">







<link rel="canonical" href="http://localhost:4000/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG/">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="dair.ai Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="http://localhost:4000/assets/js/vendor/html5shiv.min.js"></script>
	<script src="http://localhost:4000/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body class="post">

<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.8&appId=1537934899816329";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<!-- Go to www.addthis.com/dashboard to customize your tools -->
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-4e43ef4f23bf37b0"></script>

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="http://localhost:4000/">dair.ai</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				    
				    <li><a href="http://localhost:4000/posts/" >Blog ‚úçÔ∏è</a></li>
				
				    
				    <li><a href="http://localhost:4000/about/" >About ‚ÑπÔ∏è</a></li>
				
				    
				    <li><a href="http://localhost:4000/newsletter/" >NLP Newsletter üóûÔ∏è</a></li>
				
				    
				    <li><a href="http://localhost:4000/projects/" >Projects üí°</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai" target="_blank">GitHub üìÅ</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai/dair-ai.github.io/contribute" target="_blank">Contribute ‚ú®</a></li>
				
				    
				    <li><a href="https://medium.com/dair-ai" target="_blank">Medium üì∞</a></li>
				
				    
				    <li><a href="https://nlpoverview.com/" target="_blank">NLP Overview üìò</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai/nlp_highlights" target="_blank">2019 NLP Highlights (PDF) üî•</a></li>
				
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->



<div id="main" role="main">
  <div class="article-author-side">
    

<div itemscope itemtype="http://schema.org/Person">


	<img src="http://localhost:4000/images/victor.jpg" class="bio-photo" alt="Victor Garritano Noronha bio photo">


  <h3 itemprop="name">Victor Garritano Noronha</h3>
  <p></p>

  <a href="http://twitter.com/vic_garritano" class="author-social" target="_blank"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a>
  
  
  
  
  
  
  <a href="http://github.com/VictorGarritano" class="author-social" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a>
  
  
  
  
  
  
  
  
  
  
</div>

  </div>
  <article class="post">
    <div class="headline-wrap">
      
        
          <h1><a href="http://localhost:4000/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG/" rel="bookmark" title="NLP Newsletter [PT-BR]: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,‚Ä¶">NLP Newsletter [PT-BR]: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,‚Ä¶</a></h1>
        
      
    </div><!--/ .headline-wrap -->

    
    <div class="article-wrap">
      <p><img src="https://cdn-images-1.medium.com/max/1200/1*3vNKhz6K-oGQ8aLi3mo84Q.png" alt="" /></p>

<h1 id="publica√ß√µes-">Publica√ß√µes üìô</h1>

<p><strong><em>Turing-NLG: A 17-billion-parameter language model by Microsoft</em></strong></p>

<p><br />
O Turing Natural Language Generation (T-NLG) √© um modelo de linguagem com 17 bilh√µes de par√¢metros <a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">proposto</a> pelo grupo de pesquisa em Intelig√™ncia Artificial da Microsoft. O T-NLG √© composto por 78 camadas, baseado na arquitetura dos Transformers, que superou o estado da arte anterior (atribuido ao <a href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a> da Nvidia) considerando a Perplexidade na base de dados WikiText-103. O modelo foi testado em diversas tarefas, como <em>question answering</em> e sumariza√ß√£o abstrata, onde foram observados comportamentos interessantes e desej√°veis para modelos dessa categoria, como <em>‚Äúzero shot‚Äù question answering</em> (onde o modelo responde a um pergunta sem estar ciente do contexto explicitamente), e baixa necessidade de bases previamente anotadas, para as tarefas citadas anteriormente. O modelo pode ser treinado gra√ßas √† biblioteca DeepSpeed, utilizando o esquema de otimiza√ß√£o ZeRO (que tamb√©m aparece nessa edi√ß√£o da <em>Newsletter</em>).</p>

<!-- Turing Natural Language Generation (T-NLG) is a 17-billion-parameter language model [proposed](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/) by Microsoft AI researchers. Besides being the largest known language model to date (depicted in the figure below), T-NLG is a 78-layers Transformer-based language model that outperforms the previous state-of-the-art results (held by NVIDIA‚Äôs [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)) on WikiText-103 perplexity. It was tested on a variety of tasks such as question answering and abstractive summarization while demonstrating desirable benefits such as zero short question capabilities and minimizing supervision, respectively. The model is made possible by a training optimization library called DeepSpeed with ZeRO, which is also featured later in this newsletter. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*CAZm7uj8EaupnvnJ.png" alt="" /></p>

<p><em>Modelos de Linguagem e suas quantidades de par√¢metros‚Ää‚Äî</em>‚Ää<a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/"><em>fonte</em></a></p>

<p><br />
<strong><em>Neural based Dependency Parsing</em></strong></p>

<p><br />
A pesquisadora Miryam de Lhoneux liberou recentemente sua tese de Doutorado, entitulada ‚Äú<a href="http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1357373&amp;dswid=7905">Linguistically Informed Neural Dependency Parsing for Typologically Diverse Languages</a>‚Äù. O trabalho desenvolvido prop√µe a utiliza√ß√£o de abordagens baseadas em redes neurais para a tarefa de an√°lise de depend√™ncias (<a href="http://nlpprogress.com/english/dependency_parsing.html">dependency parsing</a>) em idiomas com diversas tipologias (l√≠nguas que apresentam padr√µes funcionais e estruturais diferentes). O trabalho apresentado pela autora indica que a incorpora√ß√£o de RNNs e camadas recursivas nos analisadores pode ser ben√©fica para a tarefa, uma vez que essas arquiteturas podem indicar o conhecimento lingu√≠stico necess√°rio √† an√°lise. Outras extens√µes incluem a utiliza√ß√£o de analisadores poliglotas e estrat√©gias de compartilhamento de par√¢metros para a an√°lise de linguagens correlacionadas ou descorrelacionadas.</p>

<!-- Miryam de Lhoneux released her Ph.D. thesis titled ‚Äú[Linguistically Informed Neural Dependency Parsing for Typologically Diverse Languages](http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1357373&dswid=7905)‚Äù. This work is about using neural approaches for [dependency parsing](http://nlpprogress.com/english/dependency_parsing.html) in typologically diverse languages (i.e. languages that construct and express meaning in structurally different ways). This paper reports that RNNs and recursive layers could be beneficial for incorporating into parsers as they help to inform models with important linguistic knowledge needed for parsing. Other ideas include the use of polyglot parsing and parameter sharing strategies for parsing in related and unrelated languages. -->

<p><br />
<strong><em>End-to-end Cloud-based Information Extraction with BERT</em></strong></p>

<p><br />
Um time de pesquisadores publicou um <a href="https://arxiv.org/abs/2002.01861">artigo</a> descrevendo como modelos de Transformers, como o BERT, podem auxiliar sistemas de extra√ß√£o de informa√ß√£o de ponta a ponta em documentos de dom√≠nios espec√≠ficos, como documenta√ß√£o regulat√≥ria e de concess√£o de propriedades. Esse tipo de trabalho, al√©m de otimizar opera√ß√µes de neg√≥cios, demonstra a efici√™ncia e aplicabilidade de modelos baseados no BERT em cen√°rios onde bases de dados anotadas s√£o extremamente limitadas. Uma plataforma na nuvem √© apresentada e discutida, assim como os detalhes de sua implementa√ß√£o (ver figura abaixo).</p>

<!-- A team of researchers published a [paper](https://arxiv.org/abs/2002.01861) describing how Transformer models like BERT can help for end-to-end information extraction in domain-specific business documents such as regulatory filings and property lease agreements. Not only can this type of work help to optimize business operations but it also shows the applicability and effectiveness of BERT-based models on regimes with very low annotated data. An application, and its implementation details, that operates on the cloud is also proposed and discussed (see figure below). -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*KqViSLhP0otleDY-XFy3Bg.png" alt="" /></p>

<p><a href="https://arxiv.org/abs/2002.01861"><em>fonte</em></a></p>

<p><br />
<strong><em>Question Answering Benchmark</em></strong></p>

<p><br />
Em <a href="https://arxiv.org/abs/2001.11770v1">Wolfson et al. (2020)</a>, apresenta-se um <em>benchmark</em> para a tarefa de <em>question understanding</em> e um m√©todo para decomposi√ß√£o de quest√µes, uma etapa necess√°ria para a determina√ß√£o de uma resposta apropriada. Os autores recorreram √† um servi√ßo de <em>crowdsourcing</em> para a cria√ß√£o da base anotada de decomposi√ß√£o de quest√µes. Com objetivo de demonstar a viabilidade e aplicabilidade do m√©todo proposto, os autores mostraram que √© poss√≠vel melhorar o desempenho de modelos utilizando essa t√©cnica sobre a base de dados HotPotQA.</p>

<!-- [Wolfson et al. (2020)](https://arxiv.org/abs/2001.11770v1) published a question understanding benchmark and a method for breaking down a question that is necessary for computing an appropriate answer. They leverage crowdsourcing to annotate the required steps needed to break down questions. To show the feasibility and applicability of the approach, they improve on open-domain question answering using the HotPotQA dataset. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*AztG-Inqt6LGQ87lSufRcw.png" alt="" /></p>

<p><em>‚ÄúQuest√µes de diferentes fontes de informa√ß√µes exibem uma estrutura composicional semelhante. Quest√µes em linguagem natural (parte de cima) s√£o decompostas seguindo a metodologia QDMR (meio) e deterministicamente mapeadas para uma linguagem pseudo-formal (parte de baixo).‚Äù‚Ää‚Äî</em>‚Ää<a href="https://arxiv.org/pdf/2001.11770v1.pdf"><em>fonte</em></a>
<!-- Questions over different sources share a similar compositional structure. Natural language questions from multiple sources (top) are annotated with the QDMR formalism (middle) and deterministically mapped into a pseudo-formal language (parte inferior). --></p>

<p><br />
<strong><em>Radioactive data: tracing through training</em></strong></p>

<p><br />
Membros da equipe de pesquisa em IA do Facebook publicaram recentemente um <a href="https://ai.facebook.com/blog/using-radioactive-data-to-detect-if-a-data-set-was-used-for-training/">trabalho interessante</a> que prop√µe a marca√ß√£o de imagens (referenciadas como <em>radioactivate data</em>) de tal maneira que seja poss√≠vel verificar se uma determinada base de dados foi utilizada no treinamento de um modelo de Aprendizado de M√°quina. Os autores conclu√≠ram que √© poss√≠vel utilizar uma marca√ß√£o mais robusta, que move as <em>features</em> para uma determinada dire√ß√£o, e que pode ser empregada para auxiliar a detec√ß√£o de dados ‚Äúradioativos‚Äù, mesmo quando apenas <strong>1%</strong> destes est√£o presentes na base de treinamento.</p>

<p>Essa √© uma tarefa bem desafiadora, uma vez que qualquer modifica√ß√£o nos dados pode potencialmente prejudicar o desempenho do modelo. De acordo com os autores, o trabalho proposto pode ‚Äú<em>ajudar pesquisadores e engenheiros a monitorar quais bases de dados foram utilizadas no treinamento de um modelo, com o objetivo de compreender melhor como bases de dados de diferentes naturezas influenciam o desempenho de diversas redes neurais</em>‚Äù. Parece uma tarefa crucial para aplica√ß√µes <em>mission-critical</em>. Confira o artigo completo <a href="https://arxiv.org/pdf/2002.00937.pdf">aqui</a>.</p>

<!-- Facebook AI researchers recently published [an interesting work](https://ai.facebook.com/blog/using-radioactive-data-to-detect-if-a-data-set-was-used-for-training/) that aims to mark images (referred to as radioactive data) so as to verify if that particular data set was used for training the ML model. They found that it is possible to use a clever marker that moves features towards a direction, which the model uses to help detect the usage of radioactive data even when only 1 percent of the training data is radioactive. This is challenging since any change in the data can potentially degrade the model accuracy. According to the authors, this work can ‚Äú*help researchers and engineers to keep track of which data set was used to train a model so they can better understand how various data sets affect the performance of different neural networks*‚Äù. It seems like an important approach in mission-critical ML applications. Check out the full paper [here](https://arxiv.org/pdf/2002.00937.pdf). -->

<p><br />
<strong><em>REALM: Retrieval-Augmented Language Model Pre-Training</em></strong></p>

<p><br />
O <a href="https://kentonl.com/pub/gltpc.2020.pdf">REALM</a> √© um m√©todo de recupera√ß√£o em larga escala baseado em redes neurais, que faz uso de bases de conhecimento textual para pr√©-treinar um modelo de linguagem de maneira n√£o-supervisionada. Essencialmente, o objetivo da abordagem √© capturar o conhecimento,  de uma maneira mais interpret√°vel, expondo o modelo √† conhecimentos gerais utilizados durante o processo de treinamento e infer√™ncia atrav√©s do <em>backpropagation</em>. As bases onde o m√©todo foi testado e avaliado incluem <em>benchmarks</em> de <em>open-domain question answering</em>. Al√©m do aumento observado na acur√°cia do modelo, outros benef√≠cios incluem modularidade e interpretabilidade dos componentes.</p>

<!-- [REALM](https://kentonl.com/pub/gltpc.2020.pdf) is a large-scale neural-based retrieval approach that makes use of a corpus of textual knowledge to pre-train a language model in an unsupervised manner. This approach essentially aims to capture knowledge in a more interpretable way by exposing the model to world knowledge that is used for training and predictions via backpropagation. Tasks approached and evaluated using REALM include open-domain question answering benchmarks. Besides the improvements in the accuracy of the model, other benefits include the modularity and interpretability components. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*MJO-yzCwsB5ydKGz7hKHVA.png" alt="" /></p>

<p><a href="https://kentonl.com/pub/gltpc.2020.pdf"><em>fonte</em></a></p>

<h1 id="criatividade-e-sociedade-">Criatividade e Sociedade üé®</h1>

<p><strong><em>Apresenta√ß√µes remotas de artigos e p√¥steres em confer√™ncias cient√≠ficas</em></strong></p>

<p><br />
Durante a semana passada, uma <a href="https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences">peti√ß√£o</a> circulou na internet, reivindicando a permiss√£o para apresenta√ß√µes remotas de artigos e p√¥steres em confer√™ncias cient√≠ficas, como as relacionadas √† Aprendizado de M√°quina. Para saber mais, acesse <a href="https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences">change.org</a>. Parece que Yoshua Bengio, um dos pioneiros do <em>Deep Learning</em>, est√° convocando as pessoas √† assinar a peti√ß√£o. Ele deixou isso bem claro em seu novo <a href="https://yoshuabengio.org/2020/02/10/fusce-risus/">blog</a>.</p>

<!-- The past week there was the circulation of a [petition](https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences) to allow for remote paper and poster presentations at scientific conferences like ML related ones. Go read more about it on [change.org](https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences). It seems Yoshua Bengio, a pioneer in deep learning, is advocating for people to go and sign the petition. He made this clear in his new [blog](https://yoshuabengio.org/2020/02/10/fusce-risus/). -->

<p><br />
<strong><em>Abstra√ß√£o e Desafios de Racioc√≠nio</em></strong></p>

<p><br />
Fran√ßois Chollet postou recentemente uma <a href="https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview">competi√ß√£o no Kaggle</a> onde ele disponibilizou o <em>Abstraction and Reasoning Corpus (ARC)</em>, uma base de dados que tem como objetivo encorajar os usu√°rios a desenvolver sistemas de IA para resolver tarefas √†s quais nunca foram expostos. A esperan√ßa √© que essa competi√ß√£o seja o pontap√© inicial para  a constru√ß√£o de modelos mais robustos de IA, capazes de resolver novos problemas por conta pr√≥pria de maneira mais eficiente e r√°pida, ajudando na resolu√ß√£o de aplica√ß√µes mais desafiadoras do mundo real como a melhoria de carros aut√¥nomos que operam em ambientes diversos e extremos.</p>

<!-- Fran√ßois Chollet has recently posted a [Kaggle competition](https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview) where he released the Abstraction and Reasoning Corpus (ARC) that aims to encourage users to create AI systems that can solve reasoning tasks it has never been exposed to. The hope is to begin to build more robust AI systems that are able to better and quickly solve new problems on its own which could help to address the more challenging real-world applications such as improving self-driving cars that operate in extreme and diverse environments. -->

<p><br />
<strong><em>Publica√ß√µes de Aprendizado de M√°quina e Processamento de Linguagem Natural em 2019</em></strong></p>

<p><br />
Marek Rei liberou sua <a href="https://www.marekrei.com/blog/ml-and-nlp-publications-in-2019/">an√°lise anual</a> com estat√≠sticas das publica√ß√µes sobre ML e NLP em 2019. As confer√™ncias consideradas nas an√°lises foram ACL, EMNLP, NAACL, EACL, COLING, TACL, CL, CoNLL, NeurIPS, ICML, ICLR, e AAAI.</p>

<p><br />
<strong><em>Growing Neural Cellular Automata</em></strong></p>

<p><br />
Morfog√™nese √© um processo de auto-organiza√ß√£o pelo qual alguns animais, como as salamandras, podem regenerar partes de seus corpos que sofreram danos. O processo √© robusto a perturba√ß√µes e adaptativo na natureza. Inspirado nesse esse fen√¥meno biol√≥gico e com a necessidade de uma melhor compreens√£o desse mecanismo, pesquisadores publicaram um <a href="https://distill.pub/2020/growing-ca/">trabalho</a> entitulado ‚Äú<em>Growing Neural Cellular Automata</em>‚Äù, que adota um modelo diferenci√°vel para o processo de morfog√™nese buscando replicar os comportamentos e propriedades de sistemas de auto-repara√ß√£o.</p>

<p>Espera-se que o processo seja capaz de criar m√°quinas ‚Äúauto-repar√°veis‚Äù que possuam a mesma robustez e maleabilidade dos organismos biol√≥gicos. Al√©m disso, o m√©todo pode possibilitar um melhor entendimento do processo de regenera√ß√£o em si. √Åreas que podem se beneficiar com essa pesquisa incluem a medicina regenerativa e a modelagem de sistemas sociais e biol√≥gicos.</p>

<!-- Morphogenesis is a self-organization process by which some creatures such as salamanders can regenerate or repair body damage. The process is robust to perturbations and adaptive in nature. Inspired by this biological phenomenon and a need to understand the process better, researchers published a [paper](https://distill.pub/2020/growing-ca/) titled ‚ÄúGrowing Neural Cellular Automata‚Äù, which adopts a differentiable model for morphogenesis that aims to replicate behaviors and properties of self-repairing systems. The hope is to be able to build self-repairing machines that possess the same robustness and plasticity as biological life. In addition, it would help to better understand the process of regeneration itself. Applications that can benefit include regenerative medicine and modeling of social and biological systems. -->
<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*2p62h1RaHD6d11LX8olnTA.png" alt="" /></p>

<p><a href="https://distill.pub/2020/growing-ca/"><em>fonte</em></a></p>

<p><br />
<strong><em>Visualiza√ß√£o do mecanismo de Aten√ß√£o do Transformer</em></strong></p>

<p><br />
Hendrik Strobelt compartilhou esse <a href="https://github.com/SIDN-IAP/attnvis">reposit√≥rio bem interessante</a> que mostra como construir rapidamente uma  visualiza√ß√£o simples e interativa da Aten√ß√£o do Transformer atrav√©s de uma aplica√ß√£o web utilizando as bibliotecas Hugging Face e d3.js.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*lMaZGDRJUI1Qcv7T5AdhlQ.gif" alt="" /></p>

<p><a href="https://github.com/SIDN-IAP/attnvis"><em>fonte</em></a></p>

<p><br />
<strong><em>SketchTransfer: A Challenging New Task for Exploring Detail-Invariance and the Abstractions Learned by Deep Networks</em></strong></p>

<!-- SketchTransfer proposes a new task to test the ability of deep neural networks to support invariance in the presence/absence of details. It has long been debated that deep networks cannot generalize to variations that have not yet been seen during training, something humans can do with relative ease such as dealing with the missing visual details when watching cartoons. The paper discusses and releases a dataset to help researchers carefully study the ‚Äúdetail-invariance‚Äù problem by providing unlabelled sketch images and labeled examples of real images. -->

<p><br />
O SketchTransfer prop√µe uma nova tarefa que tem por objetivo testar a habilidade de redes neurais profundas em manter a capacidade invari√¢ncia frente a presen√ßa/aus√™ncia de detalhes. Um debate de longa data existe acerca da inabilidade de redes profundas em generalizar varia√ß√µes que n√£o foram vistas durante o treinamento, algo que os humanos conseguem fazer com relativa facilidade em situa√ß√µes como, por exemplo, a falta de alguns detalhes visuais quando assistimos desenhos. O trabalho discute e disponibiliza uma base de dados esbo√ßos n√£o-anotados e imagens reais anotadas, permitindo que os pesquisadores possam estudar o problema de ‚Äú<em>detail-invariance</em>‚Äù de maneira bastante cuidadosa.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*jdYuMoHiu2yya5rHzZyjwQ.png" alt="" /></p>

<p><a href="https://arxiv.org/pdf/1912.11570.pdf"><em>fonte</em></a></p>

<h1 id="bibliotecas-e-bases-de-dados-Ô∏è">Bibliotecas e Bases de Dados ‚öôÔ∏è</h1>

<p><strong><em>DeepSpeed + ZeRO</em></strong></p>

<p><br />
A Microsoft liberou um pacote para otimiza√ß√£o chamado DeepSpeed, compat√≠vel com o PyTorch, que possibilita o treinamento de modelos com 100 bilh√µes de par√¢metros. A biblioteca d√° destaque a 4 importantes aspectos do processo de treinamento: <em>opera√ß√£o em escala</em>, <em>velocidade</em>, <em>custo</em>, e <em>usabilidade</em>. A DeepSeed foi <a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">liberada</a> junto com o ZeRO, uma tecnologia que otimiza a utiliza√ß√£o da mem√≥ria, e que possibilita o emprego do Deep Learning em larga escala de maneira distribu√≠da com as atuais tecnologias de GPU, al√©m de melhorar o <em>throughput</em> em 3-5 vezes em rela√ß√£o √† melhor solu√ß√£o atual. A tecnologia possibilita o treinamento de modelos de tamanho arbitr√°rio que podem ocupar a mem√≥ria total dispon√≠vel, distribu√≠da pelos diversos dispositivos na infra-estrutura.</p>

<!-- Microsoft open sources a training optimization library called DeepSpeed, which is compatible with PyTorch and can enable the ability to train a 100-billion-parameter model. The library focuses on four important aspects of training a model: *scale*, *speed*, *cost*, and *usability*. DeepSpeed was [released](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/) together with ZeRO which is a memory optimization technology for enabling large-scale distributed deep learning in current GPU technology while improving throughput three to five times more than the best current system. ZeRO allows the training of models with any arbitrary size that can fit in the aggregated available memory in terms of shared model states. -->
<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*MXDI1f3cSBrY5w2g.gif" alt="" /></p>

<p><a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/"><em>fonte</em></a></p>

<!-- ***A library for conducting fast and efficient 3D deep learning research*** -->

<p><br />
<strong><em>Deep Learning em Superf√≠cies 3D de Maneira R√°pida e Eficiente</em></strong></p>

<!-- [PyTorch3D](https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/) is an open-source toolkit for 3D based deep learning research. This PyTorch library aims to help with the support and understanding of 3D data in deep learning systems. The library consists of fast and optimized implementations of frequently used 3D operators and loss functions. It also comes with a modular differentiable renderer which helps to conduct research on complex 3D inputs and supports high-quality 3D predictions. -->
<p><br />
A <a href="https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/">PyTorch3D</a> √© uma biblioteca em c√≥digo aberto para a pesquisa de Deep Learning aplicado √† superf√≠cies 3D. Esse pacote, baseado no PyTorch, busca auxiliar no suporte e entendimento de dados em 3D aplicados √† redes neurais. A biblioteca consiste de implementa√ß√µes eficientes e otimizadas de operadores e fun√ß√µes de custo 3D comumente utilizadas. Um renderizador diferenci√°vel modular tamb√©m est√° dispon√≠vel, que pode ser √∫til durante a pesquisa e explora√ß√£o de entradas 3D com padr√µes complexos e na gera√ß√£o de predi√ß√µes de alta qualidade.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*VbspKMmPBUsgpdnIkd5jYA.png" alt="" /></p>

<p><a href="https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/"><em>fonte</em></a></p>

<!-- ***Managing Configuration of ML projects*** -->

<p><br />
<strong><em>Gerenciamento de Configura√ß√µes para projetos de ML</em></strong></p>

<p><br />
Hydra √© uma ferramenta de configura√ß√£o escrita em Python, que auxilia no gerenciamento de projetos complexos de ML de maneira mais eficiente. O prop√≥sito √© dar suporte a pesquisadores que utilizam o PyTorch, oferencedo a possibilidade de reutiliza√ß√£o de configura√ß√µes de projetos de maneira funcional. O benef√≠cio principal oferecido √© a possibilidade do programador <em>compor configura√ß√µes como comp√µe-se c√≥digo</em>, o que permite a r√°pida altera√ß√£o de arquivos de configura√ß√£o. A Hydra pode ainda gerenciar automaticamente o diret√≥rio de trabalho que armazena as sa√≠das do seu projeto de ML, o que √© bem √∫til quando precisamos salvar e acessar diversos resultados provenientes de m√∫ltiplos <em>jobs</em>. Para saber, mais visite o <a href="https://hydra.cc/">site</a>.</p>

<!-- Hydra is a Python-based configuration tool for more efficiently managing complex ML projects. It is meant to help PyTorch researchers by offering functional reuse of configurations for ML projects. The main benefit it offers is that it allows the programmer to *compose the configuration like composing code*, which means the configuration file can be easily overridden. Hydra can also help with automatically managing the working directory of your ML project outputs which is useful when needing to save and accessing the results of several experiments for multiple jobs. Learn more about it [here](https://medium.com/pytorch/hydra-a-fresh-look-at-configuration-for-machine-learning-projects-50583186b710%27). -->

<!-- ***A Toolkit for Causal Inferencing with Bayesian Networks*** -->

<p><br />
<strong><em>Uma biblioteca para Infer√™ncia Causal com Redes Bayesianas</em></strong></p>

<!-- [CausalNex](https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html) is a toolkit for ‚Äúcausal inference with Bayesian Networks‚Äù. The tool aims to combine machine learning and causal reasoning for uncovering structural relationships in data. The authors also prepared an introductory guide on why and how to infer causation with Bayesian networks using the proposed Python library. -->

<p><br />
A <a href="https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html">CausalNex</a> √© uma ferramenta que busca combinar o aprendizado de m√°quina e o racioc√≠nio causal, possibilitando a descoberta de relacionamentos estruturais na base de dados. Os autores prepararam um tutorial introdut√≥rio que mostra porqu√™ e como inferir causalidade com as Redes Bayesianas utilizando a biblioteca proposta.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*EYwKhdnscR7ZLuNkTqCS2Q.png" alt="" /></p>

<p><a href="https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html"><em>fonte</em></a></p>

<p><br />
<strong><em>Google Colab Pro agora est√° dispon√≠vel</em></strong></p>

<p><br />
O Google Colab comecou a oferecer uma vers√£o Pro, que disponibiliza vantagens como o acesso exclusivo √† GPUs e TPUs, tempos de execu√ß√£o mais longos e mais mem√≥ria.</p>

<!-- Google Colab is now offering a Pro edition, which offers advantages such as exclusive access to faster GPUs and TPUs, longer runtimes, and more memory. -->

<p><br />
<strong><em>TyDi QA: A Multilingual Question Answering Benchmark</em></strong></p>

<p><br />
O grupo de IA da Google introduziu recentemente a <a href="https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html">TyDi QA</a>, uma base de dados multi-idiomas que busca encorajar pesquisadores a abordar a tarefa de <em>question answering</em> em l√≠nguas mais tipologicamente diversas, ou seja, que apresentam padr√µes estrturais n√£o-convencionais. A libera√ß√£o da base visa motivar a constru√ß√£o de modelos mais robustos a idiomas tipologicamente distantes, como o √Årabe, Bengali, Coreano, Russo, Telugo e Tail√¢ndes, podendo generalizar para outros dialetos.</p>

<!-- Google AI releases [TyDi QA](https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html) which is a multilingual dataset that can encourage researchers to perform question answering on more typologically diverse languages that construct and express meaning in different ways. The idea is to motivate researchers to build more robust models on typologically distant languages, such as Arabic, Bengali, Korean, Russian, Telugu, and Thai, so as to generalize to even more languages. -->
<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*1dZv5you3jigdrQ2uAKzUw.png" alt="" /></p>

<p><a href="https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html"><em>fonte</em></a></p>

<p><br />
<strong><em>Question Answering para Node.js</em></strong></p>

<p><br />
A empresa Hugging Face liberou uma <a href="https://github.com/huggingface/node-question-answering">biblioteca para <em>question answering</em></a> baseada no DistilBERT, dando continuidade a sua miss√£o de tornar a √°rea de Processamento de Linguagem Natural mais acess√≠vel. O modelo apresentado pode rodar num ambiente de produ√ß√£o utilizando Node.js com apenas 3 linhas de c√≥digo, se benficiando da implementa√ß√£o eficiente oferecida pela Tokenizers, tamb√©m desenvolvida pelo Hugging Face, e a vers√£o em Javascript do TensorFlow (TensorFlow.js).</p>

<!-- Hugging Face releases a [question answering library](https://github.com/huggingface/node-question-answering) based on DistilBERT and continues to make NLP more accessible. This model can run in production using Node.js with just 3 lines of code. The model leverages the fast implementation of Tokenizers, also built by Hugging Face, and TensorFlow.js (a popular library for using machine learning models with Javascript). -->

<h1 id="√âtica-em-ia-">√âtica em IA üö®</h1>

<p><strong><em>Identificando vi√©s subjetivo em texto</em></strong></p>

<p><br />
Num <a href="https://podcasts.apple.com/us/podcast/will-ai-help-identify-bias-or-perpetuate-it-with-diyi-yang/id1435564422?i=1000464141922">podcast</a> que contou com a participa√ß√£o de Diyi Yang, um pesquisador em ci√™ncia social computacional, foi discutido como sistemas de IA podem auxiliar na identifica√ß√£o de vi√©s subjetivo em informa√ß√µes textuais. Essa √© uma √°rea de pesquisa importante envolvendo Intelig√™ncia Artificial e NLP, especialmente quando discutimos sobre o consumo de textos, como t√≠tulos e chamadas de not√≠cias, e a facilidade que esses meios possuem para influenciar leitores com opini√µes subjetivas.</p>

<p>Do ponto de vista da aplica√ß√£o, se torna de vital import√¢ncia a tarefa de identifica√ß√£o desse vi√©s de maneira autom√°tica, assim como a conscientiza√ß√£o dos leitores, para que esses se tornem mais atentos e criteriosos em rela√ß√£o ao conte√∫do que est√£o consumindo.</p>

<!-- This [podcast episode](https://podcasts.apple.com/us/podcast/will-ai-help-identify-bias-or-perpetuate-it-with-diyi-yang/id1435564422?i=1000464141922) features Diyi Yang, a researcher in computational social science, who talks about how AI systems can help to identify subjective bias in textual information. This is an important area of research involving AI systems and NLP especially when we discuss the consumption of text media such as news headlines that can be easily framed to bias consumers when in reality they should aim to be more objective. From an application perspective, it becomes critical to automatically identify the subjective bias present in text media so as to help consumers become more aware of the content they are consuming. The episode also discusses how AI can also perpetuate bias. -->

<p><br />
<strong><em>Artificial Intelligence, Values and Alignment</em></strong></p>

<p><br />
A dissemina√ß√£o de sistemas de IA e a forma com que esses sistemas se alinham com os valores humanos √© uma √°rea de pesquisa envolvendo √©tica na Intelig√™ncia Artificial em crescente atividade. A DeepMind publicou recentemente um <a href="https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment">artigo</a> que busca investigar de maneira mais profunda as quest√µes filos√≥ficas envolvidas no alinhamento da IA com os valores humanos. O trabalho discute duas frentes: a t√©cnica (<em>como codificar valores que permitem que agentes de IA produzam resultados confi√°veis</em>), e a normativa (<em>quais princ√≠pios deveriam ser codificados pelos modelos</em>), e como eles se relacionam e podem ser garantidos. O artigo encoraja uma abordagem baseada em princ√≠pios para o alinhamento de valores pelos sistemas de Intelig√™ncia, de modo a preservar um tratamento igualit√°rio e justo frente √†s diferen√ßas de convic√ß√µes e opini√µes.</p>

<!-- The rise of AI systems and how they align human values is an active area of research that involves ethics in AI systems. DeepMind recently released a [paper](https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment) that takes a deeper look at the philosophical questions surrounding AI alignment. The report focuses on discussing two parts, technical (i.e., *how to encode values that render reliable results from AI agents*) and normative (*what principles would be right to encode in AI*), and how they relate and can be ensured. The paper pushes for a principle-based approach for AI alignment and to preserve fair treatment despite the difference in beliefs and opinions. -->

<p><br />
<strong><em>Auditoria em Sistemas de IA</em></strong></p>

<p><br />
A VentureBeat divulgou que pesquisadores da Google, numa colabora√ß√£o com outros grupos, criaram um <em>framework</em> chamado SMACTR, que permite a auditoria de sistemas de IA. A motiva√ß√£o para esse trabalho envolve a aus√™ncia de ‚Äúpresta√ß√£o de contas‚Äù dos sistemas atuais, que s√£o colocados √† disposi√ß√£o do p√∫blico geral. A reportagem completa pode ser acessada <a href="https://venturebeat.com/2020/01/30/google-researchers-release-audit-framework-to-close-ai-accountability-gap/">aqui</a>, assim como o <a href="https://dl.acm.org/doi/abs/10.1145/3351095.3372873">trabalho completo</a>.</p>

<!-- A VentureBeat reports that Google Researchers, in collaboration with other groups, created a framework called SMACTR that allows engineers to audit AI systems. The reason for this work is to address the accountability gap that exists with current AI systems that are put in the wild to be used by consumers. Read the full report [here](https://venturebeat.com/2020/01/30/google-researchers-release-audit-framework-to-close-ai-accountability-gap/) and the full paper [here](https://dl.acm.org/doi/abs/10.1145/3351095.3372873). -->

<h1 id="artigos-e-postagens-Ô∏è">Artigos e Postagens ‚úçÔ∏è</h1>

<p><strong><em>Destila√ß√£o de modelos em sistemas de NLP</em></strong></p>

<p><br />
Num <a href="https://soundcloud.com/nlp-highlights/104-model-distillation-with-victor-sanh-and-thomas-wolf">novo epis√≥dio</a> do podcast <em>NLP Highlights</em>, Thomas Wolf and Victor Sanh discutiram sobre a destila√ß√£o de modelos e como a t√©cnica pode ser utilizada como uma alternativa fact√≠vel para a compress√£o de grandes arquiteturas, como o BERT, para aplica√ß√µes escal√°veis de NLP em cen√°rios reais. A metodologia √© discutida no trabalho publicado pelos convidados, entitulado <a href="https://arxiv.org/abs/1910.01108">DistilBERT</a>, onde s√£o constru√≠dos modelos menores (baseados na mesma arquitetura do modelo original) que tentam simluar o comportamento do modelo com maior n√∫mero de par√¢metros, de acordo com suas sa√≠das. Essencialmente, o menor modelo (<em>student</em>) tenta modelar a distribui√ß√£o de probabilidade do modelo maior (<em>teacher</em>) baseado na distribui√ß√£o emp√≠rica gerado por suas sa√≠das.</p>

<p><br />
<strong><em>BERT, ELMo, &amp; GPT-2: How contextual are contextualized word representations?</em></strong></p>

<p><br />
O sucesso de m√©todos contextualizados como o BERT para resolu√ß√£o de uma ampla gama de tarefas complexas de NLP √© um assunto que est√° em voga no momento. Nesse <a href="https://kawine.github.io/blog/nlp/2020/02/03/contextual.html">post</a>, Kawin Ethayarajh tenta responder a quest√£o que diz respeito √† qu√£o contextuais os modelos como BERT, o ELMo e o GPT-2 e seus respectivos <em>word embedddings</em> contextualizados s√£o. As caracter√≠sticas exploradas incluem m√©tricas de contextualidade, especificidade de contexto, al√©m de compara√ß√µes entre representa√ß√µes vetoriais de palavras ‚Äúest√°ticas‚Äù e suas vers√µes contextualizadas.</p>

<!-- Recently there has been a lot of talk on the success of contextualized methods like BERT for approaching a wide variety of complex NLP tasks. In this [post](https://kawine.github.io/blog/nlp/2020/02/03/contextual.html), Kawin Ethayarajh attempts to answer the question of how contextual models like BERT, ELMo and GPT-2 and their contextualized word representation are? Topics include measures of contextuality, context-specificity, and comparisons between static embeddings and contextualized representations. -->
<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*70aIv1Fkkz4rnHgQ.png" alt="" /></p>

<p><a href="https://kawine.github.io/blog/nlp/2020/02/03/contextual.html"><em>fonte</em></a></p>

<p><br />
<strong><em>Esparsidade em Redes Neurais</em></strong></p>

<p><br />
Fran√ßois Lagunas, pesquisador na √°rea de ML, escreveu esse excelente <a href="https://medium.com/huggingface/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70">post</a> compartilhando seu otimismo em rela√ß√£o √† utiliza√ß√£o de tensores esparsos em modelos de redes neurais. A expectativa √© empregar alguma forma de esparsidade visando a redu√ß√£o do tamanho dos modelos atuais, que de certa forma est√£o se tornando impratic√°veis, dadas suas colossais quantidades de par√¢metros. Os Transformers, por exemplo, com seus bilh√µes de par√¢metros, poderiam se beneficar com o emprego dessa t√©cnica.</p>

<p>Entretanto, os detalhes de implementa√ß√£o para viabilizar a utiliza√ß√£o eficiente da esparsidade em GPU ainda n√£o est√£o claros‚Ä¶ Felizmente, a comunidade de Aprendizado de M√°quina j√° est√° trabalhando nisso!</p>

<!-- Fran√ßois Lagunas, an ML researcher, wrote this great [blog post](https://medium.com/huggingface/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70) discussing his optimism for adopting sparse tensors in neural network models. The hope is to employ some form of sparsity to reduce the size of current models that at some point become unpractical due to their size and speed. This concept may be worth exploring in ML due to the sheer size of current models like Transformers (often relying on billions of parameters). However, the implementation details to support efficient sparsity in neural networks on GPUs are not so clear from a developer tool perspective and that is something the machine learning community is working on already. -->

<p><br />
<strong><em>Treinando Seu Pr√≥prio Modelo de Linguagem</em></strong></p>

<p><br />
Se voc√™ est√° interessado em aprender como treinar um modelo de linguagem do zero, confira esse excelente <a href="https://huggingface.co/blog/how-to-train">tutorial</a> da Hugging Face que utiliza as suas incr√≠veis bibliotecas Tokenizers e Transformers no treinamento do modelo.
<!--
If you are interested to learn how to train a language model scratch, check out this impressive and comprehensive [tutorial](https://huggingface.co/blog/how-to-train) by Hugging Face. They obviously leverage their own libraries Transformers and Tokenizers to train the model. --></p>

<p><br />
<strong><em>Tokenizers: How machines read</em></strong></p>

<p><br />
Cathal Horan publicou um <a href="https://blog.floydhub.com/tokenization-nlp/">blog post</a> impresssionante e bem detalhado sobre como e quais tipos de <em>tokenizers</em> v√™m sendo utilizados nos mais recentes modelos de NLP, auxiliando modelos de Intelig√™ncia a aprender por meio de informa√ß√µes textuais. O post tamb√©m discute e motiva porqu√™ a tarefa de tokeniza√ß√£o √© uma importante e desafiadora √°rea de pesquisa ativa. O artigo apresenta ainda como treinar o seu pr√≥prio <em>tokenizer</em> utilizando m√©todos como o SentencePiece e o WordPiece.</p>

<!-- Cathal Horan published an impressive and very detailed [blog post](https://blog.floydhub.com/tokenization-nlp/) about how and what type of tokenizers are being used by the most recent NLP models to help machine learning algorithms learn from textual information. He also discusses and motivated why tokenization is an exciting and important active area of research. The article even shows you how to train your own tokenizers using tokenization methods like SentencePiece and WordPiece. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*Vkjw5n9Sz0Was43haVNJMg.png" alt="" /></p>

<p><a href="https://blog.floydhub.com/tokenization-nlp/%27"><em>fonte</em></a></p>

<h1 id="educa√ß√£o-">Educa√ß√£o üéì</h1>

<p><strong><em>Machine Learning na VU Amsterdam</em></strong></p>

<p><br />
Agora voc√™ pode acompanahar o curso <a href="https://mlvu.github.io/">2020 MLVU machine learning</a> pela internet, onde est√£o inclusas a lista completa de slides, <a href="https://www.youtube.com/watch?v=excCZSTJEPs&amp;feature=youtu.be">videos</a> e o plano de estudos. O curso oferece uma introdu√ß√£o √† ML, al√©m de cobrir t√≥picos mais avan√ßados de Deep Learning, como <em>Variational AutoEncoders</em> (VAEs) e Redes Neurais Adversariais (GANs).</p>

<!-- You can now follow the [2020 MLVU machine learning](https://mlvu.github.io/) course online, which includes the full set of slides, [videos](https://www.youtube.com/watch?v=excCZSTJEPs&feature=youtu.be), and syllabus. It is meant to be an introduction to ML but it also has other deep learning related topics such as VAEs and GANs. -->

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*zFpU2rQL5Fby7X3boJyQNg.png" alt="" /></p>

<p><a href="https://mlvu.github.io/"><em>fonte</em></a></p>

<p><br />
<strong><em>Materiais de Matem√°tica para ML</em></strong></p>

<p><br />
Suzana Iliƒá e a organiza√ß√£o Machine Learning Tokyo (MLT) v√™m realizando um excelente trabalho em prol da democratiza√ß√£o do conhecimento em ML. Confira esse <a href="https://github.com/Machine-Learning-Tokyo/Math_resources">reposit√≥rio</a> que apresenta uma cole√ß√£o de fontes e materiais sobre os fundamentos matem√°ticos utilizados em Aprendizado de M√°quina.</p>

<!-- Suzana Iliƒá and the Machine Learning Tokyo (MLT) have been doing amazing work in terms of democratizing ML education. For example, check out this [repository](https://github.com/Machine-Learning-Tokyo/Math_resources) showcasing a collection of free online resources for learning about the foundations of mathematical concepts used in ML. -->

<p><br />
<strong><em>Introduction to Deep Learning</em></strong></p>

<p><br />
Acompanhe o curso ‚Äú<a href="http://introtodeeplearning.com/">Introduction to Deep Learning</a>‚Äù do MIT nesse site. Novas aulas ser√£o postadas toda semanas e todos os materiais, como slides, v√≠deos e c√≥digos utilizados, ser√£o publicados.</p>

<!-- Keep track of the ‚Äú[Introduction to Deep Learning](http://introtodeeplearning.com/)‚Äù course by MIT on this website. New lectures will be posted every week and all the sides and videos, including coding labs, will be published. -->

<p><br />
<strong><em>Deep Learning com PyTorch</em></strong></p>

<p><br />
Alfredo Canziani publicou os slides e notebooks utilizados no minicurso de Deep Learning com Pytorch. O reposit√≥rio cont√©m ainda um <a href="https://atcold.github.io/pytorch-Deep-Learning/">site</a> que incluem notas sobre os conceitos apresentados no curso.</p>

<!-- Alfredo Canziani has published the slides and notebooks for the minicourse on Deep Learning with PyTorch. The repository also contains a [companion website](https://atcold.github.io/pytorch-Deep-Learning-Minicourse/) that includes text descriptions of the concepts taught in the course. -->

<p><br />
<strong><em>Missing Semester of Your CS</em></strong></p>

<p><br />
O ‚Äú<a href="https://missing.csail.mit.edu/">Missing Semester of Your CS</a>‚Äù √© um excelente curso online composto por recursos que podem ser potencialmente √∫teis para cientistas de dados que n√£o possuem background na √°rea de desenvolvimenteo. Est√£o inclusos materiais sobre <em>shell scripting</em> e versionamento. O curso foi disponibilizado por alunos do MIT.
<br />
<img src="https://cdn-images-1.medium.com/max/800/1*weUnTXxmHxYf-B2DDaslvw.png" alt="" /></p>

<p><a href="https://missing.csail.mit.edu/2020/shell-tools/"><em>fonte</em></a></p>

<p><br />
<strong><em>Advanced Deep Learning</em></strong></p>

<p><br />
A CMU disponibilizou recentemente os slides e plano de estudos para o curso ‚Äú<a href="https://andrejristeski.github.io/10707-S20/syllabus.html">Advanced Deep Learning</a>‚Äù, que cobre t√≥picos como modelos autoregressivos, modelos generativos, aprendizado autosupervisionado, entre outros. O p√∫blico-alvo s√£o alunos de mestrado e doutorado com s√≥lidos conhecimentos de ML.</p>

<!-- A CMU released the slides and syllabus for the ‚Äú[Advanced Deep Learning](https://andrejristeski.github.io/10707-S20/syllabus.html)‚Äù course which includes topics such as autoregressive models, generative models, and self-supervised/predictive learning, among others. The course is meant for MS or Ph.D. students with an advanced background in ML. -->

<h1 id="men√ß√µes-honrosas-Ô∏è">Men√ß√µes honrosas ‚≠êÔ∏è</h1>

<p><br />
Voc√™ pode encontrar a √∫ltima Newsletter <a href="https://medium.com/dair-ai/nlp-newsletter-flax-thinc-language-specific-bert-models-meena-flyte-lasertagger-4f7da04a9060">aqui</a>. Essa edi√ß√£o cobriu t√≥picos como melhorias em agentes conversacionais, divulga√ß√£o de modelos BERT para idiomas espec√≠ficos (entre eles o <strong>Portugu√™s</strong>!!!), bases de dados publicamente dispon√≠vies, introdu√ß√£o de novas bibliotecas para Deep Learning, e muito mais.</p>

<!-- You can catch the previous NLP Newsletter here. The [issue](https://medium.com/dair-ai/nlp-newsletter-flax-thinc-language-specific-bert-models-meena-flyte-lasertagger-4f7da04a9060) covers topics such as improving conversational agents, releases of language-specific BERT models, free datasets, releases of deep learning libraries, and much more. -->

<p>Em Xu et al. (2020), foi proposto um <a href="https://arxiv.org/abs/2002.02925]">m√©todo</a> para progressivamente substituir e comprimir modelos BERT atrav√©s da separa√ß√£o de seus componentes originais. Atrav√©s dessa substitui√ß√£o progressiva, aliado ao processo de treinamento, √© poss√≠vel combinar os componentes originais e suas vers√µes compactadas. A metodologia apresentada supera outras abordagens de <em>knowledge distillation</em> no benchmark GLUE.</p>

<!-- Xu et al. (2020) proposed a [method](https://arxiv.org/abs/2002.02925]) for progressively replacing and compressing a BERT model by dividing it into its original components. Through progressive replacement and training, there is also the advantage of combining the original components and compacted versions of the model. The proposed model outperforms other knowledge distillation approaches on the GLUE benchmark. -->

<p>Um outro curso interessante √© o ‚Äú<a href="https://compstat-lmu.github.io/lecture_i2ml/index.html">Introduction to Machine Learning</a>‚Äù, que cobre o b√°sico de ML, regress√£o supervisionada, <em>random forests</em>, ajuste de par√¢metros, dentre outros conceitos fundamentais.</p>

<!-- Here is another interesting course called ‚Äú[Introduction to Machine Learning](https://compstat-lmu.github.io/lecture_i2ml/index.html)‚Äù which covers the ML basics, supervised regression, random forests, parameter tuning, and many more fundamental ML topics. -->

<p>A vers√£o para üá¨üá∑ grego do BERT (<a href="https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1">GreekBERT</a>) est√° dispon√≠vel para uso atrav√©s da biblioteca Transformers, da Hugging Face.</p>

<p>Jeremy Howard publicou um <a href="https://arxiv.org/abs/2002.04688">artigo</a> descrevendo a biblioteca de Deep Learning fast.ai, que √© amplamente utilizada para pesquisa e ensino em seus cursos de Deep Learning. Uma leitura bastante recomendada para desenvolvedores de software que trabalham construindo e melhorando pacotes de Aprendizado de M√°quina e Deep Learning.
<!--
Jeremy Howard publishes a [paper](https://arxiv.org/abs/2002.04688) describing the fastai deep learning library which is widely used for research and to teach their open courses on deep learning. A recommended read for software developers working on building and improving deep learning and ML libraries. --></p>

<p>A Deeplearning.ai completou o lan√ßamento dos seus 4 cursos da s√©rie <a href="https://www.coursera.org/specializations/tensorflow-data-and-deployment">TensorFlow: Data and Deployment Specialization</a>. A especializa√ß√£o visa ensinar desenvolverdores a como realizar o <em>deploy</em> de modelos de maneira efetiva e eficiente nos mais diferentes cen√°rios, al√©m de utilizar dados de maneiras eficazes durante o treinamento de modelos.</p>

<!-- Deeplearning.ai completes the release of all four courses of the [TensorFlow: Data and Deployment Specialization](https://www.coursera.org/specializations/tensorflow-data-and-deployment). The specialization mainly aims to educate developers on how to efficiently and effectively deploy models in different scenarios and make use of data in interesting and effective ways while training models. -->

<p>Sebastian Raschka publicou recentemente um <a href="https://arxiv.org/abs/2002.04803">artigo</a> entitulado ‚ÄúMachine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence‚Äù. O trabalho apresenta uma revis√£o bastante acess√≠vel √†s mais variadas ferramentas de ML. √â um artigo excelente para compreender as vantagens de algumas bibliotecas e conceitos utilizados em Aprendizado de M√°quina. Al√©m disso, levanta-se a discuss√£o sobre o futuro de bibliotecas de ML baseadas em Python.</p>

<!-- Sebastian Raschka recently published a [paper](https://arxiv.org/abs/2002.04803) titled ‚ÄúMachine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence‚Äù. The paper serves as a comprehensive review of the machine learning tools landscape. It is an excellent report for understanding the various advantages of some libraries and concepts used in ML engineering. In addition, a word on the future of Python-based machine learning libraries is provided. -->

      <hr />
      <footer role="contentinfo">
        <div class="social-share">
  <!-- Go to www.addthis.com/dashboard to customize your tools --> 
  <div class="addthis_inline_share_toolbox"></div>
  <!--
  <h4>Share on</h4>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG/" class="twitter" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG/" class="facebook" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=http://localhost:4000/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG/" class="google-plus" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
  </ul>-->
</div><!-- /.social-share -->
        <p class="byline"><strong>NLP Newsletter [PT-BR]: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,‚Ä¶</strong> was published on <time datetime="2020-02-16T00:00:00+01:00">February 16, 2020</time>.</p>
        
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dair-ai'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<!--
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->

      </footer>
    </div><!-- /.article-wrap -->
  
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
  
  </article>
</div><!-- /#main -->

<div class="footer-wrap">
  
  <div class="related-articles">
  <h4>You might also enjoy <small class="pull-right">(<a href="http://localhost:4000/posts/">View all posts</a>)</small></h4>
    <ul>
    
      <li><a href="http://localhost:4000/NLP_Newsletter_NLP_7-ZH-.md/" title="NLP ÁÆÄÊä•ÔºàIssue#7Ôºâ: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶">NLP ÁÆÄÊä•ÔºàIssue#7Ôºâ: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶</a></li>
    
      <li><a href="http://localhost:4000/NLP_Newsletter_NLP_7/" title="NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶">NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶</a></li>
    
      <li><a href="http://localhost:4000/NLP_Newsletter_-7_-FR/" title="NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶">NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶</a></li>
    
    </ul>
    <hr />
  </div><!-- /.related-articles -->
  
  <footer>
    

<span>&copy; 2020 dair.ai. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  var _gaq = _gaq || [];
  var pluginUrl =
 '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
  _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
  _gaq.push(['_setAccount', 'UA-158959084-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>


  
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dair-ai'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<!--
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->




</body>
</html>
