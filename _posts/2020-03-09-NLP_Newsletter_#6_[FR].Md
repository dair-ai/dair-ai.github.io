---
layout: post
title: "NLP Newsletter #6 [FR]: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML"
author: lbourdois
excerpt: ""
modified:
comments: true
tags: [nlp_newsletter]
image:
  thumb: nlp_newsletter_6.png
---



![](https://cdn-images-1.medium.com/max/1200/1*vWICxAehSy3xOnqGIXtpoQ.png)

# Avant-propos
Bienvenue au sixi√®me num√©ro de la lettre d'information consacr√©e au NLP. Merci pour votre soutien et pour avoir pris le temps de lire les derni√®res nouvelles sur le ML et le NLP. Ce num√©ro traite de sujets allant de l'extension du mod√®le Transformer au ralentissement de la publication en ML, en passant par une s√©rie de livres et de lancements de projets en ML et en NLP.


\\
***Quelques mises √† jour sur la lettre d'information sur le NLP et sur dair.ai.***

\\
Nous avons traduit la lettre d'information dans d'autres langues telles que le portugais br√©silien, le chinois, l'arabe, l'espagnol, entre autres. Merci aux personnes qui ont aid√© √† la traduction. Vous pouvez √©galement contribuer [ici](https://github.com/dair-ai/dair-ai.github.io/issues/11).

\\
Il y a un mois, nous avons officiellement lanc√© notre nouveau [site web](https://dair.ai/). Vous pouvez consulter notre [organisation GitHub](https://github.com/dair-ai) pour plus d'informations sur dair.ai et ses projets. Si vous souhaitez voir comment d'autres personnes contribuent d√©j√† √† dair.ai ou si vous souhaitez contribuer √† la d√©mocratisation de la recherche, de l'√©ducation et des technologies en mati√®re d'intelligence artificielle, consultez notre [section](https://github.com/dair-ai/dair-ai.github.io/issues) sur les questions d'actualit√©.


# Publications  üìô

***Une introduction √† la BERTologie : Ce que nous savons sur le fonctionnement de BERT***

\\
Les mod√®les bas√©s sur le Transformer se sont av√©r√©s efficaces pour aborder diff√©rents types de t√¢ches de NLP allant de la classification de s√©quences √† la r√©ponse aux questions. L'un de ces mod√®les, appel√© BERT ([Devlin et al. 2019](https://arxiv.org/abs/1810.04805)), est largement utilis√© mais comme d'autres mod√®les qui utilisent des r√©seaux de neurones profonds, nous savons tr√®s peu de choses sur leur fonctionnement interne. Un nouvel [article](https://arxiv.org/abs/2002.12327) intitul√© " A Primer in BERTology: What we know about how BERT works " vise √† r√©pondre √† certaines des interrogations portant sur les raisons pour lesquelles BERT est performant dans un si grand nombre de t√¢ches de NLP. Parmi les sujets abord√©s dans cet article, on trouve le type de connaissances acquises par BERT ainsi que leur repr√©sentation, la mani√®re dont ces connaissances sont acquises et les autres m√©thodes utilis√©es par les chercheurs pour les am√©liorer.

\\
***Explorer les limites de l'apprentissage par transfert avec un Transformer de texte √† texte***

\\
Google AI a r√©cemment publi√© une [m√©thode](https://arxiv.org/abs/1910.10683) qui rassemble tous les enseignements et les am√©liorations tir√©s des mod√®les de NLP bas√©s sur l'apprentissage par transfert.  Les auteurs l‚Äôont appel√© Text-to-Text Transfer Transformer (T5). Ce travail propose que la plupart des t√¢ches de NLP puissent √™tre formul√©es dans un format texte-texte, sugg√©rant que les entr√©es et les sorties sont des textes. Les auteurs affirment que ce " cadre fournit un objectif d‚Äôentra√Ænement coh√©rent √† la fois pour le pr√©-entra√Ænement et le fine-tuning". 			
Le T5 est essentiellement un Transformer encoder-decoder qui applique diverses am√©liorations, en particulier aux composantes d'attention qui composent le mod√®le. Le mod√®le a √©t√© pr√©-entra√Æn√© sur un ensemble de donn√©es r√©cemment publi√©, le [Colossal Clean Crawled Corpus](https://www.tensorflow.org/datasets/catalog/c4) et a √©t√© appliqu√© sur SOTA sur des t√¢ches de NLP telles que le r√©sum√©, la r√©ponse aux questions et la classification de textes.

\\
![](https://cdn-images-1.medium.com/max/800/1*T9MXxcDOd2fX6xblbu7VdQ.png)

[*(Raffel et al. 2020)*](https://arxiv.org/abs/1910.10683)


\\
***12 en 1 : Apprentissage multit√¢che de la repr√©sentation de la vision et des langues***

\\
La recherche actuelle utilise des t√¢ches et des ensembles de donn√©es ind√©pendants pour effectuer des recherches sur la vision et le langage m√™me lorsque les "comp√©tences de compr√©hension du langage fond√©es sur la vision" requises pour effectuer ces t√¢ches se chevauchent. Une nouvelle [publication](https://arxiv.org/abs/1912.02315) (qui sera pr√©sent√©e √† la CVPR) propose une approche multit√¢che √† grande √©chelle pour mieux mod√©liser et entra√Æner conjointement les t√¢ches de vision et du langage afin de g√©n√©rer un mod√®le de vision et de langue plus g√©n√©rique. Le mod√®le r√©duit la taille des param√®tres et fonctionne bien pour des t√¢ches telles que la recherche d'images bas√©e sur des l√©gendes et la r√©ponse visuelle √† des questions.
\\
![](https://cdn-images-1.medium.com/max/800/1*yyvN4bK0K2iykyJ2-QVBjw.png)


[*(Lu et al. 2020)*](https://arxiv.org/abs/1912.02315)

\\
***BERT peut voir √† l'ext√©rieur de la bo√Æte : Sur la transf√©rabilit√© intermodale des repr√©sentations textuelles***

\\
Les chercheurs et collaborateurs de reciTAL ont publi√© un [article](https://arxiv.org/abs/2002.10832) qui vise √† r√©pondre √† la question de savoir si un mod√®le BERT peut produire des repr√©sentations qui se g√©n√©ralisent √† d'autres modalit√©s que le texte, comme par exemple la vision. Ils proposent un mod√®le appel√© BERT-gen qui exploite des repr√©sentations mono ou multimodales et qui obtient de meilleurs r√©sultats sur les ensembles de donn√©es de g√©n√©ration de questions visuelles.

\\
![](https://cdn-images-1.medium.com/max/800/1*2NgR7yBuVLDcEza9UT41dw.png)

[*(Scialom et al. 2020)*](https://arxiv.org/abs/2002.10832)


# Cr√©ativit√© et soci√©t√© üé®

***La prochaine d√©cennie en IA : quatre √©tapes vers une intelligence artificielle robuste***

\\
Gary Marcus a r√©cemment publi√© un [article](https://arxiv.org/abs/2002.06177) dans lequel il explique une s√©rie de mesures que nous devrions prendre selon lui afin de construire des syst√®mes d'IA plus robustes. L'id√©e centrale dans ce papier est de se concentrer sur la construction de syst√®mes hybrides et ax√©s sur la connaissance, guid√©s par des mod√®les cognitifs, plut√¥t que de se concentrer sur la construction de syst√®mes plus importants qui n√©cessitent plus de donn√©es et de puissance de calcul.

\\
***10 technologies de pointe pour 2020***

\\
La revue technologique du MIT a publi√© une liste des [10 perc√©es](https://www.technologyreview.com/lists/technologies/2020/) qu'elle a identifi√©es et qui feront la diff√©rence dans la r√©solution de probl√®mes susceptibles de changer notre fa√ßon de vivre et de travailler. La liste - sans ordre particulier - comprend l'internet non piratable, la m√©decine hyper-personnalis√©e, l'argent num√©rique, les m√©dicaments anti-√¢ge, les mol√©cules d√©couvertes par l'IA, les m√©ga-constellations de satellites, la supr√©matie quantique, l'IA minuscule, la confidentialit√© diff√©rentielle et l'attribution du climat.

\\
***Il est temps de repenser le processus de publication en ML***

\\
Yoshua Bengio a r√©cemment fait part de ses [pr√©occupations](https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/) concernant les cycles rapides des publications du ML. La principale est qu'en raison de la rapidit√© de la publication, beaucoup d'articles publi√©s contiennent des erreurs et sont juste incr√©mentiels. A contrario, ceux sur lesquels plus de temps est consacr√© afin d‚Äôen assurer la rigueur, semble dispara√Ætre. De plus, ce sont les √©tudiants qui doivent faire face aux cons√©quences n√©gatives de cette pression et de ce stress. Pour rem√©dier √† cette situation, Bengio parle de ses actions pour aider √† ralentir le processus de publication des recherches pour le bien de la science.

# Outils et jeux de donn√©es ‚öôÔ∏è

***Mise en ≈ìuvre du r√©seau PointerGenerator dans AllenNLP***

\\
Les r√©seaux ¬´ Pointer-Generator ¬ª visent √† augmenter les mod√®les d'attention utilis√©s pour am√©liorer [la synth√®se abstraite]( https://arxiv.org/abs/1704.04368). Si vous souhaitez utiliser cette technique en utilisant AllenNLP, Kundan Krishna a d√©velopp√© une [librairie](https://github.com/kukrishna/pointer-generator-pytorch-allennlp) qui vous permet d'ex√©cuter un mod√®le pr√©-entra√Æn√© (fourni) ou d‚Äôentra√Æner votre propre mod√®le.

\\
***Questions/r√©ponses pour diff√©rentes langues***

\\
Avec la prolif√©ration des mod√®les de Transformer et leur efficacit√© pour les t√¢ches de NLP, des efforts impressionnants ont √©t√© d√©ploy√©s pour publier diff√©rents types de jeux de donn√©es dans diff√©rentes langues. Par exemple, Sebastian Ruder a [partag√© une liste](https://twitter.com/seb_ruder/status/1231713840502657025?s=20) de jeux de donn√©es qui peuvent √™tre utilis√©s pour des t√¢ches de r√©ponses aux questions dans diff√©rentes langues : [DuReader](https://www.aclweb.org/anthology/W18-2605/), [KorQuAD](https://arxiv.org/abs/1909.07005), [SberQuAD](https://arxiv.org/abs/1912.09723), [FQuAD](https://arxiv.org/abs/2002.06071), [Arabic-SQuAD](https://arxiv.org/abs/1906.05394), [SQuAD-it](https://github.com/crux82/squad-it) et [Spanish SQuAD](https://arxiv.org/abs/1912.05200v2).

\\
***PyTorch Lightning***

\\
PyTorch Lightning est un [outil](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09) qui vous permet de r√©aliser un entra√Ænement abstrait qui n√©cessiterait l‚Äôutilisation de GPU/TPU d'une pr√©cision de 16 bits. PyTorch Lightning permet d‚Äôentra√Æner des mod√®les sur des plusieurs GPU et TPU sans avoir besoin de changer votre code PyTorch actuel.

\\
***Graph Neural Networks dans TensorFlow 2***

\\
Une √©quipe de recherche de Microsoft publie une [librairie](https://github.com/microsoft/tf2-gnn) qui donne acc√®s aux impl√©mentations de nombreuses architectures de r√©seaux neuronaux en graphes (GNN). Cette librairie est bas√©e sur TensorFlow 2 et fournit √©galement des modules de manipulation de donn√©es qui peuvent √™tre directement utilis√©s dans des boucles d‚Äôentrainement/√©valuation.

\\
***Pr√©-entra√Ænement de SmallBERTa - Un petit mod√®le pour s'entra√Æner sur un petit jeu de donn√©es***

\\
Avez-vous d√©j√† voulu entra√Æner votre propre mod√®le linguistique √† partir de z√©ro mais n'avez pas eu assez de ressources pour le faire ? Si c'est le cas, Aditya Malte vous propose un [notebook](https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb) qui vous apprend √† entrainer un mod√®le linguistique √† partir de z√©ro avec un ensemble de donn√©es plus restreint.

# Ethique en IA üö®

***Pourquoi les visages ne disent pas toujours la v√©rit√© sur les sentiments***

\\
Depuis un certain temps, de nombreux chercheurs et entreprises ont tent√© de construire des mod√®les d'IA qui comprennent et peuvent reconna√Ætre les √©motions dans un contexte textuel ou visuel. Un nouvel [article](https://www.nature.com/articles/d41586-020-00507-5) relance le d√©bat sur le fait que les techniques d'IA qui visent √† reconna√Ætre les √©motions √† partir des images de visages ne le font pas correctement. L'argument principal, soulev√© par des psychologues, est qu'il n'existe aucune preuve d'expressions universelles pouvant √™tre utilis√©es pour la d√©tection d'√©motions bas√©es uniquement sur des images de visages. Il faudrait qu'un mod√®le comprenne mieux par exemple les traits de personnalit√© ou encore les mouvements du corps, afin de se rapprocher r√©ellement d'une d√©tection plus pr√©cise des √©motions affich√©es par les humains.

\\
***Differential Privacy and Federated Learning Explained***

\\
L'une des consid√©rations √©thiques √† prendre en compte lors de la construction de syst√®mes d'IA est la garantie du respect de la vie priv√©e. Actuellement, cela peut √™tre r√©alis√© de deux mani√®res, soit en utilisant une intimit√© diff√©rentielle, soit par un apprentissage f√©d√©r√©. Si vous voulez en savoir plus sur ces sujets, Jordan Harrod nous fournit une excellente introduction dans cette [vid√©o](https://www.youtube.com/watch?v=MOcTGM_UteM) qui comprend √©galement une session de pratique avec l'utilisation d'un notebook.


# Articles et Blog ‚úçÔ∏è

***Plong√©e dans le Reformer***

\\
Madison May a √©crit un nouvel [article sur son blog](https://www.pragmatic.ml/reformer-deep-dive/) consacr√© au [Reformer]( https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html), propos√© par Google AI. Nous avons √©galement pr√©sent√© le Reformer dans un [pr√©c√©dent num√©ro](https://medium.com/dair-ai/nlp-newsletter-reformer-deepmath-electra-tinybert-for-search-vizseq-open-sourcing-ml-68d5b6eed057) de la newsletter. Pour une explication en fran√ßais, vous pouvez consulter l‚Äôillustration [suivante](https://lbourdois.github.io/blog/nlp/Reformer/).

\\
***Une plateforme de blogging gratuite***

\\
[fastpages]( https://fastpages.fast.ai/fastpages/jupyter/2020/02/21/introducing-fastpages.html) vous permet de cr√©er automatiquement et gratuitement un blog en utilisant les pages GitHub. Cette solution simplifie le processus de publication d'un blog et prend √©galement en charge l'utilisation de documents Word et de notebook Jupyter.

\\
***Conseils pour un entretien chez Google***

\\
Pablo Castro, de l'√©quipe Google Brain, a publi√© un [article sur son blog](https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html) mettant en avant une liste de conseils pour les personnes int√©ress√©es par un entretien d'embauche chez Google. Parmi les sujets abord√©s figurent des conseils sur la fa√ßon de se pr√©parer √† l'entretien, sur ce √† quoi il faut s'attendre pendant l'entretien et sur ce qui se passe apr√®s l'entretien.

\\
***Les Transformers sont des Graph Neural Networks***

\\
Les r√©seaux neuronaux de graphes (GNN) et les Transformers se sont av√©r√©s efficaces pour diff√©rentes t√¢ches de NLP. Pour mieux comprendre le fonctionnement interne de ces approches et leurs relations, Chaitanya Joshi a √©crit un [article](https://graphdeeplearning.github.io/post/transformers-are-gnns/) expliquant la connexion entre les GNN et les Transformers, ainsi que les diff√©rentes fa√ßons dont ces m√©thodes peuvent √™tre combin√©es dans une sorte de mod√®le hybride.

\\
***CNNs et Equivariance***

\\
Fabian Fuchs et Ed Wagstaff [discutent](https://fabianfuchsml.github.io/equivariance1of2/) de l'importance de l'√©quivariance et de la mani√®re dont les CNN la font respecter. Le concept d'√©quivariance est d'abord d√©fini, puis discut√© dans le contexte de CNN appliqu√©s √† la traduction.

\\
***L‚Äôauto-apprentissage avec les images***

\\
L‚Äôauto-apprentissage a √©t√© beaucoup √©voqu√© dans les pr√©c√©dents num√©ros de la newsletter en raison du r√¥le qu‚Äôil a jou√© dans les techniques modernes de mod√©lisation des langues. Ce [billet](https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%e7%bd%91/) de Jonathan Whitaker fournit une explication de l‚Äôauto-apprentissage dans le contexte des images. Si le sujet vous int√©resse, Amit Chaudhary a √©galement √©crit un [article](https://amitness.com/2020/02/illustrated-self-supervised-learning/) d√©crivant le concept de mani√®re visuelle.


# Education üéì
***Stanford CS330: Deep Multi-Task et Meta-Learning***

\\
Stanford a r√©cemment publi√© une [playlist vid√©o YouTube](https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5) sur son nouveau cours consacr√© √† l‚Äôapprentissage multi-t√¢ches et au m√©ta-apprentissage. Parmi les sujets abord√©s, citons le m√©ta-apprentissage bay√©sien, le lifelong apprentissage, une introduction √† l'apprentissage par renforcement, etc...

\\
***Notebooks PyTorch***

\\
dair.ai publie une [s√©rie de notebook](https://github.com/dair-ai/pytorch_notebooks) qui visent √† vous faire d√©couvrir les r√©seaux neuronaux profonds √† l'aide de PyTorch. Il s'agit d'un travail en cours et certains sujets d'actualit√© comprennent la fa√ßon de mettre en ≈ìuvre un mod√®le de r√©gression logistique √† partir de z√©ro et la fa√ßon de programmer un NN ou un RNN √† partir de z√©ro. Les notebooks sont √©galement disponibles dans le d√©p√¥t GitHub.

\\
***Le livre de fastai book (√©bauche) ***

\\
Jeremy Howard et Sylvain Gugger ont publi√© une [liste compl√®te](https://github.com/fastai/fastbook) de notebooks (non termin√©s) en vue de leur prochain cours qui pr√©sentera des concepts de deep learning et diff√©rentes m√©thodes d‚Äôutilisation de PyTorch et la librairie fastai.

\\
***Cours gratuits sur la datascience***

\\
Au cas o√π vous l'auriez manqu√©, Kaggle propose une s√©rie de [petits cours gratuits](https://www.kaggle.com/learn/overview) sur les outils de datascience. Certains de ces cours comprennent, entre autres, l'explication de l'apprentissage machine, une introduction √† l'apprentissage machine et √† Python, la visualisation de donn√©es, l'ing√©nierie des fonctionnalit√©s et l'apprentissage approfondi.

\\
Un autre [cours de datascience en ligne](https://lewtun.github.io/dslectures/) propose un programme, des diapositives et des notebooks sur l'analyse exploratoire des donn√©es, l'interpr√©tation des mod√®les ou encore le NLP.

\\
***8 cr√©ateurs et contributeurs parlent de leurs librairies PyTorch***

\\
nepture.ai a publi√© un vaste [article](https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&utm_medium=tweet&utm_campaign=blog-model-training-libraries-pytorch-ecosystem) qui contient des discussions d√©taill√©es avec les principaux cr√©ateurs et contributeurs de PyTorch (parcours, philosophie du projet, outils qui l'entourent, etc‚Ä¶).

\\
***Visualisation des mod√®les adaptatifs d'attention r√©duite***

\\
Sasha Rush partage un [notebook](https://colab.research.google.com/drive/1EB7MI_3gzAR1gFwPPO27YU9uYzE_odSu) qui explique et montre les d√©tails techniques sur la mani√®re de produire des sorties softmax sparses. Il aborde √©galement la fa√ßon d'induire la sparcit√© dans la composante d‚Äôattention d'un mod√®le de Transformer, ce qui aide √† produire une probabilit√© nulle pour les mots non pertinents dans un contexte donn√©, am√©liorant ainsi la performance et l'interpr√©tabilit√©.

\\
![](https://cdn-images-1.medium.com/max/800/1*7BB322LlVgt1zzk-cviSoA.png)


# Mentions sp√©ciales ‚≠êÔ∏è

Conor Bell a cod√© un [script python](https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4) qui vous permet de visualiser et de pr√©parer facilement un jeu de donn√©es pouvant √™tre utilis√© pour un mod√®le StyleGAN.

\\
Manu Romero [a contribu√©](https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos) au fine-tuning d‚Äôun mod√®le POS pour l'espagnol. Le mod√®le est sur la librairie Transfomers d‚ÄôHugging Face. Il sera int√©ressant de voir cet effort dans d'autres langues.

\\
Ce [r√©pertoire Github](https://github.com/tomohideshibata/BERT-related-papers) contient une longue liste de documents r√©dig√©s sur BERT qui abordent diff√©rents probl√®mes tels que la compression de mod√®les, les domaines sp√©cifiques, le multi-mod√®le, la g√©n√©ration, les t√¢ches en aval, etc.

\\
Connor Shorten a publi√© une courte [vid√©o](https://www.youtube.com/watch?time_continue=79&v=-Bh_7tzyoR4&feature=emb_logo) de 15 minutes expliquant un framework √† aborder pour r√©duire l'effet des "raccourcis" dans l'auto-apprentissage de la repr√©sentation. C'est important car, s'il n'est pas bien fait, le mod√®le peut ne pas apprendre des repr√©sentations s√©mantiques utiles et se r√©v√©ler potentiellement inefficace dans un contexte d'apprentissage par transfert.

\\
Sebastian Ruder a publi√© un nouveau num√©ro de sa newsletter qui met en lumi√®re des sujets et des ressources allant d'analyses d‚Äôarticles sur le NLP et le ML de 2019 √† des diapositives sur l'apprentissage par transfert et des √©l√©ments essentiels sur le deep learning. Vous pouvez le consulter [ici]( http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195).

----------

Vous pouvez retrouver la pr√©c√©dente newsletter [ici](https://dair.ai/NLP_Newsletter_-5_-FR/)  

\\
Si vous avez des jeux de donn√©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine √©dition de la newletter, n'h√©sitez pas √† me contacter √† ellfae@gmail.com ou par message sur [Twitter](https://twitter.com/omarsar0).

\\
[Abonnez-vous]( https://dair.ai/newsletter/) pour recevoir les prochains num√©ros dans votre bo√Æte mail.
