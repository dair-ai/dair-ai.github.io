---
layout: post
title: "NLP Newsletter [FR] #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,‚Ä¶"
author: lbourdois
excerpt: ""
modified:
comments: true
tags: [nlp_newsletter]
image:
  thumb: nlp_newsletter_5.png
---


![](https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png)


# Avant-propos
Tout d'abord, je ne saurais trop tous vous remercier pour le soutien et les encouragements incroyables que vous avez apport√©s √† cette newsletter. Son √©laboration n√©cessite des recherches et une r√©daction fastidieuse que je trouve √† la fois enrichissantes et utiles, afin de vous fournir le meilleur contenu. J'esp√®re que vous les appr√©ciez, car c'est le cas pour moi. üòâ


# Publications üìô

***Une compr√©hension th√©orique de l'autodistillation***

\\
L'[autodistillation]( https://arxiv.org/pdf/1503.02531.pdf) est le processus de transfert de connaissances d'une architecture √† une seconde qui est identique. Les pr√©dictions du mod√®le original sont transmises comme valeurs cibles au second mod√®le pendant la phase d‚Äôentra√Ænement. Outre les propri√©t√©s souhaitables, comme la r√©duction de la taille du mod√®le, les r√©sultats empiriques montrent que cette approche fonctionne bien sur des ensembles de donn√©es maintenus. Un groupe de chercheurs a r√©cemment publi√© un article qui fournit une analyse th√©orique visant √† mieux comprendre ce qui se passe dans ce processus et pourquoi il est efficace. Les r√©sultats montrent que quelques cycles d'autodistillation amplifient la r√©gularisation ([en limitant progressivement le nombre de fonctions de base qui repr√©sentent la solution](https://twitter.com/TheGradient/status/1228132843630387201?s=20)), ce qui tend √† r√©duire le sur-apprentissage. (Lire l'article complet [ici](https://arxiv.org/abs/2002.05715))

\\
![](https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png)

[*source*](https://arxiv.org/abs/2002.05715)

\\
***Les ann√©es 2010 : Une d√©cennie d'apprentissage approfondi / Perspectives pour les ann√©es 2020***

\\
[J√ºrgen Schmidhuber](http://people.idsia.ch/~juergen/), un pionnier de l'intelligence artificielle, a r√©cemment publi√© un [article sur son blog](http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html) qui se concentre sur un aper√ßu historique de l'apprentissage profond depuis 2010. Parmi les sujets abord√©s, citons les LSTM, les feedforward NN, les GAN, l'apprentissage par renforcement, le m√©ta-apprentissage, la distillation, l'apprentissage par l'attention, etc. L'article se termine par une perspective sur les ann√©es 2020, encourageant l'attention sur des questions urgentes telles que la vie priv√©e et les march√©s des donn√©es.

\\
***Utilisation des r√©seaux de neurones pour r√©soudre des √©quations math√©matiques***

\\
Les chercheurs du FAIR de Facebook ont publi√© un [article]( https://arxiv.org/abs/1912.01412) qui propose un mod√®le entra√Æn√© sur des probl√®mes math√©matiques ainsi que leurs solutions associ√©es, afin d‚Äôapprendre √† pr√©dire les solutions possibles pour des t√¢ches telles que la r√©solution de probl√®mes d'int√©gration. L'approche est bas√©e sur une approche similaire √† celle utilis√©e dans la traduction automatique o√π les expressions math√©matiques sont repr√©sent√©es comme une sorte de langage et les solutions sont trait√©es comme le probl√®me de traduction. Ainsi, au lieu que le mod√®le produise une traduction, le r√©sultat est la solution elle-m√™me. Les chercheurs affirment ainsi que les r√©seaux neuronaux profonds ne sont pas seulement bons pour le raisonnement symbolique, mais aussi pour des t√¢ches plus diverses.

\\
![](https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png)

*√âquations fournies en entr√©e avec la solution correspondante fournie par le mod√®le ‚Äì* [*source*](https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/)


# Cr√©ativit√© et soci√©t√© üé®

***L‚ÄôIA au service de la d√©couverte scientifique***

\\
Mattew Hutson [rapporte](https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times) comment l‚ÄôIA peut √™tre utilis√©e pour produire des √©mulateurs qui ont une utilit√© importante dans la mod√©lisation de ph√©nom√®nes naturels complexes qui pourraient, √† leur tour, conduire √† diff√©rents types de d√©couvertes scientifiques. Le d√©fi avec la construction de ces √©mulateurs est qu'ils n√©cessitent souvent d‚Äôimportantes donn√©es et une exploration approfondie des param√®tres. Un [article](https://arxiv.org/abs/2001.08055) r√©cent propose DENSE, une approche bas√©e sur la [recherche d'architecture neurale](https://en.wikipedia.org/wiki/Neural_architecture_search) pour construire des √©mulateurs pr√©cis tout en ne s'appuyant que sur une quantit√© limit√©e de donn√©es d'entra√Ænement. Ils l'ont test√©e en effectuant des simulations pour des cas tels que l'astrophysique, la climatologie et la fusion, entre autres.

\\
***Am√©liorer la ¬´ traduction ¬ª de l'image √† l'illustration***

\\
GANILLA est une approche qui propose l'utilisation de GAN pour am√©liorer le transfert √† la fois du style et du contenu dans la [t√¢che de traduction d'image √† image](https://paperswithcode.com/task/image-to-image-translation) non appari√©e. En particulier, un mod√®le d'illustration d'image √† image est propos√© (avec un r√©seau g√©n√©rateur am√©lior√©) et √©valu√© sur la base d'un nouveau cadre d'√©valuation quantitative qui prend en compte √† la fois le contenu et le style. La nouveaut√© de ce travail r√©side dans le r√©seau g√©n√©rateur propos√© qui tient compte d'un √©quilibre entre le style et le contenu, ce que les mod√®les pr√©c√©dents n'ont pas r√©ussi √† atteindre. Des codes et mod√®les pr√©-entrain√©s sont mis √† [disposition](https://github.com/giddyyupp/ganilla). Lisez le document complet [ici](https://arxiv.org/abs/2002.05638).

\\
![](https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png)

\\
***Andrew Ng parle de l'int√©r√™t pour l‚Äôauto-apprentissage***

\\
Andrew Ng, le fondateur de deeplearning.ai, est intervenu dans un podcast sur l'intelligence artificielle pour [parler](https://www.youtube.com/watch?v=0jspaMLxBig) de sujets tels que ses d√©buts dans le ML, l'avenir de l'IA, l'enseignement de l'IA, ses recommandations pour une bonne utilisation du ML, ses objectifs personnels et les techniques de ML auxquelles il faudra pr√™ter attention dans les ann√©es 2020.

\\
Andrew a expliqu√© pourquoi il est tr√®s enthousiaste √† l'id√©e de s‚Äôinitier √† l‚Äôauto-apprentissage. [L'auto-apprentissage supervis√©](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html) consiste √† formuler un probl√®me d'apprentissage dont le but est d‚Äôobtenir une supervision √† partir des donn√©es elles-m√™mes. L‚Äôint√©r√™t est d'utiliser de grandes quantit√©s de donn√©es non lab√©lis√©es, ce qui est plus disponibles en plus grande quantit√© que les donn√©es lab√©lis√©es. Les repr√©sentations, par opposition √† l'ex√©cution de la t√¢che, sont importantes et peuvent √™tre utilis√©es pour traiter des t√¢ches en aval, comme c'est le cas dans les mod√®les linguistiques tels que le [BERT](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert).

\\
Il y a √©galement beaucoup d'int√©r√™t √† utiliser l'auto-apprentissage supervis√©](pour apprendre des repr√©sentations visuelles g√©n√©ralis√©es qui rendent les mod√®les plus pr√©cis dans des environnements √† faibles ressources. Par exemple, une m√©thode r√©cente appel√©e [SimCLR](https://arxiv.org/abs/2002.05709) (dirig√©e par Geoffrey Hinton) propose un cadre pour am√©liorer les r√©sultats de la classification des images dans diff√©rents contextes tels que l'apprentissage par transfert et l'apprentissage semi-supervis√©.

\\
![](https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png)

[*source*](https://arxiv.org/abs/2002.05709)


# Outils et jeux de donn√©es ‚öôÔ∏è

***Les libraries li√©es √† JAX***

\\
[JAX](https://github.com/google/jax) est une nouvelle biblioth√®que qui combine NumPy et la diff√©renciation automatique pour mener des recherches de ML de haut niveau. Afin de simplifier les pipelines utilisant JAX, DeepMind a publi√© [Haiku](https://github.com/deepmind/dm-haiku) et [RLax](https://github.com/deepmind/rlax). RLax simplifie l'impl√©mentation de mod√®les bas√©s sur l‚Äôapprentissage par renforcement et Haiku simplifie la construction de r√©seaux neuronaux en utilisant des mod√®les de programmation orient√©s objet.

\\
***Un outil de traitement des donn√©es Wikip√©dia***

\\
[Sparkwiki](https://github.com/epfl-lts2/sparkwiki) est un outil permettant de traiter les donn√©es de Wikip√©dia. Cette version s'inscrit dans le cadre de nombreux efforts visant √† permettre des recherches int√©ressantes en mati√®re d'analyse comportementale, telles que la [capture des tendances et des biais linguistiques dans les diff√©rentes √©ditions linguistiques de Wikip√©dia](https://arxiv.org/abs/2002.06885). Les auteurs ont d√©couvert qu'ind√©pendamment de la langue, le comportement de navigation des utilisateurs de Wikip√©dia montre qu'ils ont tendance √† partager des int√©r√™ts communs comme par exemples les films, la musique et le sport, mais que les diff√©rences deviennent plus apparentes avec les √©v√©nements locaux et les particularit√©s culturelles.

\\
![](https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg)

\\
***Mise √† jour de la librairie Transformers***

\\
Une [nouvelle version](https://github.com/huggingface/transformers/releases/tag/v2.5.0) de la librairie Transformers d‚ÄôHugging Face est disponible. Elle comprend l'int√©gration de leur librairie Tokenizer qui vise √† acc√©l√©rer des mod√®les comme BERT, RoBERTa, GPT2, et d'autres mod√®les construits par la communaut√©.


# Ethique en IA üö®

***Consid√©rations √©thiques pour les mod√®les de NLP et de ML***

\\
Dans un nouvel [√©pisode](https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender) des [NLP Highlights](https://soundcloud.com/nlp-highlights), Emily Bender et les intervenants parlent de certaines consid√©rations √©thiques qui peuvent se poser lors du d√©veloppement de mod√®les de NLP dans un contexte d‚Äôutilisation universitaire et grand public. Parmi les sujets abord√©s, citons les consid√©rations √©thiques lors de la conception des t√¢ches de NLP, les approches de collecte de donn√©es et, finalement, la publication des r√©sultats.
En plus de toutes les consid√©rations ci-dessus, une pr√©occupation qui est toujours discut√©e dans la communaut√© de l'IA est de se concentrer trop sur l'optimisation d'une mesure, ce qui va √† l'encontre des fondements de ce que l'IA vise √† atteindre (c√†d une IA g√©n√©rale). Rachel Thomas et David Uminsky discutent des erreurs possibles en [analysant de mani√®re approfondie]( https://arxiv.org/abs/2002.08512) diff√©rents cas d'utilisation. Ils proposent √©galement un cadre simple pour att√©nuer le probl√®me, qui implique l'utilisation et la combinaison de plusieurs mesures, suivies par l'implication des personnes directement concern√©es par la technologie.

# Articles et Blog ‚úçÔ∏è

***Le GPT2 annot√©***

\\
Aman Arora a r√©cemment publi√© un article sur son blog, intitul√© le "[The Annotated GPT-2](https://amaarora.github.io/2020/02/18/annotatedGPT2.html)", qui explique le fonctionnement interne du GPT-2. Son approche s'inspire de "[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)" qui a adopt√© une approche d'annotation pour expliquer les parties importantes du mod√®le par le biais de code et d'explications faciles √† suivre. Aman a fait de gros efforts pour r√©impl√©menter le GPT-2 d'OpenAI en utilisant PyTorch et la biblioth√®que Transformers de Hugging Face.

\\
![](https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png)

[*source*](https://amaarora.github.io/2020/02/18/annotatedGPT2.html)

\\
***Au-del√† de BERT ?***

\\
Sergi Castella expose son [point de vue]( https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1) sur ce qui se trouve au-del√† de BERT. Les principaux sujets abord√©s sont l'am√©lioration des mesures, la fa√ßon dont la librairie Transformers d‚ÄôHuggingfFace permet de faire des recherches, les jeux de donn√©es int√©ressants √† consulter, etc‚Ä¶

\\
***Op√©rateur de compression matricielle***

\\
TensorFlow blog a publi√© un [article](https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016) expliquant les techniques et l'importance de la compression des matrices dans un mod√®le de r√©seau neuronal profond. La compression des matrices peut aider √† construire des mod√®les petits plus efficaces qui peuvent √™tre incorpor√©s dans des appareils tels que les t√©l√©phones et les assistants vocaux. En se concentrant sur la compression des mod√®les par des m√©thodes telles que la low-rank-approximation et la quantization, nous n'avons pas besoin de compromettre la qualit√© du mod√®le.

\\
![](https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png)

[*source*](https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016)


# Education üéì

***Les bases du NLP***

\\
Elvis a publi√© une √©bauche du chapitre 1 de sa nouvelle s√©rie intitul√©e "[Les bases du NLP](https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684)". Il enseigne les concepts du NLP en partant des bases tout en partageant les meilleures pratiques, les r√©f√©rences importantes, et les erreurs courantes √† √©viter. Un [Google Colab](https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ) est disponible et le projet sera maintenu [ici](https://github.com/dair-ai/nlp_fundamentals).

\\
***[Online] Review/Discussion: Part I Mathematical Foundations Reading Session***

\\
Machine Learning Tokyo organise une discussion en ligne sur les chapitres qui ont √©t√© couverts lors de leurs r√©centes sessions d'√©tude en ligne. Le groupe avait auparavant √©tudi√© des chapitres bas√©s sur le livre intitul√© [Mathematics For Machine Learning](https://mml-book.github.io/), √©crit par Marc Peter Deisenroth, A Aldo Faisal et Cheng Soon Ong. L'√©v√©nement est pr√©vu pour le 8 mars 2020.

\\
***Recommandations de livres***

\\
Dans une partie pr√©c√©dente, nous avons discut√© de l'importance de la compression matricielle pour la construction de petits mod√®les de ML. Si vous souhaitez en savoir plus sur la fa√ßon de construire des r√©seaux neuronaux profonds plus petits pour les syst√®mes embarqu√©s, consultez cet excellent livre intitul√© [TinyML](https://tinymlbook.com/?linkId=82595412), √©crit par Pete Warden et Daniel Situnayake.

\\
![](https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg)

[*source*](https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043)

\\
Un autre livre int√©ressant √† surveiller et qui est √† para√Ætre est "[Deep Learning for Coders with fastai and PyTorch" : AI Applications Without a PhD](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527)" de Jeremy Howard et Sylvain Gugger. Ce livre vise √† fournir les bases math√©matiques n√©cessaires pour construire et former des mod√®les permettant d'aborder des t√¢ches dans les domaines de computer vision et du NLP.

\\
![](https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg)


[*source*](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527)


# Mentions sp√©ciales ‚≠êÔ∏è


[Torchmeta](https://arxiv.org/abs/1909.06576) est une librairie qui permet d'utiliser facilement des chargeurs de donn√©es connexes pour la recherche sur le m√©ta-apprentissage. Elle a √©t√© r√©dig√©e par Tristan Deleu.

\\
Manuel Tonneau a √©crit un [article](https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter) offrant un regard plus approfondi sur certains des m√©canismes impliqu√©s dans la mod√©lisation du langage. Parmi les sujets abord√©s, citons la greedy recherche, la beam recherche et l'√©chantillonnage de noyaux.

\\
Le MIT [publie](http://introtodeeplearning.com/) le programme complet et le calendrier du cours intitul√© "Introduction to Deep Learning". L'objectif est de publier chaque semaine des vid√©os et des diapositives.

\\
Apprenez comment entra√Æner un mod√®le de reconnaissance d'entit√©s nomm√©es (NER) en utilisant une approche bas√©e sur [Transformers](https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py) en moins de 300 lignes de code. Vous pouvez trouver le programme Google Colab qui l'accompagne [ici](https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn).

----------

Vous pouvez retrouver la pr√©c√©dente newsletter [ici](https://dair.ai/NLP_Newsletter_-4_-FR/)  

\\
Si vous avez des jeux de donn√©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine √©dition de la newletter, n'h√©sitez pas √† me contacter √† ellfae@gmail.com ou par message sur [Twitter](https://twitter.com/omarsar0). 

\\
[Abonnez-vous]( https://dair.ai/newsletter/) pour recevoir les prochains num√©ros dans votre bo√Æte mail.
