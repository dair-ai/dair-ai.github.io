---
layout: post
title: "NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, √âtica em NLP, Torchmeta,‚Ä¶"
author: fclesio
modified:
comments: true
excerpt: "Esta edi√ß√£o abrange t√≥picos como a compreens√£o de self-distillation, tradu√ß√£o de imagem para ilustra√ß√£o, considera√ß√µes √©ticas para modelos de NLP, etc."
tags: [nlp_newsletter]
image:
  thumb: nlp_newsletter_5.png
---


![](https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png)

\\
Antes de tudo, gostaria de agradecer de ‚ù§Ô∏è a todos voc√™s pelo incr√≠vel apoio e incentivo para continuar com a NLP Newsletter. Esse esfor√ßo requer pesquisa, edi√ß√£o, e tradu√ß√£o tediosas, mas que considero gratificantes e √∫teis para fornecer o melhor conte√∫do. Espero que voc√™ esteja gostando deste conte√∫do. üòâ

\\
[*Assine a NLP Newsletter*](https://dair.ai/newsletter/) *üîñ para receber edi√ß√µes futuras via e-mail.*

# Publica√ß√µes üìô

***Um entendimento te√≥rico do self-distillation***

\\
No contexto de Deep Learning, [*self-distillation*](https://arxiv.org/pdf/1503.02531.pdf) (*NT: auto-destila√ß√£o*) √© o processo de transfer√™ncia de conhecimento de uma arquitetura para outra. As previs√µes do modelo original s√£o alimentadas como valores de destino para o outro modelo durante o treinamento. Al√©m de ter propriedades desej√°veis como a redu√ß√£o do tamanho dos modelos, os resultados emp√≠ricos mostram que essa abordagem funciona bem em conjuntos de dados n√£o vistos anteriormente pelo modelo (NT: amostras _held out_). Um grupo de pesquisadores publicou recentemente um artigo que fornece uma an√°lise te√≥rica com o foco em um melhor entendimento sobre o que est√° acontecendo neste processo de _destila√ß√£o do conhecimento_ e o porque ele √© eficaz. Os resultados mostram que alguns poucos ciclos de destila√ß√£o amplificam a regulariza√ß√£o (devido ao fato que a t√©cnica [*progressivamente ajuda a limitar o n√∫mero de fun√ß√µes base que representam a solu√ß√£o*](https://twitter.com/TheGradient/status/1228132843630387201?s=20)) as quais tendem a reduzir o over-fitting. (Leia o paper [**aqui**](https://arxiv.org/abs/2002.05715))


\\
![](https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png)

[*Fonte*](https://arxiv.org/abs/2002.05715)

\\
***Os anos 2010s: Nossa d√©cada de Deep Learning / Perspectivas para os 2020s***

\\
[J√ºrgen Schmidhuber,](http://people.idsia.ch/~juergen/) um dos pioneiros em Intelig√™ncia Artificial,  [**postou recentemente em seu blog**](http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html) uma vis√£o hist√≥rica sobre Deep Learning desde o ano de 2010. Alguns t√≥picos incluem [LSTMs](https://en.wikipedia.org/wiki/Long_short-term_memory), [feedforward neural networks](https://en.wikipedia.org/wiki/Feedforward_neural_network), [GANs](https://en.wikipedia.org/wiki/Generative_adversarial_network), [deep reinforcement learning](https://en.wikipedia.org/wiki/Deep_reinforcement_learning), [meta-learning](https://en.wikipedia.org/wiki/Meta_learning_(computer_science)), world models, [distilling NNs](https://arxiv.org/abs/1503.02531), [attention learning](https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f), etc. O artigo traz algumas perspectivas futuras para os anos 2020 chamando aten√ß√£o para quest√µes como privacidade e mercado de dados.

\\
***Usando Redes Neurais para a resolu√ß√£o de equa√ß√µes matem√°ticas***

\\
Pesquisadores do Facebook AI publicaram um [**paper**](https://arxiv.org/abs/1912.01412) em que apresentam um modelo treinado em problemas de matem√°tica para prever poss√≠veis solu√ß√µes para in√∫meras tarefas como, por exemplo, problemas de integra√ß√£o. A abordagem √© baseada em uma nova estrutura semelhante √† usada na [neural machine translation](https://en.wikipedia.org/wiki/Neural_machine_translation) (*NT: tradu√ß√£o autom√°tica neural*), em que express√µes matem√°ticas s√£o representadas como um tipo de linguagem e as solu√ß√µes tratadas como um problema de tradu√ß√£o. Assim, ao inv√©s do modelo produzir uma tradu√ß√£o, a sa√≠da desta tradu√ß√£o √© a pr√≥pria solu√ß√£o do problema. Com isso, os pesquisadores afirmam que as redes Deep Learning n√£o s√£o apenas boas em racioc√≠nio simb√≥lico, mas podem ser usadas tamb√©m para tarefas mais diversas.

\\
![](https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png)

*Equa√ß√µes sendo usadas como entrada, juntamente com a solu√ß√£o correspondente gerada pelo modelo‚Äî*‚Ää[*fonte*](https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/)


# Criatividade e Sociedade üé®

***Intelig√™ncia Artificial para descobertas cient√≠ficas***

\\
Mattew Hutson [**informa**](https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times) como a intelig√™ncia artificial (IA) pode ser utilizada para produzir emuladores que t√™m um uso importante na modelagem de fen√¥menos naturais complexos e que, por sua vez, podem levar a diferentes tipos de *descobertas cient√≠ficas*. A mudan√ßa na constru√ß√£o desses emuladores acontece devido ao fato de que estes modelos geralmente exigem dados em larga escala e uma vasta explora√ß√£o de par√¢metros. Um [**paper recente**](https://arxiv.org/abs/2001.08055) prop√µe um m√©todo chamado DENSE que √© uma abordagem baseada em [*neural architecture search (NAS)*](https://en.wikipedia.org/wiki/Neural_architecture_search) (NT: Explora√ß√£o e busca de arquitetura de Redes Neurais) para criar emuladores precisos, contando apenas com uma quantidade limitada de dados de treinamento. Eles o testaram executando simula√ß√µes para casos que incluem astrof√≠sica, ci√™ncia clim√°tica e energia de fus√£o, entre outros.

\\
***Melhorando a tradu√ß√£o de imagem para imagem***

\\
[GANILLA](https://arxiv.org/abs/2002.05638) √© uma abordagem que prop√µe o uso de [GANs](https://en.wikipedia.org/wiki/Generative_adversarial_network) para melhorar a transfer√™ncia de estilo e conte√∫do em pares para tarefas de tradu√ß√£o [*image-to-image*](https://paperswithcode.com/task/image-to-image-translation) (NT: imagem para imagem). A abordagem prop≈çe um modelo de imagem para imagem (com uma rede de geradores aprimorada) e este modelo √© avaliado com base em uma nova estrutura de avalia√ß√£o quantitativa que considera tanto o conte√∫do quanto o estilo. A novidade do trabalho est√° na rede de geradores proposta, que considera um equil√≠brio entre estilo e conte√∫do que os modelos anteriores n√£o conseguem. O c√≥digo e os modelos pr√©-treinados est√£o [dispon√≠veis](https://github.com/giddyyupp/ganilla). Leia o artigo completo [**aqui**](https://arxiv.org/abs/2002.05638).

\\
![](https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png)

\\
***Andrew Ng fala sobre o interesse em aprendizagem auto-supervisionada***

\\
Andrew Ng, o fundador do [deeplearning.ai](deeplearning.ai), falou no [**podcast de Intelig√™ncia Artificial do Lex Friedman**](https://www.youtube.com/watch?v=0jspaMLxBig) sobre os seguintes t√≥picos: seus primeiros anos em ML, o futuro da IA, educa√ß√£o em IA, recomenda√ß√µes para o uso adequado da ML, seus objetivos pessoais e quais t√©cnicas de ML que devemos prestar aten√ß√£o nesta d√©cada de 2020.

\\
Andrew explicou o motivo da sua anima√ß√£o em rela√ß√£o ao *self-supervised representation learning.* [**Self-supervised learning**](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html) (NT: aprendizado de representa√ß√£o auto-supervisionado) envolve a estrutura√ß√£o de um problema de aprendizagem que visa obter supervis√£o dos pr√≥prios dados para fazer uso de grandes quantidades de dados n√£o rotulados, o que √© mais comum que os dados rotulados limpos. As representa√ß√µes s√£o importantes e podem ser usadas para lidar com tarefas posteriores, semelhantes √†s usadas em modelos de linguagem como o [BERT](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert).


\\
Tamb√©m h√° muito interesse em usar o aprendizado auto-supervisionado para treinamento de representa√ß√µes visuais generalizadas que tornam os modelos mais precisos em ambientes com poucos recursos. Por exemplo, um m√©todo recente chamado [**SimCLR**](https://arxiv.org/abs/2002.05709) (liderado por [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton)) prop√µe uma estrutura para *aprendizagem auto-supervisionada contrastante* (*NT: contrastive self-supervised learning*) de representa√ß√µes visuais para melhorar a classifica√ß√£o de imagens em diferentes configura√ß√µes, como transfer√™ncia de aprendizado (NT: [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning)) e aprendizado semi-supervisionado.

\\
![](https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png)

[*fonte*](https://arxiv.org/abs/2002.05709)


# Ferramentas e Datasets ‚öôÔ∏è

***Bibliotecas JAX***

\\
[JAX](https://github.com/google/jax) √© uma nova biblioteca que combina o NumPy e [diferencia√ß√£o autom√°tica](https://en.wikipedia.org/wiki/Automatic_differentiation) para realizar pesquisas de ML de alto desempenho. Para simplificar os pipelines para a constru√ß√£o de redes neurais usando JAX, a [DeepMind](https://deepmind.com/) lan√ßou o [**Haiku**](https://github.com/deepmind/dm-haiku) e [**RLax**](https://github.com/deepmind/rlax). O RLax simplifica a implementa√ß√£o de agentes de aprendizado por refor√ßo e o Haiku simplifica a constru√ß√£o de redes neurais usando *modelos familiares com o paradigma de programa√ß√£o orientada a objetos.*

\\
***Uma ferramenta para processar dados da Wikip√©dia***

\\
[**Sparkwiki**](https://github.com/epfl-lts2/sparkwiki) √© uma ferramenta para processar dados da Wikip√©dia. Esta vers√£o faz parte de muitos esfor√ßos para permitir pesquisas interessantes de an√°lise comportamental, como [a captura de tend√™ncias e preconceitos em diferentes idiomas na Wikip√©dia](https://arxiv.org/abs/2002.06885). Os autores descobriram que, independentemente do idioma, o comportamento de navega√ß√£o dos usu√°rios da Wikip√©dia mostra que eles tendem a compartilhar interesses comuns por categorias como filmes, m√∫sica e esportes, mas as diferen√ßas se tornam mais aparentes com eventos locais e particularidades culturais.

\\
![](https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg)

\\
***Tokenizers em Rust, DistilBERT e outros***

\\
Um novo release dos [**Transformers**](https://github.com/huggingface/transformers/releases/tag/v2.5.0) da Hugging Face inclui a integra√ß√£o de sua biblioteca de tokeniza√ß√£o r√°pida, que visa acelerar modelos como o [BERT](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270), [RoBERTa](https://arxiv.org/abs/1907.11692), [GPT-2](https://openai.com/blog/better-language-models/) e outros modelos criados pela comunidade.


# √âtica em Intelig√™ncia Artificial üö®

***Considera√ß√µes √©ticas para modelos de NLP (Processamento de Linguagem Natural) e Machine Learning***

\\
Em um novo [**epis√≥dio**](https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender) do postcast [NLP Highlights,](https://soundcloud.com/nlp-highlights) [Emily Bender](https://twitter.com/emilymbender) e os hosts conversaram sobre algumas considera√ß√µes √©ticas no desenvolvimento de modelos e tecnologias de NLP no contexto da academia e do seu uso no mundo real. Alguns dos t√≥picos da discuss√£o incluem considera√ß√µes √©ticas nas tarefas de NLP, abordagens sobre coleta de dados e eventualmente considera√ß√µes na publica√ß√£o de resultados.


\\
Al√©m de todas as considera√ß√µes acima, uma preocupa√ß√£o discutida √© que a comunidade de IA est√° se concentrando demais na otimiza√ß√£o de m√©tricas espec√≠ficas, o que contraria os objetivos que a IA pretende alcan√ßar. Rachel Thomas e David Uminsky discutem os problemas dessa abordagem atrav√©s de uma [**an√°lise completa**](https://arxiv.org/abs/2002.08512) de diferentes casos de uso. Eles tamb√©m prop√µem uma estrutura simples para mitigar este problema, que envolve o uso e a combina√ß√£o de v√°rias m√©tricas, seguidas pelo envolvimento das pessoas afetadas diretamente pela tecnologia.


# Artigos e Blog posts ‚úçÔ∏è

**"The Annotated GPT-2"**

\\
Aman Arora publicou recentemente uma postagem no blog excepcionalmente intitulada ["The Annotated GPT-2‚Äú](https://amaarora.github.io/2020/02/18/annotatedGPT2.html) explicando o funcionamento interno do modelo baseado em [Transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) chamado [GPT-2](https://openai.com/blog/better-language-models/). Sua abordagem foi inspirada em [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) que adotou uma abordagem de anota√ß√£o para explicar as partes importantes do modelo. Aman fez um grande esfor√ßo para reimplementar o [GPT-2](https://openai.com/blog/better-language-models/) da [OpenAI](https://openai.com/) usando o [PyTorch](https://pytorch.org/) e a biblioteca [Transformers da Hugging Face](https://huggingface.co/transformers/). √â um trabalho brilhante!

\\
![](https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png)

[*fonte*](https://amaarora.github.io/2020/02/18/annotatedGPT2.html)

\\
***Al√©m do BERT?***

\\
[**Um ponto interessante foi levantado**](https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1) por Sergi Castella sobre o que est√° al√©m do [BERT](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270). Os principais t√≥picos incluem o aprimoramento das m√©tricas, uma reflex√£o de como a biblioteca [Transformers da Hugging Face](https://huggingface.co/transformers/) ajuda na pesquisa, alguns conjuntos de dados interessantes para an√°lise, etc.

\\
***Operador de Compress√£o de Matrizes***

\\
O Blog do TensorFlow publicou um [**post**](https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016) explicando as t√©cnicas e a import√¢ncia por tr√°s da compress√£o matrizes em um modelo de Deep Learning. *A compacta√ß√£o matricial* (*NT: Matrix compression*) pode ajudar a criar modelos menores e mais eficientes que podem ser incorporados a dispositivos menores, como telefones e assistentes dom√©sticos. Concentrar-se na compress√£o dos modelos por meio de m√©todos como [low-rank-approximation](https://en.wikipedia.org/wiki/Low-rank_approximation) e [quantiza√ß√£o](https://en.wikipedia.org/wiki/Quantization_(signal_processing)) significa que n√£o precisamos comprometer a qualidade do modelo.

\\
![](https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png)

[*fonte*](https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016)


# Educa√ß√£o üéì

***Fundamentos de NLP***

\\
Estou animado por lan√ßado um rascunho do Cap√≠tulo 1 da minha nova s√©rie chamado [**Fundamentos de NLP**](https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-senten√ßa-segmenta√ß√£o-b362c5d07684). Esta s√©rie ensina conceitos de NLP a partir do b√°sico, compartilhando boas pr√°ticas, refer√™ncias importantes, erros comuns a serem evitados e o que est√° por vir no que se refere a NLP. Um [notebook no Colab](https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ) foi inclu√≠do e o projeto ser√° mantido [aqui](https://github.com/dair-ai/nlp_fundamentals).

\\
![](https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif)

\\
***Revis√£o/Discuss√£o Online: Parte I sess√£o de leitura para fundamentos da matem√°tica***

\\
O time do [Meetup "Machine Learning Tokyo"](https://www.meetup.com/Machine-Learning-Tokyo/) est√° hospedando uma discuss√£o on-line remota, revisando cap√≠tulos que foram abordados em suas recentes sess√µes de estudo on-line. O grupo j√° havia estudado cap√≠tulos com base no livro [Mathematics For Machine Learning](https://mml-book.github.io/) escrito por Marc Peter Deisenroth, A Aldo Faisal e Cheng Soon Ong. O [**evento**](https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/) est√° programado para 8 de mar√ßo de 2020.

\\
***Recomenda√ß√µes de livros***

\\
Em um segmento anterior, discutimos a import√¢ncia da compress√£o de matriz para a constru√ß√£o de modelos pequenos (em termos de espa√ßo) de ML. Se voc√™ estiver interessado em aprender mais sobre como construir redes neurais profundas menores para sistemas embarcados, confira este √≥timo livro chamado [**TinyML**](https://tinymlbook.com/?linkId=82595412) de Pete Warden e Daniel Situnayake.

\\
![](https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg)

[*fonte*](https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043)

\\
Outro livro interessante para ficar de olho √© o pr√≥ximo t√≠tulo  **‚Äú**[**Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD**](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527)**‚Äù** de Jeremy Howard e Sylvain Gugger. O livro tem como objetivo fornecer a base matem√°tica necess√°ria para criar e treinar modelos para abordar tarefas nas √°reas de vis√£o computacional e NLP.

\\
![](https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg)


[*source*](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527)


# Men√ß√µes honrosas ‚≠êÔ∏è

Voc√™ pode acessar a NLP Newsletter anterior em PT-BR [aqui](https://dair.ai/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG/).

\\
[**Torchmeta**](https://arxiv.org/abs/1909.06576) √© uma biblioteca para pesquisa em meta-aprendizado. Esta biblioteca √© de autoria de Tristan Deleu.

\\
Manuel Tonneau escreveu um [**post**](https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter) oferecendo uma vis√£o mais detalhada em rela√ß√£o ao hardware envolvido em modelagem de linguagem. Alguns t√≥picos incluem *greedy* e [beam search](https://en.wikipedia.org/wiki/Beam_search) e [nucleus sampling](https://openreview.net/forum?id=rygGQyrFvH).

\\
O MIT [**lan√ßou**](http://introtodeeplearning.com/) o plano de estudos completo e a programa√ß√£o do curso intitulado "Introdu√ß√£o ao Deep Learning", incluindo v√≠deos das palestras j√° ministradas. Eles pretendem lan√ßar palestras em v√≠deo e slides uma vez por semana.

\\
Aprenda a treinar um modelo para [reconhecimento de entidade (NER)](https://en.wikipedia.org/wiki/Named-entity_recognition) usando uma abordagem baseada no Transformer em [**300 linhas de c√≥digo**](https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py). Voc√™ pode encontrar o Google Colab em anexo [aqui](https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn).

----------

Se voc√™ tiver datasets, projetos, postagens de blog, tutoriais ou documentos que deseja compartilhar na pr√≥xima edi√ß√£o da NLP Newsletter, entre em contato conosco pelo e-mail ellfae@gmail.com ou via [**DM no Twitter**](https://twitter.com/omarsar0).

\\
[*Assine a NLP Newsletter*](https://dair.ai/newsletter/) *üîñ para receber edi√ß√µes futuras via e-mail.*
