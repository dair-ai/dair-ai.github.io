<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<title>[pt-br]NLP_Newsletter_Tokenizers,_TensorFlow_2_1,_TextVe.html</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>

</head>

<body>

<h1>NLP Newsletter: <strong>Tokenizadores, TensorFlow 2.1, Vetoriza√ß√£o de Texto, TorchIO, D√©ficits de NLP, ‚Ä¶</strong></h1>

<hr />

<p><img src="https://cdn-images-1.medium.com/max/2400/1*gLVPodYjYd4YaF9sJbSpjg.png" alt="" /></p>

<p>Ol√° e Feliz Ano Novo! Devido a v√°rios pedidos, eu decidi trazer de volta a <strong>Newsletter de NLP</strong>. Dessa vez irei mant√™-la pequena e focada (tamb√©m mantida no <a href="https://github.com/dair-ai/nlp_newsletter">reposit√≥rio</a>). O objetivo dessa newsletter √© te manter informado em alguns dos mais recentes e interessantes t√≥picos relacionados a NLP e ML (em algumas categorias) sem tomar muito tempo do seu dia ocupado.</p>

<h1>Publica√ß√µes üìô</h1>

<p><strong><em>Sistema de IA para resson√¢ncia de c√¢ncer de mama </em></strong>
A DeepMind publicou um novo artigo na Nature intitulado ‚Äú<a href="https://www.nature.com/articles/s41586-019-1799-6">International evaluation of an AI system for breast cancer screening</a>‚Äù (Avalia√ß√£o internacional de um sistema de IA para resson√¢ncia de c√¢ncer de mama). De acordo com os autores, o trabalho √© sobre a avalia√ß√£o de um sistema de IA que ultrapassa especialistas humanos na resson√¢ncia de c√¢ncer de mama. Se isso √© realmente alcan√ß√°vel pelos sistemas de IA atuais ainda est√° em debate e h√° uma cr√≠tica cont√≠nua nesse tipo de sistema e como ele √© avaliado. Aqui est√° um <a href="https://www.nature.com/articles/d41586-019-03822-8">pequeno resumo</a> do artigo.</p>

<p><strong><em>Extra√ß√£o de Informa√ß√£o</em></strong>
O Pankaj Gupta liberou publicamento sua tese de Ph.D. intitulada ‚Äú<a href="https://www.researchgate.net/publication/336739252_PhD_Thesis_Neural_Information_Extraction_From_Natural_Language_Text">Neural Information Extraction From Natural Language Text</a>‚Äù (Extra√ß√£o neural de informa√ß√£o de texto de linguagem natural). A discuss√£o principal √© como extrair eficientemente as rela√ß√µes sem√¢nticas de texto em linguagem natural usando abordagens neurais. Tal esfor√ßo em pesquisa visa contribuir com a constru√ß√£o estruturada de bases de conhecimento, que possam ser usadas em uma s√©rie de aplica√ß√µes de NLP como busca na web, respostas de quest√µes, dentre outras.</p>

<p><strong><em>Recomenda√ß√µes Melhoradas</em></strong>
Pesquisadores no MIT e na IBM desenvolveram um <a href="http://news.mit.edu/2019/finding-good-read-among-billions-of-choices-1220">m√©todo</a> (publicado na NeurIPS ano passado) para categoriza√ß√£o, surgimento, e busca de documentos relevantes baseados numa combina√ß√£o de tr√™s ferramentas altamente usadas para an√°lise de texto: <em>modelagem de t√≥picos</em>, <em>embeddings de palavras</em>, e <em>transporte ideal</em>. O m√©todo tamb√©m d√° resultados promissores para a classifica√ß√£o de documentos. Tais m√©todos s√£o aplic√°veis em uma grande variedade de cen√°rios que requerem sugest√µes melhoradas e mais r√°pidas em grande escala tal como sistemas de busca e recomenda√ß√£o.</p>

<h1>ML e NLP Criatividade e Sociedade üé®</h1>

<p><strong><em>Carreiras em IA</em></strong>
O <a href="https://hai.stanford.edu/sites/g/files/sbiybj10986/f/ai_index_2019_report.pdf">relat√≥rio</a> do √≠ndice de IA de 2019 sugere que h√° mais demanda que suprimento de praticantes de IA. Entretanto, h√° v√°rios aspectos de empregos relacionados a IA como transi√ß√µes de carreira e entrevistas que ainda n√£o s√£o propriamente definidas.</p>

<p>Nesse <a href="https://towardsdatascience.com/how-i-found-my-current-job-3fb22e511a1f">post</a>, Vladimir Iglovivok detalha sua carreira e aventura em ML desde a constru√ß√£o de sistemas de recomenda√ß√£o tradicionais a constru√ß√£o de modelos espetaculares de vis√£o computacional que ganharam competi√ß√µes no Kaggle. Ele agora trabalha em ve√≠culos aut√¥nomos na Lyft, mas a <a href="https://towardsdatascience.com/how-i-found-my-current-job-3fb22e511a1f">jornada</a> de chegar l√° n√£o foi t√£o f√°cil.</p>

<p>Se voc√™ est√° realmente interessado e s√©rio em uma carreira em IA, a companhia do Andrew Ng, deeplearning.ai, fundou Workera, que visa a especificamente ajudar cientistas de dados e engenheiros de aprendizado de m√°quina com suas carreiras em IA. Obtenha o relat√≥rio oficial <a href="https://workera.ai/candidates/report">aqui</a>.</p>

<h1>Ferramentas e Base de Dados de ML/NLP ‚öôÔ∏è</h1>

<p><strong><em>Um tokenizador ultrarr√°pido</em></strong>
Hugging Face, a startup de NLP por tr√°s dos Transformers, liberou de forma open-sourced os Tokenizers, uma implementa√ß√£o ultrarr√°pida de tokeniza√ß√£o que pode ser usada em pipelines modernos de NLP. Cheque o <a href="https://github.com/huggingface/tokenizers">reposit√≥rio no GitHub </a> para a documenta√ß√£o em como usar os Tokenizers.</p>

<p><img src="https://cdn-images-1.medium.com/max/1600/1*BGcXk6Yf9fXGZlEtxz1hcg.jpeg" alt="" /></p>

<p><em>Tokenizers‚Ää‚Äî‚ÄäPython</em></p>

<p>O TensorFlow 2.1 incorpora uma nova camada de <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization">Vetoriza√ß√£o de Texto</a> que permite que voc√™ lide facilmente com strings brutas e execute eficientemente uma normaliza√ß√£o de texto, tokeniza√ß√£o, gera√ß√£o de n-gramas, e indexa√ß√£o de vocabul√°rio. Leia o lan√ßamento aqui e cheque o <a href="https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3">notebook Colab do Chollet</a> demonstrando como usar a feature para classifica√ß√£o de texto fim-a-fim.</p>

<p><strong><em>NLP e ML para Busca</em></strong>
Um dos campos que fez tremendo progresso no ano passado foi NLP  com uma gama de melhoras e dire√ß√µes novas de pesquisa. Um dos dom√≠nio que pode potencialmente se beneficiar de transfer learning em NLP √© a <em>busca</em>.</p>

<p>Apesar de a busca pertencer ao campo de recupera√ß√£o da informa√ß√£o, h√° uma oportunidade para construir engines que melhores a busca sem√¢ntica usando t√©cnicas modernas de NLP como representa√ß√µes contextualizadas de um modelo baseado em transformer como <a href="https://arxiv.org/abs/1810.04805">BERT</a>. O Google liberou um <a href="https://www.blog.google/products/search/search-language-understanding-bert/">post de blog</a> alguns meses atr√°s discutindo como eles est√£o alavancando modelos BERT para melhorar e entender buscas.</p>

<p>Se voc√™ est√° curioso sobre como representa√ß√µes contextualizadas podem ser aplicadas √† busca usando tecnologias abertas tais como Elasticsearch e Tensorflow, voc√™ pode dar uma olhada ou nesse <a href="https://towardsdatascience.com/elasticsearch-meets-bert-building-search-engine-with-elasticsearch-and-bert-9e74bf5b4cf2">post</a> ou <a href="https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a">aqui</a>.</p>

<p><strong><em>An√°lise m√©dica de imagem</em></strong></p>

<p><a href="https://github.com/fepegar/torchio">TorchIO</a> √© um pacote de Python baseado na biblioteca popular chama PyTorch. TorchIO oferece funcionalidades para f√°cil e eficientemente ler e amostrar imagems m√©dicas 3D. Caracter√≠sticas incluem transforma√ß√µes espaciais para aumento e pr√©-processamento de dados.</p>

<p><img src="https://cdn-images-1.medium.com/max/1600/0*FSPuSC8TK9X-NQ2q.gif" alt="" /></p>

<p><a href="https://github.com/fepegar/torchio">fonte</a></p>

<h1>√âtica em IA üö®</h1>

<p><strong><em>Comportamento fraudulento na comunidade de ML</em></strong>
Isso acabou de sair! O primeiro lugar de uma competi√ß√£o do Kaggle foi desqualificado por uma atividade fraudulenta. O time usou uma t√°tica esperta mas irrespons√°vel e inaceit√°vel ao vencer o primeiro lugar da competi√ß√£o. Aqui est√° a <a href="https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/125436">hist√≥ria completa</a>. Essa hist√≥ria real√ßa um dos muitos dos comportamentos inaceit√°vies que a comunidade de aprendizado de m√°quina quer mitigar. O uso apropriado e √©tico de tecnologias de ML √© o √∫nico caminho a seguir.</p>

<p><strong><em>Vi√©s de g√™nero em tradu√ß√£o de m√°quina</em></strong>
A respeito do t√≥pico se tradu√ß√£o de m√°quina reflete vi√©s de g√™nero, um grupo de pesquisadores publicou esse excelente <a href="https://arxiv.org/abs/1809.02208">artigo</a> apresentando um estudo de caso usando o tradutor do Google. Uma das descobertas dos autores afirma que o tradutor do Google ‚Äúexibe uma ted√™ncia forte a padr√µes masculinos, em particular para campos ligados a distribui√ß√µes desbalanceadas de g√™nero tal como trabalhos STEM.‚Äù</p>

<p><strong><em>Vi√©s e Justi√ßa em ML</em></strong>
Se voc√™ quer se atualizar com tudo de √©tica e justi√ßa em IA, esse √© um √≥timo <a href="https://twimlai.com/twiml-talk-336-trends-in-fairness-and-ai-ethics-with-timnit-gebru/">epis√≥dio de podcast</a> com a participa√ß√£o de Timnit Gebru e apresentado por TWIML.</p>

<p>Timit √© uma pesquisadora proeminente em justi√ßa em ML que, junto com Eun Seo Jo, publicou um <a href="https://arxiv.org/abs/1912.10389">artigo</a> onde identificam cinco abordagens chave em pr√°ticas de coleta de documentos em arquivos que podem prover m√©todos mais confi√°veis para coleta de dados em ML sociocultural. Isso pode potencialmente levar a um m√©todo de coleta de dados mais sistem√°tico, ganhado de uma pesquisa colaborativa.</p>

<p>Sina Fazelpour e Zachary Lipton recentemente publicaram um <a href="http://zacklipton.com/media/papers/fairness-non-ideal-fazelpour-lipton-2020.pdf">artigo</a> onde argumentam que, devido √† natureza de como nosso mundo n√£o ideal surgiu, √© poss√≠vel que ML justo, baseado no pensamento ideal, pode potencialmente levar a pol√≠ticas e interven√ß√µes mal guiadas. Na verdade, suas an√°lises demonstraram ‚Äúque defici√™ncias em algoritmos de ML refletem problemas maiores encarados pela abordagem ideal.‚Äù</p>

<h1>Artigos e posts de blogs ‚úçÔ∏è</h1>

<p><strong><em>D√©ficits em NLP</em></strong>
Benjamin Heinzerling publicou um artigo interessante no The Gradient onde eles discute √°reas onde NLP falha, como compreens√£o de argumentos e racioc√≠nio de senso comum. Benjammin faz refer√™ncia ao <a href="https://www.aclweb.org/anthology/P19-1459/">artigo</a> recente por Nivin &amp; Kao, que desafia e questiona as capacidades de transfer learning e modelos de lingauem para entendimento de linguagem natural (NLU) em alto n√≠vel. <a href="https://thegradient.pub/nlps-clever-hans-moment-has-arrived/">Leia</a> mais sobre esse excelente resumo da an√°lise executada na pesquisa.</p>

<p><strong><em>Destaques de NLP e ML em 2019</em></strong>
Para o ano novo, eu liberei um <a href="https://medium.com/dair-ai/nlp-year-in-review-2019-fb8d523bcb19">relat√≥rio</a> documentando alguns dos destaques mais interessantes em NLP e ML pelos quais passei em 2019.</p>

<p>Sebastian Ruder tamb√©m escreveu recentemente um <a href="https://ruder.io/research-highlights-2019/">post de blog</a> excelente e detalhado sobre as top dez dire√ß√µes de pesquisa em ML e NLP que ele achou impactante em 2019. Na lista est√£o t√≥picos como pr√© treino n√£o supervisionado e universal, ML e NLP aplicados √† ci√™ncia, aumentando modelos pr√© treinados, Transformers eficientes e de longo alcance, entre outros.</p>

<p><img src="https://cdn-images-1.medium.com/max/1600/0*8zoPc5OnYERIaaMP.png" alt="" /></p>

<p><em>‚ÄúVideoBERT (</em><a href="https://arxiv.org/abs/1904.01766"><em>Sun et al., 2019</em></a><em>), uma variante recente multimodal do BERT que gera ‚Äútokens‚Äù de v√≠deos dado uma receita (acima) e prediz os tokens futuros em escalas de tempo diferentes dado um token de v√≠deo(abaixo).‚Äù‚Ää‚Äî</em>‚Ää<a href="https://arxiv.org/pdf/1904.01766.pdf"><em>fonte</em></a></p>

<p>A Google AI de pesquisa publicou um <a href="https://ai.googleblog.com/2020/01/google-research-looking-back-at-2019.html">resumo</a> da pesquisa conduzida por eles durantes o ano, e as dire√ß√µes futuras de pesquisa que eles est√£o prestando aten√ß√£o.</p>

<h1>Educa√ß√£o em ML/NLP  üéì</h1>

<p><strong><em>Democratizando educa√ß√£o em IA</em></strong>
Num esfor√ßo de democratizar educa√ß√£o em IA e para educar as massas sobre as implica√ß√µes de tecnologia de IA, a Universidade de Helsinki fez uma parceria com a Reaktor para lan√ßar um curso brilhante (de gra√ßa) que cobre os fundamentos de IA. O <a href="https://www.elementsofai.com/">curso popular</a> se chama ‚ÄúElementos de AI‚Äù e inclui t√≥picos tal como √©tica em IA, filosofia em IA, redes neurais, regra de Naive Bayes, dentre outros t√≥picos.</p>

<p>Stanford CS224N est√° de volta com outra <a href="http://web.stanford.edu/class/cs224n/">itera√ß√£o</a> do popular curso de ‚ÄúNatural Language Processing with Deep Learning‚Äù. O curso oficialmente come√ßou em 7 de Janeiro dessa ano, ent√£o se voc√™ quiser acompanhar, vai para o site para o programa completo, slides, v√≠deos, sugest√µes de leitura de paper, etc.</p>

<p><strong><em>Top livros de NLP e ML</em></strong>
Eu tweetei meus top livros de ML e NLP pr√°ticos e te√≥ricos, e foi bem recebido. Eu gostaria de compartilhar aquela lista aqui via tweet: https://twitter.com/omarsar0/status/1214547402838986754?s=20</p>

<p><strong><em>Machine Learning com M√©todos de Kernel</em></strong>
M√©todos de Kernel tais como PCA e k-means est√£o por aqui h√° muito tempo, e isso √© porque eles t√™m sido aplicados com sucesso a uma variedade grande de aplica√ß√µes tal quais grafos e sequ√™ncias biol√≥gicas. Cheque esse conjunto compreens√≠vel de <a href="http://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/slides/master2017/master2017.pdf">slides</a> cobrindo um alcance grade de m√©todos de kernel e suas aplica√ß√µes internas. Aqui est√° tamb√©m um √≥timo <a href="https://francisbach.com/cursed-kernels/">blog</a> (mantido por Francis Bach) discutindo aspectos de m√©todos de kernel e outros t√≥picos de aprendizado de m√°quina.</p>

<h1>Men√ß√µes Honrosas ‚≠êÔ∏è</h1>

<p>Aqui est√° uma lista de hist√≥rias dignas de nota e da sua aten√ß√£o:</p>

<ul>
<li>John Langford cuida desse incr√≠vel <a href="https://hunch.net/">blog</a> que discute teoria de aprendizado de m√°quina</li>
<li>Muitas das tecnologias da ind√∫stria orientadas a ML t√™m usado m√°quinas de Gradient Boosting por anos. Cheque esse <a href="https://opendatascience.com/xgboost-enhancement-over-gradient-boosting-machines/?utm_campaign=Learning%20Posts&amp;utm_content=111061559&amp;utm_medium=social&amp;utm_source=twitter&amp;hss_channel=tw-3018841323">post</a> introduzindo uma das bibliotecas usadas para aplicar o gradient boosting chamado XGBoost.

<ul>
<li>Se voc√™ est√° interessado em aprender como fazer o design e construir aplica√ß√µes de aprendizado de m√°quina e lev√°-las √† produ√ß√£o, Emmanual Ameisen cobre isso com esse <a href="https://www.amazon.com/Building-Machine-Learning-Powered-Applications/dp/149204511X/">livro</a>.</li>
</ul>
</li>
</ul>


<hr />

<p><em>Se voc√™ tem uma hist√≥ria que gostaria de ser publicada na pr√≥xima edi√ß√£o da Newsletter de NLP, por favor mande um email para ellfae@gmail ou me mande uma mensagem via</em> <a href="https://twitter.com/omarsar0"><em>Twitter</em></a><em>.</em></p>

</body>
</html>
