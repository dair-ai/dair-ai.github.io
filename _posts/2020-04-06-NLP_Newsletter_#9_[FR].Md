---
layout: post
title: "NLP Newsletter #9 [FR]: Illustrated GNN Guide, TextVQA and TextCaps, KeraStroke, SyferText, torchlayers,‚Ä¶"
author: lbourdois
excerpt: "This issue includes topics that range from a privacy-preserving NLP tool to interactive tools for searching COVID-19 related papers to an illustrated guide to graph neural networks."
modified:
comments: true
tags: [nlp_newsletter]
image:
  thumb: nlp_newsletter_9.png
---


![](https://cdn-images-1.medium.com/max/1200/1*Vq-bFSTjqYjDGAR4Ja1abw.png)


# Avant-propos d‚ÄôElvis

Bienvenue au 9√®me num√©ro de la lettre d'information consacr√©e au NLP. Nous esp√©rons que vous et vos proches allez bien et que vous √™tes en s√©curit√©.

\\
Avant d‚Äôentrer dans le vis du sujets, voici quelques mises √† jour de dair.ai qui ont √©t√© effectu√©es :

- ***Appel √† contributions pour l‚ÄôOpen science***
De nombreuses collaborations int√©ressantes sont en cours et nous souhaitons inviter tous ceux qui sont int√©ress√©s √† contribuer √† l‚ÄôOpen science. Nous recherchons des b√©n√©voles, des √©crivains, des r√©viseurs, des √©diteurs, des d√©veloppeurs, des conf√©renciers, des chercheurs, des responsables de projets ,... [Rejoignez-nous](https://github.com/dair-ai/dair-ai.github.io/issues/54) !

- ***NLP Research Highlights Issue #1*** au cas o√π vous l'auriez manqu√©, nous exposons dans cet [article](https://dair.ai/NLP_Research_Highlights_-_Issue_-1/), une s√©lection d'articles int√©ressants et importants de NLP publi√©s au cours des derniers mois que nous r√©sumons.


# Publications üìô

***Neuro√©volution des agents auto-interpr√©tables***

\\
Tang et al. (2020) pr√©sentent un [travail](https://attentionagent.github.io/) qui vise √† faire √©voluer un agent pour qu'il utilise une fraction de son apport visuel dans le but de survivre √† une t√¢che. Par exemple, √©viter de s'√©craser dans un virage ou d'esquiver les boules de feu, comme le montre la figure ci-dessous. En utilisant la [neuroevolution](https://en.wikipedia.org/wiki/Neuroevolution) pour entra√Æner des architectures d'auto-attention*, les auteurs ont pu entra√Æner des agents √† effectuer diff√©rentes t√¢ches tout en ne permettant qu'une fraction de l'entr√©e. Les avantages du mod√®le sont une r√©duction substantielle de la taille des param√®tres, la possibilit√© d'interpr√©ter les politiques et le fait de permettre au mod√®le de ne tenir compte que des indices visuels essentiels √† la t√¢che*.

\\
![](https://cdn-images-1.medium.com/max/800/1*Gm8jlCUvP_JECEPspDypWg.png)

\\
***RONEC :  Romanian Named Entity Corpus***

\\
[RONEC](https://arxiv.org/abs/1909.01247) est un corpus d'entit√©s nomm√©es pour la langue roumaine qui contient plus de 26 000 entit√©s dans environ 5 000 phrases annot√©es et appartenant √† 16 classes distinctes. Les phrases ont √©t√© extraites d'un journal libre de droits d'auteur, couvrant plusieurs styles. Ce corpus repr√©sente la premi√®re initiative dans l'espace linguistique roumain sp√©cifiquement cibl√©e pour la reconnaissance des entit√©s nomm√©es. Il est disponible aux formats BIO et CoNLL-U Plus, et il est libre d'utilisation et d'extension [ici](https://github.com/dumitrescustefan/ronec).


\\
***Lois de mise √† l'√©chelle des performances des mod√®les linguistiques***

\\
Des chercheurs de John Hopkins et d'OpenAI ont men√© une [√©tude empirique](https://arxiv.org/abs/2001.08361) pour comprendre les lois de mise √† l'√©chelle des performances des mod√®les linguistiques. Ce type d'√©tude peut √™tre utilis√© comme un guide pour prendre de meilleures d√©cisions sur la fa√ßon d'utiliser plus efficacement les ressources. Dans l'ensemble, il a √©t√© constat√© que les grands mod√®les sont nettement plus efficaces en termes d'√©chantillonnage. Si les calculs et les donn√©es sont limit√©s, il est pr√©f√©rable d‚Äôentra√Æner un grand mod√®le en quelques √©tapes plut√¥t que d‚Äôentra√Æner un mod√®le plus petit jusqu'√† ce qu'il converge (voir les r√©sultats r√©sum√©s dans la figure ci-dessous). Les auteurs fournissent davantage de conclusions et de recommandations lorsqu'il s'agit d‚Äôentra√Æner de grands mod√®les linguistiques (par exemple, les Transformers) sur les aspects du sur-apprentissage, du choix de la taille optimale des batch, du fine-tunning, etc‚Ä¶

\\
![](https://cdn-images-1.medium.com/max/800/1*plbjqswVe4Cq8UVggqPH1w.png)

[*Kaplan et al. (2020)*](https://arxiv.org/abs/2001.08361)

\\
***Calibration des transformers pr√©-entra√Æn√©s***

\\
Les transformers pr√©-entra√Æn√©s √©tant de plus en plus utilis√©s dans des applications du monde r√©el, il est important de comprendre √† quel point leurs r√©sultats sont "fiables". Des travaux [r√©cents](https://arxiv.org/abs/2003.07892) de l'UT Austin montrent que les probabilit√©s post√©rieures de BERT et de RoBERTa sont relativement calibr√©es (c'est-√†-dire conformes aux r√©sultats empiriques) pour trois t√¢ches : inf√©rence en langage naturel, d√©tection de paraphrase, raisonnement de bon sens. Ces r√©sultats sont obtenus avec des ensembles de donn√©es √† la fois dans le domaine et hors domaine. Les r√©sultats montrent : (1) lorsqu'ils sont utilis√©s, les mod√®les pr√©- entra√Æn√©s sont calibr√©s dans le domaine ; et (2) la mise √† l'√©chelle de la temp√©rature est efficace pour r√©duire davantage l'erreur de calibrage dans le domaine, tandis que le lissage des labels pour augmenter l'incertitude empirique aide √† calibrer les donn√©es post√©rieures hors du domaine.
\\
![](https://cdn-images-1.medium.com/max/800/1*ose0s-G0WfPFoleZJ1htrQ.png)
[*Desai and Durrett (2020)*](https://arxiv.org/pdf/2003.07892.pdf)

\\
***Les m√©caniques statistiques de l‚Äôapprentissage profond***

\\
Un r√©cent [article](https://www.annualreviews.org/doi/abs/10.1146/annurev-conmatphys-031119-050745) examine de plus pr√®s le lien entre les sujets physiques/math√©matiques et l'apprentissage approfondi. Les auteurs ont pour objectif de discuter de sujets qui recoupent la m√©canique statistique et l'apprentissage machine, afin de r√©pondre aux questions qui aident √† comprendre le c√¥t√© th√©orique des r√©seaux neuronaux profonds et les raisons de leur succ√®s.

\\
***Vers un ImageNet pour le Speech-to-Text***

\\
Dans un nouvel [article](https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/) publi√© dans The Gradient, Alexander Veysov explique pourquoi ils pensent que la bascule semblable √† ce que ImageNet a √©t√© pour la vision par ordinateur est arriv√©e pour le Speech-to-Text (STT) en langue russe. Ces derni√®res ann√©es, les chercheurs ont √©galement fait cette affirmation √† propos du NLP. Cependant, pour parvenir √† un tel moment de STT, Alexander affirme que de nombreuses pi√®ces doivent √™tre r√©unies, comme la mise √† disposition de mod√®les √† grande √©chelle, la minimisation des besoins de calcul et l'am√©lioration de l'accessibilit√© des grands mod√®les pr√©-entra√Æn√©s.


# Cr√©ativit√©, √©thique et soci√©t√© üåé

***Recherche d'articles li√©s au COVID-19***

\\
La semaine derni√®re, nous avons pr√©sent√© un jeu de donn√©es public appel√© [CORD-19](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) qui contient des documents relatifs au COVID-19.\\
Gabriele Sarti a cod√© un [outil interactif](https://github.com/gsarti/covid-papers-browser) qui vous permet de rechercher et de parcourir plus efficacement les articles li√©s au COVID-19 en utilisant un [mod√®le fine-tun√© de SciBERT](https://huggingface.co/gsarti/scibert-nli).

\\
![](https://cdn-images-1.medium.com/max/800/0*tu8nzUIEuSizuW5-.gif)

\\
reciTAL a √©galement lanc√© un projet appel√© [COVID-19 Smart Search Engine](https://covidsmartsearch.recital.ai/) afin d'am√©liorer la recherche et la navigation sur les articles li√©s au COVID-19. Le but √©tant d'aider les chercheurs et les professionnels de la sant√© √† trouver et √† d√©couvrir rapidement et efficacement les informations relatives au COVID-19.

\\
![](https://cdn-images-1.medium.com/max/800/1*Y5W3sib4y6UxSr3YkQSu5Q.png)

\\
***SyferText***

\\
OpenMined a d√©voil√© [SyferText](https://github.com/OpenMined/SyferText), une nouvelle librairie qui vise √† permettre une utilisation s√©curis√©e et priv√©e du NLP. Elle en est √† ses d√©buts, mais nous pensons qu'il s'agit d'un effort tr√®s important vers des syst√®mes d'IA plus s√ªrs et √©thiques. Voici quelques [tutoriels](https://github.com/OpenMined/SyferText/tree/master/tutorials) pour commencer.

\\
***David contre Goliath : vers des mod√®les plus petits pour un NLP moins gourmand, plus rapide et plus √©cologique***

\\
Plus c'est gros, mieux c'est ? Lorsque l'on examine l'√©volution de la taille des mod√®les linguistiques au cours des derni√®res ann√©es, on peut penser que la r√©ponse est oui. Pourtant, le co√ªt financier et environnemental de l‚Äôentra√Ænement de tels mod√®les est tr√®s √©lev√©. En outre, plus grand dans ce cas signifie g√©n√©ralement plus lent, mais la vitesse est essentielle dans la plupart des applications. C'est ce qui motive la tendance actuelle en NLP √† promouvoir des mod√®les plus petits, plus rapides et plus √©cologiques tout en pr√©servant les performances. Dans ce billet, [Manuel Tonneau](https://www.linkedin.com/in/ACoAABs605YB_jMmGA0iLFongpVm1iKjFmKLSts/) pr√©sente cette nouvelle tendance en faveur de mod√®les plus petits en se concentrant sur trois mod√®les r√©cents et populaires, DistilBERT de [Hugging Face](https://www.linkedin.com/company/huggingface/), PD-BERT de [Google](https://www.linkedin.com/company/google/) et BERT-of-Theseus de [Microsoft](https://www.linkedin.com/company/microsoft/).

\\
***Une enqu√™te sur l'apprentissage approfondi pour la d√©couverte scientifique***

\\
De nombreuses grandes entreprises qui ont aujourd'hui concentr√© leurs efforts sur la recherche en IA estiment que l'apprentissage approfondi peut √™tre utilis√© comme un outil de d√©couverte scientifique. Ce [document](https://arxiv.org/abs/2003.11755) donne un aper√ßu complet des mod√®les d'apprentissage approfondi couramment utilis√©s pour diff√©rents cas d'utilisation scientifique. Le document pr√©sente √©galement des conseils de mise en ≈ìuvre, des tutoriels, d'autres r√©sum√©s de recherche et des outils.


# Outils et jeux de donn√©es ‚öôÔ∏è

***TextVQA et TextCaps***

\\
Afin d'encourager la construction de mod√®les capables de mieux d√©tecter et lire le texte compris dans des images, ainsi que d‚Äôam√©liorer le raisonnement pour r√©pondre √† des questions et g√©n√©rer des l√©gendes, Facebook AI organise deux concours distincts. Les concours s'appellent [TextVQA](https://textvqa.org/challenge) Challenge et [TextCaps](https://textvqa.org/textcaps/challenge) Challenge. Ils portent respectivement sur les t√¢ches de r√©ponse visuelle aux questions et sur la g√©n√©ration de l√©gendes.

\\
***KeraStroke***

\\
L'un des plus grands obstacles √† surmonter lors de la conception de r√©seaux neuronaux est le sur-apprentissage. Les techniques actuelles telles que le dropout, la r√©gularisation et l'arr√™t anticip√© sont tr√®s efficaces dans la plupart des cas d'utilisation, mais elles peuvent avoir tendance √† ne pas √™tre √† la hauteur lorsqu'on utilise de grands mod√®les ou de petits ensembles de donn√©es. En r√©ponse √† cela, Charles Averill a d√©velopp√© [KeraStroke](https://pypi.org/project/kerastroke/#description), une nouvelle suite de techniques d'am√©lioration de la g√©n√©ralisation utile pour les grands mod√®les ou les petits ensembles de donn√©es. En modifiant les valeurs de poids dans certains cas pendant l'entra√Ænement, les mod√®les s'adaptent dynamiquement aux donn√©es d'entra√Ænement qui leur sont fournies.


\\
***torchlayers***

\\
[torchlayers](https://github.com/szymonmaszke/torchlayers) est un nouvel outil construit sur PyTorch qui permet l'inf√©rence automatique de la forme et de la dimension des couches disponibles dans le module torch.nn comme les couches convolutionnelles, r√©currentes, de transformateur, etc. Cela signifie que vous n'avez pas besoin de d√©finir explicitement la forme des caract√©ristiques d'entr√©e qui doivent √™tre sp√©cifi√©es manuellement dans les couches. Cela simplifie la d√©finition d'un mod√®le dans PyTorch. Voir ci-dessous un exemple de classificateur de base impl√©ment√© avec les couches torche :

\\
![](https://cdn-images-1.medium.com/max/800/1*tHOme2pZ39sNziqdUCsn4g.png)

*Nous pouvons voir, √† partir de l'extrait de code, que la couche lin√©aire ne requiert que la taille des caract√©ristiques de sortie, par opposition √† la taille de sortie et d'entr√©e. Ceci est d√©duit par les couches de torche en fonction de la taille d'entr√©e.*


\\
***Haystack***

\\
[Haystack](https://github.com/deepset-ai/haystack/) vous permet d'utiliser des mod√®les de transformer pour r√©pondre aux questions. Il utilise un Retriever-Reader-Pipeline, o√π le Retriever est un algorithme rapide pour trouver les documents candidats et le Reader est un transformer qui extrait la r√©ponse granulaire. Il s'appuie sur Transformers d‚ÄôHugging Face et sur Elasticsearch. C'est un logiciel libre, tr√®s modulaire et facile √† √©tendre.

\\
***Enseigner √† une IA √† r√©sumer les articles de presse : un nouveau jeu de donn√©es pour le r√©sum√© abstrait***

\\
Curation Corp est une entreprise de libre acc√®s qui fournit 40 000 r√©sum√©s d'articles r√©dig√©s par des professionnels. Cet [article](https://medium.com/curation-corporation/teaching-an-ai-to-abstract-a-new-dataset-for-abstractive-auto-summarisation-5227f546caa8) constitue une belle introduction √† la synth√®se de textes et aux d√©fis que pose cette t√¢che particuli√®re. En outre, il pr√©sente l'ensemble de donn√©es, les probl√®mes qui peuvent √™tre r√©solus avec celui-ci, des mesures d'√©valuation pour la synth√®se de texte, et se termine par une discussion pour le travail futur. Les instructions pour acc√©der √† l'ensemble de donn√©es se trouvent dans ce [d√©p√¥t Github](https://github.com/CurationCorp/curation-corpus), ainsi que des [exemples](https://github.com/CurationCorp/curation-corpus/tree/master/examples) d'utilisation de l'ensemble de donn√©es pour le r√©glage fin.

\\
En ce qui concerne la synth√®se des textes, l'√©quipe de HuggingFace a ajout√© [BART](https://github.com/pytorch/fairseq/blob/master/examples/bart/README.md) et [T5](https://github.com/dair-ai/nlp_paper_summaries/blob/master/Language%20Modeling/t5-text-to-text-transformer.md) √† sa librairie [Transformers](https://github.com/huggingface/transformers/releases). Ces ajouts permettent d'effectuer toutes sortes de t√¢ches, telles que le r√©sum√© abstrait, la traduction et les questions-r√©ponses‚Ä¶

\\
J‚Äôai voulu utiliser la pipeline de traduction du T5 pour effectuer la traduction de cette newsletter. L‚Äô√©x√©cution est plut√¥t lente et la qualit√© pas forc√©ment au rendez-vous. Je suppose que cela viens du fait que la newsletter utilise un vocabulaire bien sp√©cifique (math√©matiques, NLP, etc‚Ä¶). Il faudrait effectuer des tests sur des corpus moins sp√©cialis√©s. La traduction manuelle a encore quelques beaux jours üòâ


# Articles et Blog ‚úçÔ∏è

***Un guide illustr√© des r√©seaux neuronaux graphiques (GNN)***

\\
Les r√©seaux neuronaux graphiques ont r√©cemment √©t√© adopt√©s pour des t√¢ches telles que l'am√©lioration des mod√®les de vision par ordinateur et la pr√©vision des effets secondaires dus aux interactions m√©dicamenteuses. Dans cet [aper√ßu](https://dair.ai/An_Illustrated_Guide_to_Graph_Neural_Networks/), Rish pr√©sente un guide intuitif et illustr√© sur les r√©seaux de neurones graphiques. (*Disponible sur* [*dair.ai*](https://dair.ai/))

\\
![](https://cdn-images-1.medium.com/max/800/1*Ru3CizrB14hvpZQ7ZtgIag.png)

\\
***Finetuning avec JAX + Haiku***

\\
Le mois dernier encore, DeepMind a d√©voil√© Haiku, la version JAX de sa librairie Sonnet. Ce [post](https://www.pragmatic.ml/finetuning-transformers-with-jax-and-haiku/) pr√©sente un mod√®le RoBERTa pr√©-entra√Æn√© √† JAX + Haiku, puis montre le finetuning du mod√®le pour r√©soudre une t√¢che en aval. Il se veut un guide pratique pour l'utilisation des utilitaires expos√©s par Haiku pour permettre l'utilisation de "modules" l√©gers orient√©s objet dans le contexte des contraintes de programmation fonctionnelle de JAX.

\\
***Un petit voyage dans la vall√©e du traitement du langage naturel et du pr√©traitement de texte pour la langue allemande***

\\
Fl√°vio Cl√©sio a √©crit un [article](https://flavioclesio.com/2020/02/17/a-small-journey-in-the-valley-of-natural-language-processing-and-text-pre-processing-for-german-language/) tr√®s d√©taill√© sur les d√©fis que repr√©sente le traitement des probl√®mes de NLP en langue allemande. Il partage de nombreuses le√ßons apprises, ce qui a fonctionn√© et ce qui n'a pas fonctionn√©, discute de plusieurs m√©thodes de pointe, des probl√®mes communs √† √©viter, et d'une tonne de ressources d'apprentissage, de documents et de billets de blog.

\\
***PIAF***

\\
L‚Äô√©quipe de PIAF (Pour une IA Francophone, For a French Speaking AI) a r√©cemment publi√© un article en anglais abordant leur projet d‚Äôune base de questions-r√©ponses en fran√ßais ainsi que la place de la francophonie dans le NLP. Il s‚Äôagit en r√©alit√© de la traduction d‚Äôun de leurs post datant de janvier que vous pouvez consulter en fran√ßais [ici]( https://piaf.etalab.studio/francophonie-ia/).

\\
***Un classificateur personnalis√© en plus du mod√®le de langage de type BERT***

\\
Marcin a √©crit un nouveau [guide](https://zablo.net/blog/post/custom-classifier-on-bert-model-guide-polemo2-sentiment-analysis/) montrant comment construire son propre classificateur (par exemple, le classificateur de sentiments) sur des mod√®les de langage de type BERT. Il montre √©galement comment utiliser d'autres librairies modernes pour les diff√©rentes parties du mod√®le, telles que HuggingFace Tokenizer et PyTorchLightning. Vous trouverez le notebook Colab [ici](https://colab.research.google.com/drive/1sajgpLTrTJDzRSlxycy8aE6ysxGqaT18).

\\
![](https://cdn-images-1.medium.com/max/800/0*66IKvwYU2Lq1kjyn.png)

[*Source*](https://zablo.net/blog/post/custom-classifier-on-bert-model-guide-polemo2-sentiment-analysis/)


# Education üéì

***Analyse exploratoire des donn√©es textuelles***

\\
Dans ce [code](https://dair.ai/Exploratory_Data_Analysis_for_Text_Data/), Yonathan Hadar passe en revue plusieurs m√©thodes d'analyse exploratoire de donn√©es textuelles avec divers exemples de codes. Nous avons pr√©sent√© ce tutoriel sur le site dair.ai car il s'agit d'un tutoriel tr√®s complet qui utilise des m√©thodes standard d'analyse de donn√©es que tout sp√©cialiste des donn√©es trouvera utiles. C'est un bon point de d√©part pour quiconque commence √† jouer avec des donn√©es textuelles.


\\
***Embeddings in Natural Language Processing***
\\
Mohammad Taher Pilehvar et Jose Camacho-Collados ont publi√© la premi√®re √©bauche d'un prochain [livre](https://medium.com/r/?url=http%3A%2F%2Fjosecamachocollados.com%2Fbook_embNLP_draft.pdf) intitul√© "Embeddings in Natural Language Processing". L'id√©e de ce livre est de discuter du concept d‚Äôembedding qui repr√©sente certaines des techniques les plus utilis√©es en NLP. Comme [indiqu√©](https://medium.com/r/?url=https%3A%2F%2Ftwitter.com%2FCamachoCollados%2Fstatus%2F1246013768074747906%3Fs%3D20) par les auteurs, le livre comprend "les bases des mod√®les d'espace vectoriel et du word embedding dans des phrases plus r√©centes et des techniques d‚Äôembedding contextualis√©e bas√©es sur des mod√®les de langage pr√©-entra√Æn√©s".


\\
***A Brief Guide to Artificial Intelligence***

\\
Le Dr James V Stone a r√©cemment publi√© son nouveau [livre](https://jim-stone.staff.shef.ac.uk/AIGuide/) intitul√© "A Brief Guide to Artificial Intelligence", dont l'objectif est de fournir une vue d'ensemble des syst√®mes d'intelligence artificielle actuels et de leur capacit√© √† accomplir une s√©rie de t√¢ches. Comme indiqu√© dans le r√©sum√©, le livre est "√©crit dans un style informel, avec un glossaire complet et une liste de lectures compl√©mentaires, ce qui en fait une introduction id√©ale au domaine en rapide √©volution de l'IA".

\\
![](https://cdn-images-1.medium.com/max/800/0*I2YYXTCQTBmk_YiF.jpg)

\\
***Cours de ML et de DL***

\\
Sebastian Raschka a publi√© deux [√©pisodes](https://www.youtube.com/watch?time_continue=1&v=QQD9Y2FiotQ&feature=emb_logo) de son cours sur "Introduction to Deep Learning and Generative Models Vous pouvez trouver des notes de cours et d'autres documents dans ce [repertoire](https://github.com/rasbt/stat453-deep-learning-ss20).

\\
Voici un autre ensemble de [conf√©rences](https://www.youtube.com/watch?v=e-erMrqBd1w&feature=youtu.be) sur le th√®me de la "g√©om√©trie diff√©rentielle discr√®te".

\\
Peter Bloem a publi√© les [ressources](https://mlvu.github.io/) (comprenant des vid√©os et des diapositives) pour son cours d'introduction √† l'apprentissage automatique dispens√© √† l'universit√© d'Amsterdam. Les sujets abord√©s vont des mod√®les lin√©aires et de la recherche aux mod√®les probabilistes en passant par les mod√®les pour les donn√©es s√©quentielles.

\\
***CNN Architecture‚Ää‚Äî‚ÄäImplementations***

\\
Dimitris Katsios fournit un ensemble de [tutoriels](https://github.com/Machine-Learning-Tokyo/CNN-Architectures/tree/master/Implementations) qui donnent des conseils sur la fa√ßon de mettre en ≈ìuvre des architectures de r√©seaux neuronaux convolutifs (CNN) √† partir d'articles originaux. Il propose une recette sur la fa√ßon de les mettre en ≈ìuvre tout en partageant les √©tapes qui comprennent des diagrammes et du code avec la possibilit√© de d√©duire la structure du mod√®le.


# Mentions sp√©ciales ‚≠êÔ∏è

Il y a quelques mois, nous avons pr√©sent√© le livre de Luis Serrano sur le ML de Grokking. [Ici](https://content.alegion.com/podcast/grokking-machine-learning-with-luis-serrano) vous pouvez entendre Luis parler un peu plus de son livre et de son parcours.

\\
Voici plusieurs newsletters qui pourraient m√©riter votre attention : [Sebastian Ruder's NLP News](http://newsletter.ruder.io/), [Made With ML](https://madewithml.com/blog/newsletter/2020-03-25/), [SIGTYP's newsletter](https://sigtyp.github.io/sigtyp-newsletter-Mar-2020.html), [MLT Newsletter](https://mailchi.mp/c70ebcf424b2/mlt-newsletter-6), [Nathan's AI newsletter](http://newsletter.airstreet.com/issues/your-guide-to-ai-in-q1-2020-part-1-2-212335), etc...

\\
Jupyter est d√©sormais dot√© d'un [d√©bogueur visuel](https://blog.jupyter.org/a-visual-debugger-for-jupyter-914e61716559) üòä

\\
![](https://cdn-images-1.medium.com/max/800/0*FE5Fj5DFb0U9565a.gif)

\\
Abhishek Thakur poss√®de une cha√Æne [YouTube](https://www.youtube.com/channel/UCBPRJjIWfyNG4X-CRbnv78A) o√π il se prom√®ne √† travers le code pour montrer comment utiliser les m√©thodes modernes d'apprentissage automatique et de NLP.

\\
David Silver, professeur et chercheur renomm√© en mati√®re d'apprentissage par renfocement, a [re√ßu](https://awards.acm.org/about/2019-acm-prize) le prix ACM de l'informatique pour ses avanc√©es dans le domaine des jeux vid√©o. Silver a dirig√© l'√©quipe Alpha Go qui a battu Lee Sedol au Go.

\\
Pour ceux qui souhaitent conna√Ætre les diff√©rences et le fonctionnement interne des m√©thodes populaires de NLP telles que BERT et word2vec, Mohd [fournit](https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/) un excellent aper√ßu, accessible et d√©taill√© de ces approches.

\\
TensorFlow 2.2.0-rc-1 a √©t√© [publi√©](https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0-rc1?linkId=85073218). Il comprend des fonctionnalit√©s telles qu'un profileur qui aide √† rep√©rer les goulots d'√©tranglement dans vos mod√®les de ML et √† guider l'optimisation de ces mod√®les. De plus, Colab utilise d√©sormais TensorFlow 2 par [d√©faut](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb?linkId=85251566).

\\
Enfin, Gabriel Peyr√© fournit un ensemble de [notes](https://mathematical-tours.github.io/book-sources/optim-ml/OptimML.pdf) pour son cours sur l'optimisation du ML. Les notes incluent l'analyse convexe, le SGD, les MLP, etc‚Ä¶

\\
Vous pouvez retrouver la pr√©c√©dente newsletter [ici](https://dair.ai/NLP_Newsletter_-8_-FR/)

\\
Si vous avez des jeux de donn√©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine √©dition de la newletter, n'h√©sitez pas √† contacter Elvis √† ellfae@gmail.com ou par message sur [Twitter](https://twitter.com/omarsar0).

\\
[Abonnez-vous](https://dair.ai/newsletter/) pour recevoir les prochains num√©ros dans votre bo√Æte mail.
