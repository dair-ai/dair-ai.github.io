---
layout: post
title: "NLP Newsletter #12 : Hateful Memes, TextAttack, DETR, BLEURT, GameGAN, Survey Papers,‚Ä¶"
author: lbourdois
excerpt: "In this issue, we cover topics that range from progress in language modeling to Transformer-based object detection to how to stay informed with ML."
modified:
comments: true
tags: [nlp_newsletter]
image:
  thumb: nlp_newsletter_12.png
---

![](https://cdn-images-1.medium.com/max/1200/1*g36Zf0zqinVfWEfocBa0gA.png)


# Avant-propos d‚ÄôElvis
Bienvenue au douzi√®me num√©ro de la lettre d‚Äôinformation consacr√©e au NLP.  

\\
Cela fait environ un mois que nous n'avons pas publi√© de nouveau num√©ro de la newsletter. L'interruption est termin√©e et nous sommes heureux de vous pr√©senter d'autres travaux issus des communaut√©s de l'apprentissage automatique et du traitement du langage naturel qui ont eu lieu au cours des derni√®res semaines.

\\
Nous avons pris le temps de r√©fl√©chir √† la mani√®re d'am√©liorer la newsletter et avons re√ßu d'excellents commentaires. Nous vous remercions pour votre soutien.


***Quelques mises √† jour sur la lettre d‚Äôinformation sur le NLP et sur dair.ai :***

La communaut√© dair.ai a produit un travail incroyable et a contribu√© √† am√©liorer la d√©mocratisation de l'√©ducation, de la recherche et des technologies. Voici ce que nous avons fait au cours des derni√®res semaines :

- [ML Visuals] (https://github.com/dair-ai/ml-visuals) est un nouvel effort de collaboration pour aider la communaut√© d'apprentissage machine √† am√©liorer la communication scientifique en fournissant gratuitement des visuels et des figures professionnelles en rapport avec ML. Vous √™tes libre d'utiliser les visuels dans vos pr√©sentations ou vos articles de blog.
- Notre [discussion hebdomadaire] (https://github.com/dair-ai/ml-nlp-paper-discussions) tente de r√©unir des experts et des d√©butants pour s'informer mutuellement sur les publications parues r√©cemment en NLP et ML. Il n'y a pas d'exigences pour participer, il suffit d'apporter votre volont√© d'apprendre et nous serons heureux de vous aider en r√©pondant √† vos questions et en nous engageant dans des discussions plus approfondies sur les articles de ML.
- D√©but ao√ªt, nous lancerons notre premier groupe d'√©tude. Nous couvrirons l'excellent livre intitul√© ["Dive into Deep Learning"] (https://d2l.ai/index.html) d'Aston Zhang, Zack C. Lipton, Mu Li et Alex J. Smola. Pour en savoir plus sur ce programme, consultez notre [page Meetup] (https://www.meetup.com/dair-ai/events/271394829/). Il n'y a pas de conditions pr√©alables mais nous vous fournirons de nombreux documents √† lire pour √™tre mieux pr√©par√© aux le√ßons.
- Vous pouvez consulter nos r√©centes conf√©rences sur cette [cha√Æne YouTube] (https://www.youtube.com/channel/UCyna_OxOWL7IEuOwb7WhmxQ?view_as=subscriber). Cet effort vise √† mieux faire conna√Ætre le travail des ing√©nieurs et chercheurs en NLP. Si vous souhaitez donner une conf√©rence, veuillez consulter cet [appel √† conf√©rence] (https://github.com/dair-ai/dair-ai.github.io/wiki/Call-for-Talks).
\\

# Publications üìô

***Language Models are Few-Shot Learners***

\\
Jusqu'√† pr√©sent, nous avons constat√© le succ√®s des mod√®les Transformers pour toute une s√©rie de t√¢ches de NLP. R√©cemment, [Brown et al. (2020)](https://arxiv.org/abs/2005.14165) ont propos√© le GPT-3, un mod√®le de langage autor√©gressif qui s'appuie sur le GPT-2, avec une taille de 175 milliards de param√®tres. C'est le plus grand mod√®le de LM jamais form√© et il vise √† r√©pondre √† la question de savoir si l'augmentation de la taille du LM (en termes de taille) am√©liore les performances de nombreuses t√¢ches de NLP. En outre, la question plus importante est de savoir si le mod√®le de LM √©tendu peut effectuer du *few-shot learning* pour ces t√¢ches et comment cela se compare √† d'autres paradigmes d'apprentissage comme le fine-tuning, le one-shot learning, et le zero-shot learning. Il est int√©ressant de noter que le mod√®le fonctionne tr√®s bien pour une vari√©t√© de t√¢ches, mais qu'il est moins performant pour les t√¢ches qui exigent un certain niveau de raisonnement de bon sens. L'avantage du grand mod√®le de LM semble √™tre qu'il ne n√©cessite pas de fine-tuning (dans une vari√©t√© de cas), ce qui signifie qu'√† un certain point, il pourrait √™tre possible d'√©tendre facilement l'apprentissage √† des t√¢ches en aval encore plus complexes et nouvelles sans avoir besoin de collecter des ensembles de donn√©es supervis√©es.

\\
![](https://cdn-images-1.medium.com/max/800/1*QuDYXo8McJoYwCdlosVhWw.png)

*Source:* [*Brown et al. (2020)*](https://arxiv.org/abs/2005.14165)

\\
***G√©n√©rer des notes SOAP √† partir de conversations m√©decin-patient***

\\
La documentation √©lectronique des dossiers de sant√© implique un processus rigoureux et long, g√©n√©ralement pr√©par√© manuellement par les m√©decins qui consacrent de longues heures √† cette t√¢che. C'est pourquoi [Khrisna et al. (2020)](https://arxiv.org/abs/2005.01795) proposent une approche pour aider √† automatiser la g√©n√©ration de la documentation sous forme de notes SOAP. Les auteurs exp√©rimentent diff√©rentes approches de ML en tirant parti des conversations qui ont lieu entre les m√©decins et les patients lors d'une visite. Leur approche combine des modules d'extraction et d'abstraction entra√Æn√©s aux conversations cliniques et obtient des scores ROUGES √©lev√©s pour la t√¢che de r√©daction des notes SOAP.


\\
***BLEURT : Apprendre des mesures robustes pour la g√©n√©ration de textes***

\\
Il est bien connu dans le domaine du NLP que certaines mesures d'√©valuation (par exemple, BLEU et ROUGE) ne sont pas les plus fiables. Sellam et al. (2020)](https://arxiv.org/abs/2004.04696) proposent une mesure d'√©valuation appel√©e BLEURT qui peut mieux mod√©liser les jugements humains. La m√©trique de g√©n√©ration de texte est bas√©e sur BERT et vise √† satisfaire l'expressivit√© et la robustesse par un pr√© entra√Ænement sur de grandes quantit√©s de donn√©es synth√©tiques. Par rapport √† d'autres mesures (par exemple, Meteor et BLEU) utilisant les mod√®les BERT vanilla, BLEURT tend √† mieux mod√©liser l'√©valuation humaine et donc √† √™tre plus performant en termes de pr√©cision.

\\
Si vous souhaitez une mise √† jour des diff√©rents param√®tres d'√©valuation utilis√©s en NLP, cette [r√©cente enqu√™te](https://arxiv.org/abs/2006.14799) fournit une discussion approfondie de l'√©valuation en NLP.

\\
***Raisonnement diff√©rent pour le texte***
\\
Les moteurs de recherche actuels permettent g√©n√©ralement d'utiliser une requ√™te pour obtenir des pages ou des informations pertinentes. Cependant, ils ne sont pas aussi performants lorsqu'il s'agit d'obtenir des r√©ponses √† des requ√™tes qui impliquent plusieurs documents pour arriver √† une r√©ponse, comme c'est le cas avec les r√©ponses √† des questions √† sauts multiples. Les m√©thodes actuelles utilisent soit une extraction + lecture (soutenue par un DNN), soit une base de connaissances pour effectuer une certaine forme d'extraction afin d'aider √† traiter cette t√¢che particuli√®re et √† trouver des r√©ponses raisonnables √† ces questions. Cette derni√®re m√©thode fonctionne bien jusqu'√† ce que l'information soit mise √† l'√©chelle et que la travers√©e sur le graphique des connaissances devienne impossible. Il est important de parcourir le graphique efficacement pour aboutir √† une r√©ponse. Bhuwan Dhingra propose un [syst√®me de bout en bout](https://blog.ml.cmu.edu/2020/05/15/differentiable-reasoning-over-text/) dans lequel l'op√©ration de travers√©e est diff√©renci√©e et peut √™tre entra√Æn√©e efficacement, m√™me sur de grands corpus comme Wikip√©dia. Gr√¢ce √† cette approche, la m√©thode est capable de raisonner sur un texte et de r√©pondre √† des questions, m√™me celles qui n√©cessitent plusieurs sauts. L'auteur fournit √©galement une d√©mo qui pr√©sente le syst√®me utilis√© pour r√©pondre √† des questions √† sauts multiples.

\\
![](https://cdn-images-1.medium.com/max/800/0*fxVzcy6AV8V8fVp2.png)

*Source:* [*CMU Blog*](https://blog.ml.cmu.edu/2020/05/15/differentiable-reasoning-over-text/)


\\
***DE‚´∂TR : D√©tection d'objets de bout en bout avec des Transformers***
\\
[Carion et al. (2020)](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers) proposent un nouvel algorithme de d√©tection d'objets qui exploite l'architecture de l'encodeur-d√©codeur du Transformer pour la d√©tection d'objets. Le DETR, comme on appelle le mod√®le, est un syst√®me non-autoregressif de bout en bout qui fait des pr√©dictions en parall√®le, ce qui permet au mod√®le d'√™tre rapide et efficace. La nouveaut√© r√©side dans l'utilisation directe d'un bloc Transformer pour effectuer la t√¢che de d√©tection d'objet, qui est pr√©sent√©e comme un probl√®me d'image √† image. Cette t√¢che est cha√Æn√©e avec un CNN qui extrait les informations locales des images. Cela signifie que le bloc Transformer est charg√© de raisonner sur l'image dans son ensemble et de produire en parall√®le l'ensemble final de pr√©dictions. L'id√©e g√©n√©rale est de permettre le raisonnement des relations entre les objets et le contexte global de l'image, ce qui est utile pour la pr√©diction des objets dans une image.
\\
![](https://cdn-images-1.medium.com/max/800/1*QjhCl88V3GzuXZWl-SGHHg.png)

*DETR*[*source*](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers)


\\
***Publications de type enqu√™te***

\\
Si vous commencez √† faire du NLP bas√© sur un apprentissage approfondi, la plupart des gens vous recommandent de commencer par apprendre √† √©crire le code pour les t√¢ches de classification ou les pipelines de collecte de donn√©es. Ces enqu√™tes peuvent vous aider √† d√©velopper votre intuition pour ces taches :

- [Deep Learning Based Text Classification: A Comprehensive Review](https://arxiv.org/abs/2004.03705)
- [Contextual Word Representations: Putting Words into Computers](https://dl.acm.org/doi/pdf/10.1145/3347145?download=true)
- [Natural Language Processing Advancements By Deep Learning: A Survey](https://arxiv.org/abs/2003.01200)
- [A Survey on Data Collection for Machine Learning: a Big Data‚Ää‚Äî‚ÄäAI Integration Perspective](https://arxiv.org/abs/1811.03402)

\\
***D√©couverte de mod√®les symboliques issus d'un apprentissage approfondi avec des biais inductifs***

\\
[Cranmer et al. (2020)](https://arxiv.org/abs/2006.11287) ont d√©velopp√© une approche de r√©seau neuronal graphique (GNN) pour apprendre des repr√©sentations de faible dimension qui sont ensuite exploit√©es pour d√©couvrir et extraire des relations physiques par r√©gression symbolique. En tirant parti des forts biais inductifs des GNN, le cadre propos√© peut √™tre appliqu√© sur des donn√©es √† grande √©chelle et entra√Æn√© √† adapter les expressions symboliques aux fonctions internes apprises par le mod√®le. En utilisant les GNN, les auteurs ont pu entra√Æner le mod√®le √† apprendre des repr√©sentations interpr√©tables et √† am√©liorer la g√©n√©ralisation de celui-ci. Les cas d'utilisation abord√©s avec la m√©thodologie comprennent la red√©couverte des lois de la force, la red√©couverte des Hamiltoniens, et l'application √† un d√©fi astrophysique du monde r√©el (*pr√©vision de la quantit√© de mati√®re en exc√®s pour un halo de mati√®re noire.*).

\\
![](https://cdn-images-1.medium.com/max/800/1*gS5npj4Y1CjGp64VXSMy4A.png)

*Figure par* [*Cranmer et al. (2020)*](https://arxiv.org/abs/2006.11287)
\\


# Outils et jeux de donn√©es ‚öôÔ∏è

***NLP de HuggingFace***
\\
HuggingFace a publi√© une librairie Python appel√©e [nlp](https://github.com/huggingface/nlp) qui vous permet de partager et de charger facilement des donn√©es/m√©triques avec un acc√®s √† plus de 100 ensembles de donn√©es NLP. Parmi les avantages de cette librairie, on peut citer l'interop√©rabilit√© avec d'autres biblioth√®ques  de ML, la rapidit√© d'ex√©cution, l'utilisation efficace de la m√©moire, la mise en cache intelligente, et bien d'autres choses encore. La librairie est accompagn√©e d'un [site web](https://huggingface.co/nlp/viewer/?dataset=glue&config=cola) pour explorer les ensembles de donn√©es.

\\
![](https://cdn-images-1.medium.com/max/800/1*jTnEcrpdG2h4Yjj7WZ9zwQ.png)
\\

***Hateful Memes Challenge***
\\
Le Hateful Memes Challenge est un concours visant √† aider √† construire des syst√®mes multimodaux plus efficaces pour les discours de haine. Dans le cadre de ce d√©fi, un ensemble de donn√©es √† grande √©chelle appel√© [Hateful Memes](https://www.drivendata.org/competitions/64/hateful-memes/) est fourni. Il combine texte et images, ce qui en fait une t√¢che difficile. L'ensemble de donn√©es a √©t√© cr√©√© par Facebook AI et h√©berg√© par DrivenData. La cagnotte est de 100 000 dollars, sans compter que la comp√©tition fait √©galement partie du parcours du concours NeurIPS. Vous pourrez √©galement trouver le [code de d√©marrage](https://github.com/facebookresearch/mmf/tree/master/projects/hateful_memes) pour vous familiariser avec la t√¢che.

\\
![](https://cdn-images-1.medium.com/max/800/1*Gww3vx0kiT33gCxjKyk43w.png)
\\

***TextAttack***
\\
[TextAttack](https://github.com/QData/TextAttack) est une librairie Python permettant de d√©velopper diff√©rentes attaques adverses en NLP et d'examiner les r√©sultats des mod√®les, d'accro√Ætre la g√©n√©ralisation des mod√®les par l'augmentation de donn√©es et d'entra√Æner facilement les mod√®les de NLP √† l'aide de commandes de base.

\\
***GameGAN***
\\
NVIDIA a entra√Æn√© un nouveau mod√®le d'IA appel√© [GameGAN](https://blogs.nvidia.com/blog/2020/05/22/gamegan-research-pacman-anniversary/) qui prend en entr√©e 50000 √©pisodes du populaire PAC-MAN et apprend les r√®gles de l'environnement en regardant le sc√©nario impliquant un agent se d√©pla√ßant dans le jeu. NVIDIA affirme qu'il s'agit du premier mod√®le de r√©seau neuronal capable d'imiter un ing√©nieur de jeu informatique en utilisant des GAN. Cette capacit√© peut √™tre utilis√©e par les d√©veloppeurs de jeux pour automatiser la g√©n√©ration de mises en page pour diff√©rents niveaux de jeu ou m√™me construire des syst√®mes de simulation plus sophistiqu√©s.

\\
***Question de compr√©hension : COVID-Q : Plus de 1 600 questions sur la COVID-19***
\\
Nous avons r√©cemment assist√© √† une explosion des applications NLP utilis√©es pour mieux comprendre les ensembles de donn√©es li√©s √† la COVID-19. R√©cemment, une √©quipe de chercheurs a cr√©√© un ensemble de donn√©es comprenant environ 1 600 questions li√©es √† la COVID, annot√©es par cat√©gorie et type de question. Voici quelques liens utiles si vous souhaitez en savoir plus sur le projet : [ensemble de donn√©es sur GitHub](https://github.com/JerryWei03/COVID-Q), [article](https://arxiv.org/abs/2005.12522), et [article de blog](https://towardsdatascience.com/what-are-people-asking-about-covid-19-a-new-question-classification-dataset-adcaeaddcce4). Si vous souhaitez savoir comment cr√©er un tel ensemble de donn√©es, l'un des auteurs pr√©sentera son exp√©rience dans le cadre de l'une de nos [rencontres] en ligne (https://www.meetup.com/dair-ai/events/271420297/).
\\


# Articles et Blog ‚úçÔ∏è

*** Recettes pour construire un chatbot √† domaine ouvert***
\\
Constanza Fierro a r√©cemment [publi√©](https://medium.com/dair-ai/recipes-for-building-an-open-domain-chatbot-488e98f658a7) un article sur les marches √† suivre afin de cr√©er un chatbot √† domaine ouvert. L'article vise √† r√©sumer un agent conversationnel propos√© par l'IA de Facebook, BlenderBot, qui am√©liore la conversation gr√¢ce √† un fine-tuning sur des ensembles de donn√©es qui mettent l'accent sur la personnalit√©, l'empathie et la connaissance. Une des nouveaut√©s de ce travail est la capacit√© d'entra√Æner les mod√®les √† g√©n√©rer et √† avoir un dialogue plus humain, m√™me avec des mod√®les plus petits.

\\
***Apprentissage machine sur les graphiques : Un mod√®le et une taxonomie compl√®te***
\\
Cette [√©tude](https://arxiv.org/abs/2005.03675) fournit une taxonomie compl√®te des approches qui visent √† apprendre les repr√©sentations graphiques. Les auteurs pr√©sentent un nouveau cadre pour unifier les diff√©rents paradigmes qui existent pour les m√©thodes d'apprentissage des repr√©sentations graphiques. Cette unification est importante pour mieux comprendre l'intuition derri√®re les m√©thodes et pour faire progresser ce domaine de recherche.

\\
![](https://cdn-images-1.medium.com/max/800/1*sh_MUYhT1P1t-Vo44rzjpw.png)

*Source:* [*https://arxiv.org/abs/2005.03675*](https://arxiv.org/abs/2005.03675)

\\
*** Zero-Shot Learning en NLP***
\\
L'un des objectifs ambitieux des chercheurs en ML est de cr√©er des syst√®mes d'IA capables d'effectuer du *zero-shot learning*, ce qui, dans le contexte du NLP, signifie simplement concevoir et entra√Æner un mod√®le √† effectuer une t√¢che pour laquelle il n'a pas √©t√© explicitement entrain√©. En d'autres termes, vous pouvez effectuer de nouvelles t√¢ches de NLP sans aucun fine-tuning, comme le GPT-2 l'a fait pour la traduction automatique. Si vous souhaitez en savoir plus sur les approches sur ce sujet, Joe Davidson a √©crit un [article de blog d√©taill√©](https://joeddav.github.io/blog/2020/05/29/ZSL.html) sur le sujet qui comprend m√™me une d√©mo et un Colab.
Un autre [guide illustr√©](https://amitness.com/2020/05/zero-shot-text-classification/) √† consulter est celui d'Amit Chaudhary qui explique comment le zero-shot learning est utilis√© pour la classification de textes.

\\
![](https://cdn-images-1.medium.com/max/800/0*i0L_fbFMBBNF7QFU.png)

*Source:* [*Amit Chaudhary*](https://amitness.com/2020/05/zero-shot-text-classification/)
\\
***Recherche en IA, reproductibilit√© et incitations***
\\
Dans un r√©cent [article de blog](https://dennybritz.com/blog/ai-replication-incentives/), Denny Britz aborde les questions de la reproductibilit√© de l'apprentissage approfondi ainsi que les syst√®mes d'incitation universitaires et notamment la mani√®re dont ceux-ci sont √† l'origine de certaines tendances de la recherche dans la communaut√© du NLP. Parmi les sujets abord√©s figurent les diff√©rences entre reproduction et r√©plication, le budget de calcul, les protocoles d'√©valuation, l'incompr√©hension de l'open source et les incitations du haut vers le bas et du bas vers le haut. C'est un article int√©ressant car il aborde des sujets tels que le budget de calcul et la reproductibilit√©, qui font g√©n√©ralement d√©faut dans les rapports scientifiques. Denny aborde √©galement l'id√©e du billet de loterie gagnant qui d√©clare que le fait de trouver une variante du mod√®le qui fonctionne pour vos exp√©riences n'implique pas qu'elle se g√©n√©ralisera √† des donn√©es sur diff√©rentes distributions de donn√©es. En fait, dans la majorit√© des cas, les billets perdants ou le reste des variations √©chou√©es ne sont pas signal√©s et ce que vous obtenez est g√©n√©ralement un papier poli. Alors, comment pouvons-nous reproduire le chemin complet vers la conclusion ?
\\


# Education üéì

***Fun Python***
\\
Rada Mihalcea [a d√©voil√©](https://web.eecs.umich.edu/~mihalcea/urls/FunPython.pdf) une s√©rie compl√®te de notebooks Python pour se familiariser avec Python. Le mat√©riel couvre les concepts de base en Python et a √©t√© con√ßu pour les √©l√®ves de 10 √† 12 ans.
\\
![](https://cdn-images-1.medium.com/max/800/1*6RD-wwbui3D8ZQOUV6nr5g.jpeg)


\\
***Deep Mind x  UCL  : S√©rie de conf√©rences sur l'apprentissage profond***
\\
DeepMind a publi√© une s√©rie de [conf√©rences vid√©o gratuites](https://www.youtube.com/playlist?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF) couvrant des sujets relatifs √† l'apprentissage machine qui vont des mod√®les avanc√©s pour la vision par ordinateur aux r√©seaux adversaires g√©n√©rateurs en passant par l'apprentissage de la repr√©sentation non supervis√©e.

\\
***Exemples de codes Keras***
\\
Au cours des derniers mois, la communaut√© a ajout√© plusieurs [exemples de code](https://keras.io/examples/) sur le site web de Keras. Les exemples vont des mod√®les de NLP aux algorithmes de vision par ordinateur en passant par les architectures d'apprentissage g√©n√©ratif. 

\\
***Applied Machine Learning 2020***
\\
Andreas Muller publie [des vid√©os](https://www.youtube.com/watch?v=d79mzijMAw0&list=PL_pVmAaAnxIRnSw6wiCpSvshFyCREZmlM) de son cours, Applied Machine Learning 2020. Celui-ci comprend des sujets comme l'introduction aux r√©seaux de neurones, les s√©ries chronologiques et les pr√©visions, le clustering, etc.


\\
***Deep Learning Drizzle***
\\
Au cas o√π vous auriez du mal √† trouver des cours de NLP ou de ML, ce [site web](https://deep-learning-drizzle.github.io/) poss√®de l'une des bases de donn√©es de cours en ligne les plus compl√®tes. La plupart d'entre eux sont disponibles sous forme de conf√©rences video.

\\
![](https://cdn-images-1.medium.com/max/800/1*yV716lh60cVP0oHzKbPMXA.png)

*Source: Deep Learning Drizzle*

\\
***CMU Neural Nets for NLP 2020***
\\
Graham Neubig publie toutes les [conf√©rences vid√©o](https://www.youtube.com/playlist?list=PL8PYTP1V4I8CJ7nMxMC8aXv8WqKYwj-aJ) du cours intitul√© "Neural Networks for NLP" (√©dition 2020). Le cours couvre des sujets tels que les CNN pour le texte, les astuces d'efficacit√© pour le NLP, l'attention, le multit√¢che et l'apprentissage multilingue. Il contient √©galement des notebooks d'accompagnement avec des mises en ≈ìuvre de certains concepts abord√©s dans le cours.

\\
***PyTorch Recipes***
\\
[PyTorch Recipes](https://pytorch.org/tutorials/recipes/recipes_index.html) est une collection de tutoriels PyTorch qui vise √† enseigner aux utilisateurs les caract√©ristiques sp√©cifiques de PyTorch. Ils sont destin√©s √† √™tre facilement applicables et sont diff√©rents des longs tutoriels qui sont √©galement disponibles sur le site web.

\\
![](https://cdn-images-1.medium.com/max/800/1*jcDapaNSSNuNx8f314WJKg.png)
\\

# Rester inform√© üéØ

Ces derni√®res ann√©es, nous avons assist√© √† une explosion de projets et de documents sur le ML. Il est devenu difficile de suivre ce qui se passe et les tendances en mati√®re de ML. Nous avons √©galement vu des efforts incroyables de la part de la communaut√© pour aider √† distiller cette information. √Ä partir de ce num√©ro de la newsletter, nous allons inclure une section pr√©sentant certaines des ressources qui devraient aider les lecteurs √† suivre et √† rester inform√©s sur les questions int√©ressantes et urgentes en mati√®re de ML. Voici la liste de ce num√©ro :

- [**Underrated ML Podcast**](https://www.underratedml.com/) [](https://www.underratedml.com/) :  un podcast qui pr√©sente des id√©es sous-estim√©es sur le ML
- [**Papers with Code**](https://paperswithcode.com/) [](https://paperswithcode.com/) : site web qui permet d‚Äôam√©liorer l'accessibilit√© des derni√®res publications via l‚Äôexplication de codes.
- [**Made with ML**](https://madewithml.com/) : plateforme communautaire qui permet de se tenir au courant des derniers projets de ML.
- [**ML Paper Discussions**](https://github.com/dair-ai/ml-nlp-paper-discussions) : discussion hebdomadaire sur les derniers articles de ML et de NLP
- [**Yannic Kilcher**](https://www.youtube.com/c/yannickilcher) : cha√Æne YouTube fournissant d'excellentes explications d‚Äôarticles.
\\


# Mentions sp√©ciales ‚≠êÔ∏è
- Deeplearning.ai [publie](https://www.coursera.org/specializations/natural-language-processing) les deux premiers cours consacr√©s au PNL. Les cours 3 et 4 seront bient√¥t publi√©s.
- Cet [article](https://www.dropbox.com/s/ec3y4khbk38e29i/NeuralNetworksEN.pdf?dl=0) pr√©sente un aper√ßu math√©matique des r√©seaux neuronaux discriminants et des r√©seaux neuronaux g√©n√©rateurs. Il a √©t√© √©crit par Gabriel Peyr√©.
- [YOLOv4](https://arxiv.org/abs/2004.10934) est la derni√®re mise √† jour du populaire algorithme de d√©tection d'objets qui vise √† fournir un algorithme plus rapide pour localiser et classifier les objets.
- Suraj Patil propose ce [tutoriel] (https://colab.research.google.com/drive/176NSaYjc2eeI-78oLH_F9-YV3po3qQQO?usp=sharing) sur la fa√ßon d'affiner le mod√®le T5 √† l'aide de la librairie Transformers.
- [VidPress](http://research.baidu.com/Blog/index-view?id=134) est l'un des derniers outils construits par Baidu pour cr√©er des vid√©os directement √† partir d'articles textuels.
- Enfin quelques [impl√©mentations](https://github.com/kushalj001/pytorch-question-answering) pour la t√¢che de questions/r√©ponses utilisant PyTorch. C'est un travail r√©alis√© par [Kushal] (https://twitter.com/kushalj001).

----------

Vous pouvez retrouver la pr√©c√©dente newsletter [ici](https://dair.ai/NLP_Newsletter_-11_-FR/) \\

Si vous avez des jeux de donn√©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine √©dition de la newletter, vous pouvez utiliser ce [formulaire](https://forms.gle/3b7Q2w2bzsXE6uYo9).
\\

[Abonnez-vous](https://dair.ai/newsletter/) pour recevoir les prochains num√©ros dans votre bo√Æte mail.

