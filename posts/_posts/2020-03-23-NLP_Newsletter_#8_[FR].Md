---
layout: post
title: "NLP Newsletter #8 [FR]: NeRF, CORD-19, Stanza, Text Generation 101, Notebooks, SECNLP, Dreamer,‚Ä¶"
author: lbourdois
excerpt: "This issue covers topics that range from synthesizing novel views of complex scenes to tutorials for text generation and transfer learning to survey papers on contextual embeddings and pretrained language models."
modified:
comments: true
tags: [nlp_newsletter]
image:
  thumb: nlp_newsletter_8.png
---


![](https://cdn-images-1.medium.com/max/1200/1*SptuncVzQw49OZFlDVaQdw.png)

# Avant-propos d‚ÄôElvis
- nous avons ajout√© une meilleure cat√©gorisation pour tous les TL;DR et les r√©sum√©s des publications li√©es au NLP : [repo](https://github.com/dair-ai/nlp_paper_summaries).
- tous les num√©ros et traductions de la newsletter sur le NLP sont r√©pertori√©s [ici](https://github.com/dair-ai/nlp_newsletter).
- cette semaine, nous avons √©galement introduit [Notebooks,](https://github.com/dair-ai/notebooks) une plateforme permettant de partager facilement les notebooks avec l'ensemble de la communaut√©. Si vous avez des notebooks que vous aimeriez partager avec la communaut√©, contactez-nous.
- nous avons partag√© un [tutoriel](https://colab.research.google.com/drive/1YuL0iqxaz09qR0_2Fgyi2wQHgil_Seqg) sur la fa√ßon d'effectuer une classification multiclasses de sentiments utilisant la vectorisation de texte ; une fonctionnalit√© exp√©rimentale de TensorFlow 2.1.0 qui aide √† g√©rer le texte dans un r√©seau neuronal.


# Publications üìô

***Enqu√™tes sur les embeddings contextuelles et les mod√®les linguistiques***

\\
Ce [document](https://arxiv.org/abs/2003.07278v1) fournit une √©tude approfondie des approches d'apprentissage des embeddings contextuelles. Il comprend √©galement un examen de ses applications pour l'apprentissage par transfert, les m√©thodes de compression de mod√®les et les analyses de mod√®les.


\\
Un autre rapport comporte un [r√©sum√©](https://arankomatsuzaki.files.wordpress.com/2020/03/written_report.pdf) des m√©thodes utilis√©es pour am√©liorer les mod√®les de langage bas√©s sur les Transformers.

\\
Enfin, une autre [enqu√™te compl√®te](https://arxiv.org/pdf/2003.08271.pdf) sur les mod√®les linguistiques et qui fournit une taxonomie des mod√®les pr√©-entrain√©s en NLP.

\\
![](https://cdn-images-1.medium.com/max/800/1*1jLfdem3xZ0I3EVSyOy48g.png)

[*Qiu et al., 2020*](https://arxiv.org/pdf/2003.08271.pdf)

\\
***Visualiser les r√©seaux de neurones avec Grand Tour***

\\
Grand Tour est une approche lin√©aire (qui diff√®re des m√©thodes non lin√©aires telles que t-SNE) qui projette un ensemble de donn√©es en grandes dimensions en deux dimensions. Dans un nouvel article de Distill [article](https://distill.pub/2020/grand-tour/), Li et al (2020) proposent d'utiliser les capacit√©s du Grand Tour pour visualiser le comportement d'un r√©seau de neurones pendant son entra√Ænement. On peut citer par exemple, les changements de poids et la mani√®re dont ils affectent le processus d'entra√Ænement, la communication de couche √† couche dans le r√©seau ou encore l'effet des exemples contradictoires lorsqu'ils sont pr√©sent√©s au r√©seau neuronal.

\\
![](https://cdn-images-1.medium.com/max/800/1*XYCRZOzslb-ZRYlmtHV0Ng.png)

*Source:* [*Distill*](https://distill.pub/2020/grand-tour/)

\\
***Initiations au m√©ta-apprentissage pour la d√©couverte de m√©dicaments***

\\
Il a √©t√© largement rapport√© que le m√©ta-apprentissage peut permettre d‚Äôam√©liorer les crit√®res d'apprentissage en peu de temps. Cela est particuli√®rement utile dans les situations o√π les donn√©es sont limit√©es, comme c'est g√©n√©ralement le cas pour la d√©couverte de m√©dicaments. Un r√©cent [travail](https://arxiv.org/abs/2003.05996) a appliqu√© une approche de m√©ta-apprentissage appel√©e Model-Agnostic-Meta-Learning (MAML) et d'autres variantes pour pr√©dire les propri√©t√©s chimiques et les activit√©s dans des environnements √† faibles ressources. Les r√©sultats montrent que les approches de m√©ta-apprentissage fonctionnent de mani√®re comparable aux bases de r√©f√©rence.

\\
***NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis***

\\
Un travail impliquant des chercheurs de l'Universit√© de Californie √† Berkeley, Google Research et l'Universit√© de Californie √† San Diego pr√©sente une m√©thode ([NeRF](http://www.matthewtancik.com/nerf)) pour synth√©tiser des vues in√©dites de sc√®nes complexes. En utilisant une collection d'entr√©es d'images RVB, le mod√®le prend des coordonn√©es 5D (emplacement et direction spatiale), entra√Æne un r√©seau enti√®rement connect√© pour optimiser *une fonction de sc√®ne volum√©trique continue*, et produit la densit√© de volume et la radiance RVB √©mise en fonction de la vue pour cet emplacement. Les valeurs de sortie sont compos√©es ensemble le long d'un rayon de la cam√©ra et rendues sous forme de pixels. Ces sorties rendues diff√©rentiables sont utilis√©es pour optimiser les repr√©sentations de la sc√®ne *en minimisant l'erreur de rendu de tous les rayons de la cam√©ra* √† partir des images RVB. Compar√© √† d'autres approches de synth√®se de vues tr√®s performantes, le NeRF est qualitativement et quantitativement meilleur et permet de rem√©dier aux incoh√©rences de rendu telles que le manque de d√©tails et les artefacts de scintillement ind√©sirables.

\\
![](https://cdn-images-1.medium.com/max/800/1*E6RQL5jdtHXR98BJJREDYg.png)

\\
***Pr√©sentation de Dreamer***

\\
[Dreamer](https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html) est un agent d'apprentissage par renforcement propos√© par les chercheurs de DeepMind et de Google AI. Son objectif est de rem√©dier √† certaines limitations (par exemple, la myopie et l'inefficacit√© des calculs) pr√©sentes dans les agents sans mod√®le. Dreamer est entra√Æn√© pour mod√©liser le monde, ce qui offre √©galement la possibilit√© d'apprendre des comportements √† long terme par r√©tropropagation en utilisant les pr√©dictions du mod√®le. Les r√©sultats de SoTA sont obtenus sur 20 t√¢ches de contr√¥le continu bas√©es sur les entr√©es d'images fournies. En outre, le mod√®le est efficace sur le plan des donn√©es et fait des pr√©dictions en parall√®le, ce qui le rend plus efficace sur le plan des calculs. Les trois t√¢ches impliqu√©es dans l‚Äôentra√Ænement de l'agent qui permettent d'atteindre les diff√©rents objectifs sont r√©sum√©es dans la figure ci-dessous :

\\
![](https://cdn-images-1.medium.com/max/800/1*DOlPDgvNu1kpTeogcLve-A.png)

*Source:* [*Google AI Blog*](https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html)

# Cr√©ativit√©, √©thique et soci√©t√© üåé

***COVID-19 Open Research Dataset (CORD-19)***

\\
Pour encourager l'utilisation de l'IA pour lutter contre le COVID-19, l‚Äô Allen Institute of AI a publi√© le [COVID-19 Open Research Dataset (CORD-19)](https://pages.semanticscholar.org/coronavirus-research), une ressource gratuite et ouverte pour promouvoir la collaboration mondiale en mati√®re de recherche. L'ensemble de donn√©es contient des milliers d'articles scientifiques qui permettent √† la communaut√© NLP d'obtenir des informations qui peuvent aider √† lutter contre [COVID-19](https://www.who.int/emergencies/diseases/novel-coronavirus-2019).


\\
***SECNLP: une enqu√™te sur le traitement du langage naturel dans le milieu clinique***

\\
[SECNLP](https://www.sciencedirect.com/science/article/pii/S1532046419302436) est une enqu√™te qui comprend un aper√ßu d√©taill√© d'une grande vari√©t√© de m√©thodes et de techniques de NLP appliqu√©es dans le domaine clinique. L'aper√ßu met principalement l'accent sur les m√©thodes d'embedding, les probl√®mes/d√©fis pos√©s par ces m√©thodes et une discussion sur les futures orientations de recherche.

\\
***AI for 3D Generative Design***

\\
Cet [article](https://blog.insightdatascience.com/ai-for-3d-generative-design-17503d0b3943) aborde une approche qui a √©t√© utilis√©e pour g√©n√©rer des objets 3D √† partir de descriptions en langage naturel. L'id√©e est de cr√©er une solution qui permet √† un concepteur de r√©it√©rer rapidement dans le processus de conception et de pouvoir explorer plus largement l'espace de conception. Apr√®s avoir cr√©√© une base de connaissances de l'espace de conception compos√©e de mod√®les 3D et de descriptions textuelles, deux autoencoders (voir figure ci-dessous) ont √©t√© utilis√©s pour coder ces connaissances de mani√®re √† pouvoir interagir avec elles de mani√®re intuitive. Le mod√®le assembl√© peut ensuite accepter une description textuelle et g√©n√©rer un design 3D, essayez-le dans cette [d√©mo](https://insight2020a.streamlit.io/starstorms9/shape/).

\\
![](https://cdn-images-1.medium.com/max/800/0*pbBv2Wa5QX7lUufY.png)

[*Source*](https://blog.insightdatascience.com/ai-for-3d-generative-design-17503d0b3943)


# Outils et jeux de donn√©es ‚öôÔ∏è

***Stanza‚Ää: une librairie Python applicable √† plusieurs langues***

\\
Le Stanford NLP Group publie [Stanza](https://stanfordnlp.github.io/stanza/) (anciennement StanfordNLP), une librairie python qui fournit des outils d'analyse de texte pr√™ts √† l'emploi pour plus de 70 langues. Les capacit√©s comprennent la tokenisation, la lemmatisation, le POS, la NER, et bien plus encore. L'outil est construit sur la librairie PyTorch et prend en charge l'utilisation de GPU et des mod√®les neuronaux pr√©-entra√Æn√©s.

\\
[Explosion](https://github.com/explosion/spacy-stanza) a √©galement construit un wrapper autour de Stanza qui vous permet d'interagir avec les mod√®les Stanza comme un pipeline spaCy.

\\
***GridWorld***

\\
Pablo Castro a cr√©√© un [site web](https://gridworld-playground.glitch.me/) qui fournit un terrain de jeu pour cr√©er un environnement sous la forme d‚Äôune grille afin d'observer et de tester comment un agent d'apprentissage par renforcement essaie de r√©soudre ce monde. Parmi les caract√©ristiques, on trouve la possibilit√© de modifier les param√®tres d'apprentissage et d'environnement en temps r√©el, de changer la position de l'agent et de transf√©rer des valeurs entre deux agents.

\\
![](https://cdn-images-1.medium.com/max/800/1*D1e6ixTEONl21t0UNmcaog.png)

\\
***X-Stance: un jeu de donn√©es multilingues et multicibles pour la d√©tection de la position***

\\
[*Stance detection*](http://nlpprogress.com/english/stance_detection.html)  peut √™tre utilis√©e pour √©valuer de fausses informations. Jannis Vamvas et Rico Sennrich ont r√©cemment publi√© un [ensemble de donn√©es sur la d√©tection de positions √† grande √©chelle](https://arxiv.org/abs/2003.08385) compos√© de textes √©crits par des candidats aux √©lections en Suisse. Les textes sont disponibles en plusieurs langues, ce qui pourrait potentiellement conduire √† des √©valuations multilingues sur la t√¢che de d√©tection de la position. Les auteurs proposent √©galement l'utilisation d'un BERT multilingue qui permet d'obtenir des performances satisfaisantes en mati√®re de transfert z√©ro entre langues et entre cibles. L'apprentissage entre cibles est une t√¢che difficile. Les auteurs ont donc utilis√© une technique simple impliquant des cibles standardis√©es pour entrainer un mod√®le unique sur toutes les questions √† la fois.

\\
***Cr√©er des heatmaps textuelles interactives pour les notebooks Jupyter***

\\
Andreas Madsen a cr√©√© une librairie python appel√©e [TextualHeatMap](https://github.com/AndreasMadsen/python-textualheatmap) qui peut √™tre utilis√©e pour visualiser et mieux comprendre quelles parties d'une phrase le mod√®le utilise pour pr√©dire le mot suivant, comme dans les mod√®les de langage.

\\
![](https://cdn-images-1.medium.com/max/800/0*IY3tKkznwarnRxdo.gif)

*Source:* [*textualheatmap*](https://github.com/AndreasMadsen/python-textualheatmap)


# Articles et Blog ‚úçÔ∏è

***Comment g√©n√©rer du texte avec la librairie Transformers***

\\
HuggingFace a publi√© un [article](https://huggingface.co/blog/how-to-generate) expliquant les diff√©rentes m√©thodes utilis√©es pour la g√©n√©ration du langage, en particulier pour les approches bas√©es sur les transformers. Parmi les techniques abord√©es, on trouve la recherche gourmande, la recherche par faisceau, l'√©chantillonnage, l'√©chantillonnage top-k et l'√©chantillonnage top-p (noyau). Il existe de nombreux articles de ce type, mais les auteurs ont pass√© plus de temps √† expliquer le c√¥t√© pratique de ces m√©thodes et la fa√ßon dont elles peuvent √™tre appliqu√©es via des extraits de code.

\\
***Entrainer RoBERTa de z√©ro***

\\
Motiv√© par l'absence d'un guide complet indiquant comment entra√Æner un mod√®le de langage de type BERT √† partir de z√©ro en utilisant la librairie du Transformer, Marcin Zablocki partage ce [tutoriel] d√©taill√© (https://zablo.net/blog/post/training-roberta-from-scratch-the-missing-guide-polish-language-model/). Le guide montre comment entra√Æner un transformer pour la langue polonaise et donne des conseils sur les erreurs courantes √† √©viter, la pr√©paration des donn√©es, la configuration, la tokenization, le suivi du processus d‚Äôentra√Ænement et le partage du mod√®le.

\\
![](https://cdn-images-1.medium.com/max/800/0*PFPgjeUmzvazglqg.png)



# Education üéì

***D√©marrer avec JAX (MLPs, CNNs & RNNs)***

\\
Robert Lange a r√©cemment publi√© un [tutoriel complet](https://roberttlange.github.io/posts/2020/03/blog-post-10/) sur la fa√ßon d‚Äôentra√Æner un GRU-RNN avec JAX. D‚Äôautres ressources li√©es √† JAX sont disponibles dans la [newsletter pr√©c√©dente]( https://dair.ai/NLP_Newsletter_-7_-FR/).


\\
***NLP for Developers : Word Embeddings***

\\
Rachael Tatman a publi√© le premier √©pisode d'une nouvelle s√©rie intitul√©e NLP for Developers qui couvrira les meilleures pratiques sur la fa√ßon d'appliquer un large √©ventail de m√©thodes de NLP. Le [premier √©pisode](http://youtube.com/watch?v=oUpuABKoElw&feature=emb_logo) comprend une introduction au word embeddings et √† leur utilisation.


\\
***Thomas Wolf : une introduction √† l‚Äôapprentissage par transfert et √† HuggingFace***

\\
Thomas Wolf a pr√©sent√© sa [conf√©rence](https://www.youtube.com/watch?v=rEGB7-FlPRs&feature=youtu.be) sur l'apprentissage par transfert au NLP Zurich meetup, offrant une introduction √† l'apprentissage par transfert en NLP. La conf√©rence comprend un aper√ßu des r√©centes avanc√©es en NLP et une introduction √† Transformers et Tokenizers, deux des librairies les plus populaires publi√©es par l'√©quipe et les contributeurs de HuggingFace.

\\
![](https://cdn-images-1.medium.com/max/800/1*PMZ8ptBWo4wZ432kr-FXqA.png)


# Mentions sp√©ciales ‚≠êÔ∏è

Amit Chaudhary partage un [article](https://amitness.com/2020/02/back-translation-in-google-sheets/) qui montre comment tirer parti de la fonction de traduction de Google Sheets afin d‚Äôaugmenter un corpus de texte limit√©.

\\
New York NLP organisera une [r√©union en ligne](https://www.meetup.com/NY-NLP/events/269502774/) pour une conf√©rence intitul√©e "Utiliser Wikip√©dia et Wikidata en NLP" .

\\
Lavanya Shukla a √©crit un [tutoriel](https://app.wandb.ai/cayush/pytorchlightning/reports/Use-Pytorch-Lightning-with-Weights-%26-Biases--Vmlldzo2NjQ1Mw) sur la fa√ßon d'utiliser PyTorch Lightning pour optimiser les hyperparam√®tres d'un r√©seau de neurones tout en profitant des structures/styles de code simples fournis dans PyTorch Lightning. Le mod√®le r√©sultant et ses performances sont visualis√©s √† l'aide des r√©sultats produits par le logger WandB.

\\
Un groupe de chercheurs a publi√© une [√©tude](https://arxiv.org/abs/2003.07845) qui examine plus en d√©tail pourquoi la batch normalization (BN) tend √† d√©grader les performances des m√©thodes bas√©es sur les transformers. Sur la base de ces r√©sultats, les auteurs proposent une nouvelle approche appel√©e ¬´ power normalization ¬ª pour traiter les probl√®mes rencontr√©s dans la BN. La m√©thode surpasse √† la fois la BN et la normalisation des couches (couramment utilis√©e de nos jours) sur une vari√©t√© de t√¢ches de NLP.

\\
Cet [article de blog] (https://www.datasciencecentral.com/profiles/blogs/10-timeless-reference-books) contient une longue liste de livres pour vous aider √† d√©marrer en ML.

----------

Vous pouvez retrouver la pr√©c√©dente newsletter [ici](https://dair.ai/NLP_Newsletter_-7_-FR/)

\\
Si vous avez des jeux de donn√©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine √©dition de la newletter, vous pouvez utiliser ce [formulaire](https://forms.gle/3b7Q2w2bzsXE6uYo9).

\\
[Abonnez-vous](https://dair.ai/newsletter/) pour recevoir les prochains num√©ros dans votre bo√Æte mail.
