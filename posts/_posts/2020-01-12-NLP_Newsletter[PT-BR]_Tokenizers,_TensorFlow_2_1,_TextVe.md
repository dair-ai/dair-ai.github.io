---
layout: post
title: "NLP Newsletter #1 [PT-BR]: Tokenizadores, TensorFlow 2.1, Vetoriza√ß√£o de Texto, TorchIO, D√©ficits de NLP,‚Ä¶"
author: billy_rick
modified:
comments: true
excerpt: ""
tags: [nlp_newsletter]
image:
  thumb: nlp_newsletter_1.png
---


![](https://cdn-images-1.medium.com/max/2400/1*gLVPodYjYd4YaF9sJbSpjg.png)


Ol√° e Feliz Ano Novo! Devido a v√°rios pedidos, eu decidi trazer de volta a **Newsletter de NLP**. Dessa vez irei mant√™-la pequena e focada (tamb√©m mantida no [reposit√≥rio](https://github.com/dair-ai/nlp_newsletter)). O objetivo dessa newsletter √© te manter informado em alguns dos mais recentes e interessantes t√≥picos relacionados a NLP e ML (em algumas categorias) sem tomar muito tempo do seu dia ocupado.


# Publica√ß√µes üìô

***Sistema de IA para resson√¢ncia de c√¢ncer de mama***
A DeepMind publicou um novo artigo na Nature intitulado ‚Äú[International evaluation of an AI system for breast cancer screening](https://www.nature.com/articles/s41586-019-1799-6)‚Äù (Avalia√ß√£o internacional de um sistema de IA para resson√¢ncia de c√¢ncer de mama). De acordo com os autores, o trabalho √© sobre a avalia√ß√£o de um sistema de IA que ultrapassa especialistas humanos na resson√¢ncia de c√¢ncer de mama. Se isso √© realmente alcan√ß√°vel pelos sistemas de IA atuais ainda est√° em debate e h√° uma cr√≠tica cont√≠nua nesse tipo de sistema e como ele √© avaliado. Aqui est√° um [pequeno resumo](https://www.nature.com/articles/d41586-019-03822-8) do artigo.

***Extra√ß√£o de Informa√ß√£o***
O Pankaj Gupta liberou publicamento sua tese de Ph.D. intitulada ‚Äú[Neural Information Extraction From Natural Language Text](https://www.researchgate.net/publication/336739252_PhD_Thesis_Neural_Information_Extraction_From_Natural_Language_Text)‚Äù (Extra√ß√£o neural de informa√ß√£o de texto de linguagem natural). A discuss√£o principal √© como extrair eficientemente as rela√ß√µes sem√¢nticas de texto em linguagem natural usando abordagens neurais. Tal esfor√ßo em pesquisa visa contribuir com a constru√ß√£o estruturada de bases de conhecimento, que possam ser usadas em uma s√©rie de aplica√ß√µes de NLP como busca na web, respostas de quest√µes, dentre outras.

***Recomenda√ß√µes Melhoradas***
Pesquisadores no MIT e na IBM desenvolveram um [m√©todo](http://news.mit.edu/2019/finding-good-read-among-billions-of-choices-1220) (publicado na NeurIPS ano passado) para categoriza√ß√£o, surgimento, e busca de documentos relevantes baseados numa combina√ß√£o de tr√™s ferramentas altamente usadas para an√°lise de texto: *modelagem de t√≥picos*, *embeddings de palavras*, e *transporte ideal*. O m√©todo tamb√©m d√° resultados promissores para a classifica√ß√£o de documentos. Tais m√©todos s√£o aplic√°veis em uma grande variedade de cen√°rios que requerem sugest√µes melhoradas e mais r√°pidas em grande escala tal como sistemas de busca e recomenda√ß√£o.


# ML e NLP Criatividade e Sociedade üé®

***Carreiras em IA***
O [relat√≥rio](https://hai.stanford.edu/sites/g/files/sbiybj10986/f/ai_index_2019_report.pdf) do √≠ndice de IA de 2019 sugere que h√° mais demanda que suprimento de praticantes de IA. Entretanto, h√° v√°rios aspectos de empregos relacionados a IA como transi√ß√µes de carreira e entrevistas que ainda n√£o s√£o propriamente definidas.

Nesse [post](https://towardsdatascience.com/how-i-found-my-current-job-3fb22e511a1f), Vladimir Iglovivok detalha sua carreira e aventura em ML desde a constru√ß√£o de sistemas de recomenda√ß√£o tradicionais a constru√ß√£o de modelos espetaculares de vis√£o computacional que ganharam competi√ß√µes no Kaggle. Ele agora trabalha em ve√≠culos aut√¥nomos na Lyft, mas a [jornada](https://towardsdatascience.com/how-i-found-my-current-job-3fb22e511a1f) de chegar l√° n√£o foi t√£o f√°cil.

Se voc√™ est√° realmente interessado e s√©rio em uma carreira em IA, a companhia do Andrew Ng, deeplearning.ai, fundou Workera, que visa a especificamente ajudar cientistas de dados e engenheiros de aprendizado de m√°quina com suas carreiras em IA. Obtenha o relat√≥rio oficial [aqui](https://workera.ai/candidates/report).


# Ferramentas e Base de Dados de ML/NLP ‚öôÔ∏è

***Um tokenizador ultrarr√°pido***
Hugging Face, a startup de NLP por tr√°s dos Transformers, liberou de forma open-sourced os Tokenizers, uma implementa√ß√£o ultrarr√°pida de tokeniza√ß√£o que pode ser usada em pipelines modernos de NLP. Cheque o [reposit√≥rio no GitHub ](https://github.com/huggingface/tokenizers) para a documenta√ß√£o em como usar os Tokenizers.

![](https://cdn-images-1.medium.com/max/1600/1*BGcXk6Yf9fXGZlEtxz1hcg.jpeg)


*Tokenizers‚Ää‚Äî‚ÄäPython*

O TensorFlow 2.1 incorpora uma nova camada de [Vetoriza√ß√£o de Texto](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) que permite que voc√™ lide facilmente com strings brutas e execute eficientemente uma normaliza√ß√£o de texto, tokeniza√ß√£o, gera√ß√£o de n-gramas, e indexa√ß√£o de vocabul√°rio. Leia o lan√ßamento aqui e cheque o [notebook Colab do Chollet](https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3) demonstrando como usar a feature para classifica√ß√£o de texto fim-a-fim.

***NLP e ML para Busca***
Um dos campos que fez tremendo progresso no ano passado foi NLP  com uma gama de melhoras e dire√ß√µes novas de pesquisa. Um dos dom√≠nio que pode potencialmente se beneficiar de transfer learning em NLP √© a *busca*.

Apesar de a busca pertencer ao campo de recupera√ß√£o da informa√ß√£o, h√° uma oportunidade para construir engines que melhores a busca sem√¢ntica usando t√©cnicas modernas de NLP como representa√ß√µes contextualizadas de um modelo baseado em transformer como [BERT](https://arxiv.org/abs/1810.04805). O Google liberou um [post de blog](https://www.blog.google/products/search/search-language-understanding-bert/) alguns meses atr√°s discutindo como eles est√£o alavancando modelos BERT para melhorar e entender buscas.

Se voc√™ est√° curioso sobre como representa√ß√µes contextualizadas podem ser aplicadas √† busca usando tecnologias abertas tais como Elasticsearch e Tensorflow, voc√™ pode dar uma olhada ou nesse [post](https://towardsdatascience.com/elasticsearch-meets-bert-building-search-engine-with-elasticsearch-and-bert-9e74bf5b4cf2) ou [aqui](https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a).

***An√°lise m√©dica de imagem***

[TorchIO](https://github.com/fepegar/torchio) √© um pacote de Python baseado na biblioteca popular chama PyTorch. TorchIO oferece funcionalidades para f√°cil e eficientemente ler e amostrar imagems m√©dicas 3D. Caracter√≠sticas incluem transforma√ß√µes espaciais para aumento e pr√©-processamento de dados.

![](https://cdn-images-1.medium.com/max/1600/0*FSPuSC8TK9X-NQ2q.gif)


[fonte](https://github.com/fepegar/torchio)


# √âtica em IA üö®

***Comportamento fraudulento na comunidade de ML***
Isso acabou de sair! O primeiro lugar de uma competi√ß√£o do Kaggle foi desqualificado por uma atividade fraudulenta. O time usou uma t√°tica esperta mas irrespons√°vel e inaceit√°vel ao vencer o primeiro lugar da competi√ß√£o. Aqui est√° a [hist√≥ria completa](https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/125436). Essa hist√≥ria real√ßa um dos muitos dos comportamentos inaceit√°vies que a comunidade de aprendizado de m√°quina quer mitigar. O uso apropriado e √©tico de tecnologias de ML √© o √∫nico caminho a seguir.

***Vi√©s de g√™nero em tradu√ß√£o de m√°quina***
A respeito do t√≥pico se tradu√ß√£o de m√°quina reflete vi√©s de g√™nero, um grupo de pesquisadores publicou esse excelente [artigo](https://arxiv.org/abs/1809.02208) apresentando um estudo de caso usando o tradutor do Google. Uma das descobertas dos autores afirma que o tradutor do Google ‚Äúexibe uma ted√™ncia forte a padr√µes masculinos, em particular para campos ligados a distribui√ß√µes desbalanceadas de g√™nero tal como trabalhos STEM.‚Äù

***Vi√©s e Justi√ßa em ML***
Se voc√™ quer se atualizar com tudo de √©tica e justi√ßa em IA, esse √© um √≥timo [epis√≥dio de podcast](https://twimlai.com/twiml-talk-336-trends-in-fairness-and-ai-ethics-with-timnit-gebru/) com a participa√ß√£o de Timnit Gebru e apresentado por TWIML.

Timit √© uma pesquisadora proeminente em justi√ßa em ML que, junto com Eun Seo Jo, publicou um [artigo](https://arxiv.org/abs/1912.10389) onde identificam cinco abordagens chave em pr√°ticas de coleta de documentos em arquivos que podem prover m√©todos mais confi√°veis para coleta de dados em ML sociocultural. Isso pode potencialmente levar a um m√©todo de coleta de dados mais sistem√°tico, ganhado de uma pesquisa colaborativa.

Sina Fazelpour e Zachary Lipton recentemente publicaram um [artigo](http://zacklipton.com/media/papers/fairness-non-ideal-fazelpour-lipton-2020.pdf) onde argumentam que, devido √† natureza de como nosso mundo n√£o ideal surgiu, √© poss√≠vel que ML justo, baseado no pensamento ideal, pode potencialmente levar a pol√≠ticas e interven√ß√µes mal guiadas. Na verdade, suas an√°lises demonstraram ‚Äúque defici√™ncias em algoritmos de ML refletem problemas maiores encarados pela abordagem ideal.‚Äù


# Artigos e posts de blogs ‚úçÔ∏è

***D√©ficits em NLP***
Benjamin Heinzerling publicou um artigo interessante no The Gradient onde eles discute √°reas onde NLP falha, como compreens√£o de argumentos e racioc√≠nio de senso comum. Benjammin faz refer√™ncia ao [artigo](https://www.aclweb.org/anthology/P19-1459/) recente por Nivin & Kao, que desafia e questiona as capacidades de transfer learning e modelos de lingauem para entendimento de linguagem natural (NLU) em alto n√≠vel. [Leia](https://thegradient.pub/nlps-clever-hans-moment-has-arrived/) mais sobre esse excelente resumo da an√°lise executada na pesquisa.

***Destaques de NLP e ML em 2019***
Para o ano novo, eu liberei um [relat√≥rio](https://medium.com/dair-ai/nlp-year-in-review-2019-fb8d523bcb19) documentando alguns dos destaques mais interessantes em NLP e ML pelos quais passei em 2019.

Sebastian Ruder tamb√©m escreveu recentemente um [post de blog](https://ruder.io/research-highlights-2019/) excelente e detalhado sobre as top dez dire√ß√µes de pesquisa em ML e NLP que ele achou impactante em 2019. Na lista est√£o t√≥picos como pr√© treino n√£o supervisionado e universal, ML e NLP aplicados √† ci√™ncia, aumentando modelos pr√© treinados, Transformers eficientes e de longo alcance, entre outros.

![](https://cdn-images-1.medium.com/max/1600/0*8zoPc5OnYERIaaMP.png)


*‚ÄúVideoBERT (*[*Sun et al., 2019*](https://arxiv.org/abs/1904.01766)*), uma variante recente multimodal do BERT que gera ‚Äútokens‚Äù de v√≠deos dado uma receita (acima) e prediz os tokens futuros em escalas de tempo diferentes dado um token de v√≠deo(abaixo).‚Äù‚Ää‚Äî*‚Ää[*fonte*](https://arxiv.org/pdf/1904.01766.pdf)

A Google AI de pesquisa publicou um [resumo](https://ai.googleblog.com/2020/01/google-research-looking-back-at-2019.html) da pesquisa conduzida por eles durantes o ano, e as dire√ß√µes futuras de pesquisa que eles est√£o prestando aten√ß√£o.


# Educa√ß√£o em ML/NLP  üéì

***Democratizando educa√ß√£o em IA***
Num esfor√ßo de democratizar educa√ß√£o em IA e para educar as massas sobre as implica√ß√µes de tecnologia de IA, a Universidade de Helsinki fez uma parceria com a Reaktor para lan√ßar um curso brilhante (de gra√ßa) que cobre os fundamentos de IA. O [curso popular](https://www.elementsofai.com/) se chama ‚ÄúElementos de AI‚Äù e inclui t√≥picos tal como √©tica em IA, filosofia em IA, redes neurais, regra de Naive Bayes, dentre outros t√≥picos.

Stanford CS224N est√° de volta com outra [itera√ß√£o](http://web.stanford.edu/class/cs224n/) do popular curso de ‚ÄúNatural Language Processing with Deep Learning‚Äù. O curso oficialmente come√ßou em 7 de Janeiro dessa ano, ent√£o se voc√™ quiser acompanhar, vai para o site para o programa completo, slides, v√≠deos, sugest√µes de leitura de paper, etc.

***Top livros de NLP e ML***
Eu tweetei meus top livros de ML e NLP pr√°ticos e te√≥ricos, e foi bem recebido. Eu gostaria de compartilhar aquela lista aqui via tweet: https://twitter.com/omarsar0/status/1214547402838986754?s=20

***Machine Learning com M√©todos de Kernel***
M√©todos de Kernel tais como PCA e k-means est√£o por aqui h√° muito tempo, e isso √© porque eles t√™m sido aplicados com sucesso a uma variedade grande de aplica√ß√µes tal quais grafos e sequ√™ncias biol√≥gicas. Cheque esse conjunto compreens√≠vel de [slides](http://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/slides/master2017/master2017.pdf) cobrindo um alcance grade de m√©todos de kernel e suas aplica√ß√µes internas. Aqui est√° tamb√©m um √≥timo [blog](https://francisbach.com/cursed-kernels/) (mantido por Francis Bach) discutindo aspectos de m√©todos de kernel e outros t√≥picos de aprendizado de m√°quina.


# Men√ß√µes Honrosas ‚≠êÔ∏è

Aqui est√° uma lista de hist√≥rias dignas de nota e da sua aten√ß√£o:

- John Langford cuida desse incr√≠vel [blog](https://hunch.net/) que discute teoria de aprendizado de m√°quina
- Muitas das tecnologias da ind√∫stria orientadas a ML t√™m usado m√°quinas de Gradient Boosting por anos. Cheque esse [post](https://opendatascience.com/xgboost-enhancement-over-gradient-boosting-machines/?utm_campaign=Learning%20Posts&utm_content=111061559&utm_medium=social&utm_source=twitter&hss_channel=tw-3018841323) introduzindo uma das bibliotecas usadas para aplicar o gradient boosting chamado XGBoost.
- Se voc√™ est√° interessado em aprender como fazer o design e construir aplica√ß√µes de aprendizado de m√°quina e lev√°-las √† produ√ß√£o, Emmanual Ameisen cobre isso com esse [livro](https://www.amazon.com/Building-Machine-Learning-Powered-Applications/dp/149204511X/).
----------

*Se voc√™ tem uma hist√≥ria que gostaria de ser publicada na pr√≥xima edi√ß√£o da Newsletter de NLP, por favor mande um email para ellfae@gmail ou me mande uma mensagem via* [*Twitter*](https://twitter.com/omarsar0)*.*
