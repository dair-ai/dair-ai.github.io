---
layout: post
title: "Bolet√≠n informativo NLP #5 [ES]: GPT-2 Explicado, Entendiendo ‚ÄòSelf-Distillation‚Äô, Haiku, GANILLA, Sparkwiki, √âtica en el NLP, Torchmeta,..."
author: naraquev
modified:
comments: true
excerpt: ""
tags: [nlp_newsletter]
image:
  thumb: nlp_newsletter_5.png
---


![](https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png)

\\
Post Original en Ingles: [https://dair.ai/NLP_Newsletter_The_Annotated_GPT-2,_Understanding/](https://dair.ai/NLP_Newsletter_The_Annotated_GPT-2,_Understanding/)

\\
Primero que todo, no puedo agradecerles lo suficiente a todos por el increible animo y soporte para continuar con el bolet√≠n informativo de NLP. Este esfuerzo requiere una investigaci√≥n y edici√≥n tediosa, la cual encuentro gratificante y √∫til para proveer el mejor contenido. Espero que lo est√©n disfrutando tanto como yo.

\\
[Suscr√≠bete](https://dair.ai/newsletter/) al Bolet√≠n Informativo de NLP para recibir futuras ediciones en tu email. Este boletin lo desarrolla Elvis Saravia, editor de dair.ai

# Publicacionesüìô

***Un entendimiento te√≥rico del self-distillation***

\\
En el contexto del Deep Learning, self-distillation es el proceso de transferir conocimiento de una arquitectura a otra arquitectura id√©ntica. Las predicciones del modelo original son alimentadas como valores objetivo al otro modelo mientras se entrena. Adem√°s de tener propiedades deseables (como reducir el tama√±o del modelo) resultados emp√≠ricos demuestran que esta aproximaci√≥n trabaja bastante bien en sets de datos del tipo held-out. Un grupo de investigadores recientemente public√≥ una investigaci√≥n que provee un an√°lisis te√≥rico con foco en entender mejor qu√© est√° pasando en el proceso de destilaci√≥n del conocimiento y por que es efectivo. Los resultados muestran que unas pocas rondas de self-distillation amplifica la regularizaci√≥n ([limitando progresivamente el n√∫mero de funciones b√°sicas que representan la soluci√≥n](https://twitter.com/TheGradient/status/1228132843630387201?s=20)) lo que tiende a reducir el over-fitting. (Lee la investigaci√≥n completa [aqu√≠](https://arxiv.org/abs/2002.05715))

\\
![](https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png)

\\
*Traducci√≥n: Figura 1. Ilustraci√≥n esquem√°tica del proceso de self-distillation por dos iteraciones. Fuente: [https://arxiv.org/abs/2002.05715](https://arxiv.org/abs/2002.05715)*

\\
***Los 2010: La d√©cada del Deep Learning / Mirada al 2020***

\\
[J√ºrgen Schmidhuber,](http://people.idsia.ch/~juergen/) un pionero en la inteligencia artificial, public√≥ recientemente un [art√≠culo](http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html) titulado ‚ÄúFoco en proveer un resumen hist√≥rico del Deep Learning desde el 2010‚Äù. Algunos de los temas incluyen LSTMs, redes neuronales feedforward, GANs, DeepRL, Meta-Learning, World Models, distillings NNs, Attention Learning, etc. El art√≠culo concluye con una mirada al 2020, donde llama la atenci√≥n a temas como la privacidad y los mercados de datos.

\\
***Utilizando Redes Neuronales para resolver ecuaciones matem√°ticas avanzadas.***

\\
Investigadores de Facebook AI publicaron una [investigaci√≥n](https://arxiv.org/abs/1912.01412) que dice proponer un modelo entrenado en problemas matem√°ticos y sus soluciones para aprender a predecir posibles soluciones en problemas c√≥mo resolver integrales matematicas. El acercamiento est√° basado en un novedoso framework similar al usado en neural machine translation donde una expresi√≥n matem√°tica es representada como una especie de lenguaje y la soluci√≥n es tratada como su traducci√≥n. Por esto, en vez de que el resultado sea una traducci√≥n, es la soluci√≥n al problema matematico. Con esto, los investigadores concluyen que las redes neuronales profundas no solo son buenas para el razonamiento simb√≥lico sino tambi√©n para tareas m√°s diversas.

\\
![](https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png)

*Ecuaciones incluidas como entrada con su correspondiente soluci√≥n. [Fuente](https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/)*

# Creatividad y Sociedad üé®

***IA para el descubrimiento cient√≠fico***

\\
Mattew Hutson [reporta](https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times) c√≥mo la inteligencia artificial (IA) puede ser usada para producir emuladores con la capacidad de modelar fen√≥menos naturales complejos. Estos modelos pueden a su vez apuntar a diferentes tipos de descubrimientos cient√≠ficos. El desaf√≠o al construir estos emuladores es que necesitan una gran cantidad de datos y una b√∫squeda extensiva de par√°metros. Una [investigaci√≥n](https://arxiv.org/abs/2001.08055) reciente propone una t√©cnica llamada DENSE, basada en [neural architecture search](https://en.wikipedia.org/wiki/Neural_architecture_search) para construir emuladores precisos utilizando una cantidad de datos de entrenamiento limitada. Los investigadores hicieron distintas pruebas construyendo simuladores para caso como astrof√≠sica, ciencia clim√°tica, energ√≠a de fusi√≥n, etc.

\\
***Mejorando la traducci√≥n imagen-a-ilustraci√≥n***

\\
GANILLA es una aproximaci√≥n que propone el uso de las GANs para mejorar la transferencia de estilo y contenido en la tarea de convertir una imagen en una ilustraci√≥n. En particular, un modelo para [imagen-a-ilustraci√≥n](https://paperswithcode.com/task/image-to-image-translation) es propuesto (con una red generadora mejorada) y evaluado basado en un nuevo framework para evaluaci√≥n cuantitativa que considera tanto el contenido como el estilo de la imagen generada. La novedad de esta investigaci√≥n es en la red generadora propuesta que considera un balance entre estilo y contenido que anteriores intentos fallaron en alcanzar. El c√≥digo y modelo pre entrenado est√°n disponibles en [l√≠nea](https://github.com/giddyyupp/ganilla). [Aqu√≠](https://arxiv.org/abs/2002.05638) se puede encontrar la investigaci√≥n completa.

\\
![](https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png)

*Traducci√≥n: Ejemplos de salida de CycleGAN, DualGAN, y nuestro m√©todo (GANILLA) utilizando diferentes estilos. CycleGAN y GANILLA generan im√°genes en un estilo de ilustraci√≥n, pero DualGAN falla en transferir el estilo. Sin embargo, CycleGAN falle en preservar el contenido de la imagen original, por ejemplo, aleatoriamente coloca caras de animales en el aire. Nuestro m√©todo preserva contenido as√≠ como tambi√©n transfiere el estilo de la imagen original.*

\\
***Andrew Ng habla sobre su inter√©s en self-supervised learning***

\\
Andrew Ng, fundador de [deeplearning.ai](http://deeplearning.ai/), participa en el podcast Artificial Inteligence de Lex Friedman para [hablar](https://www.youtube.com/watch?v=0jspaMLxBig) de diferentes temas como sus comienzos en el ML, el futuro del AI y la educaci√≥n, recomendaciones de uso para ML, sus metas personales y a cu√°les t√©cnicas de ML prestar atenci√≥n en el 2020.

\\
Andrew tambien explico por que est√° muy emocionado sobre el self-supervised representation learning. [Self-supervised learning](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html) se trata sobre enfocar un problema de aprendizaje que intenta obtener la supervisi√≥n de los datos en s√≠ mismos para hacer uso de grandes cantidades de datos no clasificados, escenario m√°s com√∫n que datos limpios y clasificados. Las representaciones aprendidas, opuesto al rendimiento de la tarea, son importantes y pueden ser usadas en tareas m√°s adelante, similar a lo que est√° siendo usado en modelos de lenguaje natural como [BERT](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert).

\\
Hay mucho inter√©s en usar el self-supervised learning para aprender representaciones visuales generales que hagan al modelo m√°s preciso en situaciones de bajos recursos. Por ejemplo, un nuevo m√©todo llamado [SimCLR](https://arxiv.org/abs/2002.05709) (Dirigido por Geoffrey Hinton) propone un framework para el *constrastive self-supervised learning* de representaciones visuales para mejorar el resultado de la clasificaci√≥n de im√°genes en diferentes contextos como el *transfer learning* y el *semi-supervised learning*.

\\
![](https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png)

*Traducci√≥n: ImageNet top-1 accuracy de clasificadores lineales entrenados en representaciones aprendidas con diferentes m√©todos de auto-aprendizaje (pre entrenados en ImageNet). La cruz gris indica un modelo ResNet-50 supervisado. Nuestro m√©todo, SimCLR, es mostrado en negrita.*

# Herramientas y Set de Datos ‚öôÔ∏è

***Librer√≠as JAX***

\\
[JAX](https://github.com/google/jax) es una nueva librer√≠a que combina NumPy y diferenciaci√≥n autom√°tica para realizar investigaci√≥n de alto rendimiento en ML. Para simplificar procesos para construir redes neuronales usando JAX, DeepMind lanz√≥ [Haiku](https://github.com/deepmind/dm-haiku) y [RLax](https://github.com/deepmind/rlax). RLAx simplifica la implementaci√≥n de agentes de reinforcement learning y Haiku simplifica la construcci√≥n de redes neuronales utilizando paradigmas familiares de programaci√≥n orientada a objetos.

\\
***Una herramienta para procesar datos de Wikipedia***

\\
[Spakwiki](https://github.com/epfl-lts2/sparkwiki) es una herramienta para procesar datos de Wikipedia. Este lanzamiento es parte de muchos esfuerzos para permitir an√°lisis interesantes de comportamiento como [capturar tendencias y sesgos de lenguaje en ediciones de distinto idioma en Wikipedia](https://arxiv.org/abs/2002.06885). Los autores descubrieron que independientemente del lenguaje, el comportamiento de b√∫squeda de los usuarios de Wikipedia es muy parecido en categor√≠as como pel√≠culas, m√∫sica y deportes pero que las diferencias se vuelven m√°s aparentes con eventos locales y particularidades de cultura.

\\
![](https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg)

\\
***Tokenizadores, caso DistilBERT y modelos***

\\
Una [nueva entrega de Transformers](https://github.com/huggingface/transformers/releases/tag/v2.5.0) de Hugging Face ahora incluye la integraci√≥n de su r√°pida librer√≠a de tokenizaci√≥n que intenta mejorar el tiempo de ejecuci√≥n de modelos como BERT, RoBERTa, GPT2, y otro modelos construidos por la comunidad.

# √âtica en la Inteligencia Artificial üö®

***Consideraciones √©ticas para modelos de NLP y ML.***

\\
En un nuevo [episodio](https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender) de [NLP Highlights](https://soundcloud.com/nlp-highlights), Emily Bender habla sobre algunas consideraciones √©ticas cuando se desarrollan modelos de Procesamiento de Lenguaje Natural y tecnolog√≠as tanto en el contexto de la academia como de industria. Algunos de los t√≥picos discutidos incluyen consideraciones √©ticas cuando se dise√±an tareas, recolecci√≥n de datos, y eventualmente publicaci√≥n de resultados.

\\
Adicionalmente a todas las consideraciones mencionadas anteriormente, una preocupaci√≥n que siempre se discute en la comunidad de AI es enfocarse demasiado en optimizar una m√©trica, lo que va en contra de los cimientos de lo que el campo trata de alcanzar. Rachel Thomas y David Uminsky discuten c√≥mo esto puede salir mal a trav√©s de un [an√°lisis](https://arxiv.org/abs/2002.08512) con diferentes casos de uso. Adem√°s, proponen un framework simple para mitigar el problema que involucra el uso y combinaci√≥n de m√∫ltiples m√©tricas, seguido del involucramiento directo de los usuarios afectados por la tecnolog√≠a que se est√° desarrollando.

# Art√≠culos y Blogs ‚úçÔ∏è

***GPT-2 Explicado***

\\
Aman Arora recientemente public√≥ un art√≠culo excepcional titulado ‚Äú[**El GPT-2 Explicado**](https://amaarora.github.io/2020/02/18/annotatedGPT2.html)‚Äù explicando el funcionamiento interno del modelo basado en la t√©cnica del Transformer llamado GPT-2. Su aproximaci√≥n fue inspirada por el art√≠culo [El Transformer Explicado](https://nlp.seas.harvard.edu/2018/04/03/attention.html) que tom√≥ una aproximaci√≥n a explicar las partes m√°s importantes del modelo siguiendo ejemplos f√°ciles de seguir y c√≥digo comentado. Amant realiz√≥ un gran esfuerzo para re implementar GPT-2 de OpenAI utilizando PyTorch y la librer√≠a de Transformers de Hugging Face. Es un trabajo brillante.

\\
![](https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png)

*[Fuente](https://amaarora.github.io/2020/02/18/annotatedGPT2.html)*

\\
***M√°s all√° de BERT?***

\\
[Opini√≥n](https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1) interesante de Sergi Castella en que hay m√°s all√° de BERT. El tema principal incluye mejorar m√©tricas, como la librer√≠a Transformer de Hugging Face empodera la investigaci√≥n, set de datos interesantes para revisar, utilizaci√≥n de modelos, etc.

\\
***Operador para comprimir Matrices***

\\
El blog the TensorFlow publico un [articulo](https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016) donde explican las t√©cnicas para compresi√≥n de matrices y su importancia en un modelo de red neuronal. La compresi√≥n de matrices puede ayudar a construir modelos m√°s eficientes y reducidos que pueden ser incorporados en dispositivos m√°s peque√±os como tel√©fonos y asistentes del hogar. Enfocarse en la comprensi√≥n por m√©todos como low-rank-approximation y quantization significa que no debemos comprometer la calidad del modelo

\\
![](https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png)

*[Fuente](https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016)*

# Educaci√≥nüéì

***Fundamentos del NLP***

\\
Estoy emocionado de liberar un borrador del cap√≠tulo 1 de mi nueva serie llamado [Fundamentos del NLP](https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684). Ense√±a conceptos comenzando desde lo m√°s b√°sico, compartiendo buenas pr√°cticas, referencias importantes, errores comunes a evitar, y cu√°l es el futuro del NLP. Un [Colab Notebook](https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ) est√° incluido y el proyecto ser√° mantenido [aqu√≠](https://github.com/dair-ai/nlp_fundamentals)

\\
![](https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif)

\\
***[En l√≠nea] Revisi√≥n/Discusi√≥n: Parte 1 Sesi√≥n de lectura de Fundamentos Matem√°ticos***

\\
Machine Learning Tokio est√° organizando una discusi√≥n en l√≠nea para revisar los cap√≠tulos que fueron cubiertos en su m√°s reciente sesi√≥n de estudio. El grupo ha estudiado previamente cap√≠tulos del libro [Matem√°ticas para el Machine Learning](https://mml-book.github.io/) escrito por Marc Peter Deisenroth, A Aldo Faisal, y Cheng Soon Ong. El [evento](https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/) est√° pautado para el 8 de Marzo de 2020.

\\
***Libros Recomendados***

\\
En un segmento previo discutimos la importancia de la compresi√≥n de matrices para construir modelos de ML compactos. Si est√°s interesado en aprender m√°s sobre como construir redes neuronales m√°s peque√±as para sistemas embebidos te recomiendo este gran libro llamado [TinyML](https://tinymlbook.com/?linkId=82595412) por Pete Warden y Daniel Situnayake.

\\
![](https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg)

*[Fuente](https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043)*

\\
Otro libro interesante a tener en cuenta es ‚Äú[Deep Learning para programadores con fastai y pyTorch: Aplicaciones AI sin un PhD](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527)‚Äù por Jeremy Howard y Sylvain Gugger. El libro trata de proveer la matem√°tica necesaria para construir y entrenar modelos que resuelvan tareas en el √°rea de visi√≥n por computador y Entendimiento natural de texto.

\\
![](https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg)

*[Fuente](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527)*

# Menciones a considerar ‚≠êÔ∏è

Puedes acceder a las versiones anteriores de este bolet√≠n [aqu√≠](https://medium.com/dair-ai/nlp-newsletter-pytorch3d-deepspeed-turing-nlg-question-answering-benchmarks-hydra-sparse-322f018ee096).

\\
[Torchmeta](https://arxiv.org/abs/1909.06576) es una librer√≠a que permite el uso de data loaders para la investigaci√≥n en meta-learning. El creador es Tristan Deleu.

\\
Manuel Tonneau escribi√≥ una [pieza](https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter) ofreciendo una mirada m√°s detallada a alguna de la maquinaria involucrada en el modelamiento del lenguaje. Algunos t√≥picos incluyen b√∫squedas greedy y beam, as√≠ como tambi√©n nucleus sampling.

\\
El MIT [liber√≥](http://introtodeeplearning.com/) el programa de estudio completo del curso titulado ‚ÄúIntroducci√≥n al Deep Learning‚Äù, incluye videos de las clases que ya se dictaron. Est√°n apuntando a liberar videos y presentaciones todas las semanas.

\\
Aprende a entrenar un modelo para reconocimiento de entidades nombradas (NER) utilizando una aproximaci√≥n v√≠a Transformers en menos de [300 l√≠neas de c√≥digo](https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py). Puedes encontrar el Google Colab que lo acompa√±a [aqu√≠](https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn).
