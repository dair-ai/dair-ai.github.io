---
layout: post
title: "NLP Newsletter #2 [PT-BR]: Reformer, DeepMath, ELECTRA, TinyBERT para busca, VizSeq, Open-Sourcing ML,‚Ä¶"
author: fclesio
modified:
comments: true
excerpt: "Esta segunda newsletter aborda topics que v√£o de interpretabilidade de modelos para enovelamento de prote√≠nas (protein folding) at√© active transfer learning"
tags: [nlp_newsletter]
image:
  thumb: nlp_newsletter_2.png
---

![](https://cdn-images-1.medium.com/max/1200/1*mgWc3FhHPRfCxdPir6wSeg.png)

\\
Bem vindo novamente √† NLP Newsletter! üëã Esta segunda newsletter aborda topicos que v√£o de interpretabilidade de modelos para enovelamento de prote√≠nas (protein folding) at√© active transfer learning. *Voc√™ pode encontrar a vers√£o Markdown desta edi√ß√£o no final.*


# Publica√ß√µes üìô

***Confiando na incerteza dos modelos***

\\
Um artigo recente do Google AI, publicado na NeurIPS, analisa se as probabilidades de um modelo refletem a sua capacidade de prever dados fora de distribui√ß√£o ou mudan√ßa no conjunto de dados (shifted data).

\\
Ensembles profundos (Deep ensembles) tiveram um melhor desempenho (_i.e._, melhoraram a incerteza do modelo) no dataset com mudan√ßa no conjunto de dados (data shift), enquanto outros modelos n√£o tornaram-se mais incertos com o mesmo data shift, mas ao contrario transformaram-se confidentemente errados(Leia o paper [aqui](https://arxiv.org/abs/1906.02530) e o sum√°rio [aqui](https://ai.googleblog.com/2020/01/can-you-trust-your-models-uncertainty.html).).

\\
![](https://cdn-images-1.medium.com/max/800/0*NrsUnHS1thKq3ChK.png)

*Corrup√ß√£o da imagem‚Ää‚Äî*‚Ää[*fonte*](https://ai.googleblog.com/2020/01/can-you-trust-your-models-uncertainty.html)

\\
***Generaliza√ß√£o Sistem√°tica***

\\
Um [trabalho interessante](https://www.semanticscholar.org/paper/Systematic-Generalization%3A-What-Is-Required-and-Can-Bahdanau-Murty/6c7494a47cc5421a7b636c244e13586dc2dab007) publicado na ICLR apresenta uma compara√ß√£o entre modelos modulares e modelos gen√©ricos e as suas respectivas efetividades para *generaliza√ß√£o sistematica* em entendimento de linguagem (language understanding). Com base na avalia√ß√£o do _reasoning_ realizada em uma tarefa em que se [respondia uma pergunta visual](https://arxiv.org/abs/1909.01860), os autores concluem que pode haver a necessidade de regularizadores e priorizadores expl√≠citos para se conseguir uma generaliza√ß√£o sistem√°tica.


\\
***Um modelo eficiente baseado em Transformer chamado Reformer***

\\
√â bem conhecido que um modelo Transformer √© bastante limitado em rela√ß√£o √† janela de contexto que pode ser coberta, devido aos c√°lculos de algo custo computacional realizados na camada de [Attention](https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/). Assim, s√≥ pode ser poss√≠vel aplicar o modelo Transformer a tamanhos de texto limitados ou gerar frases curtas e/ou pe√ßas musicais. O GoogleAI publicou recentemente uma variante eficiente de um modelo Transformer chamado [Reformer](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html). O foco principal deste m√©todo √© ser capaz de lidar com janelas de contexto muito mais altas e, ao mesmo tempo, reduzir os requisitos computacionais com a melhoria da efici√™ncia do uso de mem√≥ria. O Reformer usa o "locality-sensitive-hashing" ([LSH](https://en.wikipedia.org/wiki/Locality-sensitive_hashing)) para agrupar vetores similares e criar segmentos a partir deles, o que permite o processamento paralelo. A camada de Attention √© ent√£o aplicada a estes segmentos menores e em partes vizinhas correspondentes - isto √©, o que reduz a carga computacional. A efici√™ncia de mem√≥ria √© obtida usando camadas revers√≠veis que permitem que as informa√ß√µes de entrada de cada camada sejam recalculadas sob demanda durante o treinamento via backpropagation. Esta √© uma t√©cnica simples que evita que o modelo tenha a necessidade de armazenar as ativa√ß√µes em mem√≥ria. Confira este [notebook no Colab](https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb) para ver como um modelo Reformer pode ser aplicado a uma tarefa de gera√ß√£o de imagens.

\\
![](https://cdn-images-1.medium.com/max/800/0*Q6FHJ5bqZRCrBAp9.png)

*"Locality-sensitive-hashing: O Reformer assume uma sequ√™ncia de palavras de entrada, onde cada palavra √© na verdade um vetor representando palavras individuais (ou pixels, no caso das imagens) na primeira camada e contextos maiores nas camadas subsequentes. O LSH √© aplicado √† sequ√™ncia, depois que as palavras s√£o ordenadas pelo seu hash e particionadas. O Attention √© aplicado apenas dentro de um √∫nico peda√ßo e dos seus vizinhos imediatos".‚Ää- [fonte](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html)*

\\
 ***Adapta√ß√£o de Dominio de forma n√£o-supervisionada para Classifica√ß√£o de Textos***

\\
Este [trabalho](https://arxiv.org/abs/2001.04362) prop√µe uma combina√ß√£o de medidas de dist√¢ncia incorporadas em uma fun√ß√£o de perda adicional, para treinar um modelo e melhorar a adapta√ß√£o n√£o-supervisionada do dom√≠nio. O modelo √© estendido a um DistanceNet Bandit que otimiza os resultados para "transfer√™ncia para o dom√≠nio alvo de poucos recursos". O principal problema abordado com este m√©todo √© como lidar com a disparidade entre os dados de diferentes dom√≠nios, especificamente no que diz respeito √† tarefas de NLP, como a an√°lise de sentimentos (sentiment analysis).

\\
***Melhoria de Representa√ß√µes Contextualizadas***

\\
Este [artigo](https://openreview.net/forum?id=r1xMH1BtvB) prop√µe uma tarefa de pr√©-treino mais eficiente em termos de amostragem chamada *detec√ß√£o de token*. Esta tarefa pode ser utilizada para treinar um modelo lingu√≠stico que √© mais eficiente do que m√©todos de pr√©-treino com modelagem de linguagem mascarada, como o BERT. O modelo √© chamado ELECTRA e suas representa√ß√µes contextualizadas superam as do BERT e XLNET com os mesmos dados e tamanho de modelo. O m√©todo funciona particularmente bem no regime de baixa computa√ß√£o. Este √© um esfor√ßo para construir modelos de linguagem menores e mais baratos.

\\
***Interpretabilidade de Modelos***

\\
A publica√ß√£o mais recente da Distill intitulada "[Visualizing the Impact of Feature Attribution Baselines](https://distill.pub/2020/attribution-baselines/)" discute os [gradientes integrados](https://medium.com/@kartikeyabhardwaj98/integrated-gradients-for-deep-neural-networks-c114e3968eae) que s√£o usados para interpretar redes neurais em v√°rios problemas, identificando quais recursos s√£o relevantes para prever um determinado ponto. O problema √© definir e preservar corretamente uma no√ß√£o de *falta* que √© o que se pretende com a entrada de base dos gradientes integrados. O desafio aqui, no contexto da interpretabilidade de modelos, √© que o m√©todo deve assegurar que o modelo n√£o destaque as caracter√≠sticas em falta como importantes, evitando ao mesmo tempo dar zero import√¢ncia √†s entradas de _baseline_, o que pode facilmente acontecer. O autor prop√µe avaliar quantitativamente os diferentes efeitos de algumas escolhas de _baseline_ previamente utilizadas e propostas que melhor preservem a no√ß√£o de falta.


# Criatividade e Sociedade üé®

***Incompatibilidade de sentimentos***

\\
Este [estudo longitudinal](https://ieeexplore.ieee.org/abstract/document/8952437) descobre que a emo√ß√£o extra√≠da atrav√©s do uso de algoritmos baseados em texto, muitas vezes n√£o √© a mesma que as emo√ß√µes auto-relatadas.

\\
***Compreens√£o da dopamina e o enovelamento de prote√≠nas (protein folding)***

\\
A DeepMind lan√ßou recentemente **dois** artigos interessantes na revista Nature. O primeiro [artigo](https://deepmind.com/blog/article/Dopamine-and-temporal-difference-learning-A-fruitful-relationship-between-neuroscience-and-AI) visa entender melhor como funciona a dopamina no c√©rebro usando a aprendizagem por refor√ßo. O segundo [paper](https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery) est√° mais relacionado com [enovelamento de prote√≠nas](https://en.wikipedia.org/wiki/Protein_folding) e tenta compreend√™-lo melhor para ser capaz de potencialmente descobrir tratamentos para uma ampla gama de doen√ßas. Estes s√£o grandes exemplos de como sistemas de IA poderiam potencialmente ser aplicados em aplica√ß√µes do mundo real para ajudar a sociedade.

\\
![](https://cdn-images-1.medium.com/max/800/1*0mfEtacqGLSrmaUlNjJa0g.png)

*‚ÄúFormas 3D complexas emergem de um conjunto de amino√°cidos.‚Äù‚Ää‚Äî*‚Ää[*fonte*](https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery)

\\
***Entrevistas sobre o ML na sociedade***

\\
Em uma [entrevista](https://www.youtube.com/watch?v=I-EIVlHvHRM&feature=youtu.be) com a Wired, Refik Anadol discute o potencial dos algoritmos de aprendizagem de m√°quina para cria√ß√£o de arte. Este √© um excelente exemplo de como Machine Learning pode ser usado de forma a criativa.

\\
Um dos setores em que a IA pode ter um grande impacto √© na educa√ß√£o. Em um novo [epis√≥dio](https://engineering.stanford.edu/magazine/article/emma-brunskill-amped-education-ai?sf115875862=1), que faz parte de "_The Future of Everything_", Russ Altman e Emma Brunskill t√™m uma discuss√£o profunda sobre a aprendizagem assistida por computador.

# Ferramentas e Datasets ‚öôÔ∏è

***Modelos PyTorch em produ√ß√£o***

\\
O Cortex √© uma ferramenta para automatizar a infra-estrutura e implementar modelos PyTorch como APIs em produ√ß√£o com AWS. Saiba mais sobre como isso √© feito [aqui](https://medium.com/pytorch/how-to-build-production-software-with-pytorch-9a8725382f2a).

\\
***Visualizando Sequ√™ncias de Gera√ß√£o de Texto***

\\
O time do Facebook AI lan√ßou o [VizSeq](https://ai.facebook.com/blog/vizseq-a-visual-analysis-toolkit-for-accelerating-text-generation-research/), que √© uma ferramenta que auxilia na avalia√ß√£o visual de seq√º√™ncias de gera√ß√£o de texto sob m√©tricas como BLUE e METEOR. O principal objetivo desta ferramenta √© fornecer uma an√°lise mais intuitiva dos conjuntos de dados de texto, alavancando visualiza√ß√µes e tornando-as mais escal√°veis e produtivas para todos os pesquisadores. Leia o artigo completo [aqui](https://www.aclweb.org/anthology/D19-3043.pdf).


\\
![](https://cdn-images-1.medium.com/max/800/1*Ff7BTxmEjUXHtYu9JkfClg.jpeg)

[*Fonte*](https://ai.facebook.com/blog/vizseq-a-visual-analysis-toolkit-for-accelerating-text-generation-research/)

\\
***Estado da arte em reconhecimento de fala online***

\\
O FacebookAI tornou open-source o [wav2letter@anywhere](https://ai.facebook.com/blog/online-speech-recognition-with-wav2letteranywhere/) que √© um framework de infer√™ncia que √© baseado em um modelo ac√∫stico que usa o Transformer como base para reconhecimento de fala online de √∫ltima gera√ß√£o. Grandes melhorias est√£o em torno do tamanho do modelo e reduzindo a lat√™ncia entre o √°udio e a transcri√ß√£o, o que √© importante para seja obtida uma infer√™ncia mais r√°pida em tempo real.

\\
![](https://cdn-images-1.medium.com/max/800/1*4_2Obuu8u8l2Vtp8UMHe7Q.gif)

*Processamento de fala‚Ää‚Äî*‚Ää[*fonte*](https://ai.facebook.com/blog/online-speech-recognition-with-wav2letteranywhere/)

# √âtica em IA üö®

***Implica√ß√µes de IA***

\\
Em um esfor√ßo para evitar abusos e a√ß√µes anti√©ticas dos sistemas de IA sobre o p√∫blico, a Uni√£o Europ√©ia est√° considerando proibir a tecnologia de reconhecimento facial do p√∫blico por cinco anos. ([Hist√≥ria completa](https://www.reuters.com/article/us-eu-ai/eu-mulls-five-year-ban-on-facial-recognition-tech-in-public-areas-idUSKBN1ZF2QL))

\\
***Os custos ambientais de NLP***

\\
Talvez negligenciado na maioria das vezes, este [documento](https://arxiv.org/abs/1906.02243) discute as considera√ß√µes energ√©ticas e pol√≠ticas para abordagens modernas de deep learning em NLP. √â amplamente conhecido que os modelos atuais dependem de bilh√µes de par√¢metros e que, por sua vez, dependem de grandes recursos computacionais que demandam um consumo substancial de energia. Os autores esperam espalhar mais consci√™ncia sobre os custos ambientais envolvidos no treinamento desses modernos modelos de NLP.

\\
Zachary Lipton discute equidade(fairness), interpretabilidade e os perigos do solucionismo em Machine Learning nesta [palestra](https://c4ejournal.net/2020/01/16/zack-lipton-fairness-interpretability-and-the-dangers-of-solutionism-ethics-of-ai-in-context2020-c4ej-2/) proferida na Universidade de Toronto. Os principais t√≥picos giraram em torno de considera√ß√µes cuidadosas e implica√ß√µes das abordagens de equidade em ML.


# Artigos e posts de blogs ‚úçÔ∏è

***Open-Sourcing ML***

\\
Thomas Wolf, l√≠der cient√≠fico da Hugging Face, compartilha excelentes conselhos para aqueles que planejam fazer c√≥digo ML open-source ou pesquisa. Encontre o t√≥pico do Twitter [aqui](https://twitter.com/Thom_Wolf/status/1216990543533821952?s=20).

\\
***Introdu√ß√£o para aprendizagem auto-supervisionada para computer vision****

\\
Jeremy Howard escreveu este grande [blog post](https://www.fast.ai/2020/01/13/self_supervised/) introduzindo brevemente o conceito de aprendizagem auto-supervisionada no contexto de computer vision. Eu adoro estes pequenos resumos, pois ajudam a dar uma introdu√ß√£o confi√°vel no caso de voc√™ estar interessado em aplicar t√©cnicas deste dom√≠nio ao seu pr√≥prio problema.

\\
***TinyBERT para problemas de busca***

\\
J√° vimos o sucesso de muitas variantes de modelos BERT (por exemplo, [DistilBERT](https://medium.com/huggingface/distilbert-8cf3380435b5)) que utilizam alguma forma de [destila√ß√£o do conhecimento](https://nervanasystems.github.io/distiller/knowledge_distillation.html) para diminuir substancialmente o tamanho do modelo e melhorar a velocidade. Algumas pessoas usaram uma variante do BERT chamada, [TinyBERT](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT), e aplicaram-na a uma [solu√ß√£o de busca baseada em palavras-chave](https://towardsdatascience.com/tinybert-for-search-10x-faster-and-20x-smaller-than-bert-74cd1b6b5aec). Este projeto foi inspirado por esta [solu√ß√£o de busca](https://www.blog.google/products/search/search-language-understanding-bert/) para compreender as buscas propostas pelo Google. A maior parte da arquitetura que ela funciona √© em uma CPU padr√£o e pode ser usada para melhorar e entender os resultados das buscas.

\\
***Active Transfer Learning***

\\
Rober Monarch escreveu este excelente [blog post](https://medium.com/pytorch/https-medium-com-robert-munro-active-learning-with-pytorch-2f3ee8ebec) sobre _Active Transfer Learning_ que faz parte de seu pr√≥ximo livro chamado [Human-in-the-loop Machine Learning](https://www.manning.com/books/human-in-the-loop-machine-learning). Ele est√° escrevendo √≥timos posts em seu blog sobre m√©todos para combinar intelig√™ncia humana e m√°quinas para resolu√ß√£o de problemas. Ele tamb√©m fornece implementa√ß√µes em PyTorch dos m√©todos discutidos.

\\
***Revelando os segredos ocultos do BERT***

\\
Anna Roger escreveu este divertido e interessante [blog post](https://text-machine-lab.github.io/blog/2020/bert-secrets/) que fala sobre o que realmente acontece com um BERT otimizado, e se os alegados pontos fortes s√£o usados para abordar tarefas posteriores, tais como an√°lise de sentimentos, vincula√ß√£o textual e infer√™ncia da linguagem natural, entre outras. Os resultados das an√°lises propostas sugerem que o BERT est√° excessivamente superparametrizado (overparameterized) e que os benef√≠cios identificados do componente de _self-attention_ da estrutura podem n√£o ser necessariamente ben√©ficos como se imagina. Em particular no que diz respeito √† informa√ß√£o lingu√≠stica que est√° sendo codificada e utilizada para a parte de infer√™ncia.


# Educa√ß√£o üéì

***Redes Neurais para NLP***

\\
Graham Neubig, professor de PNL na CMU, tem [lan√ßado alguns v√≠deos](https://www.youtube.com/playlist?list=PL8PYTP1V4I8CJ7nMxMC8aXv8WqKYwj-aJ) para a aula "Redes Neurais para NLP" que est√° sendo ministrada neste semestre. Eu recomendo altamente esta playlist para aqueles interessados em aprender sobre os m√©todos modernos de NLP.

\\
***Deep Learning Math (DeepMath)***

\\
Quer mergulhar profundamente na matem√°tica por tr√°s dos m√©todos de deep learning? Aqui est√° uma [s√©rie de v√≠deo-palestras](https://www.youtube.com/playlist?list=PLWQvhvMdDChzsThHFe4lYAff3pu2m0v2H) com uma vasta gama de palestrantes.

\\
***Cursos de Python e Tutoriais***

\\
Python tornou-se uma das linguagens de programa√ß√£o mais requisitadas n√£o s√≥ na ind√∫stria de TI, mas tamb√©m no espa√ßo da ci√™ncia dos dados. Em um esfor√ßo para qualificar alunos de todo o mundo com conhecimentos pr√°ticos de Python, o Google lan√ßou o "Google IT Automation with Python Professional Certificate". Saiba mais sobre o lan√ßamento [aqui](https://blog.google/outreach-initiatives/grow-with-google/new-certificate-help-people-grow-careers) e veja o curso [aqui](https://www.coursera.org/professional-certificates/google-it-automation). Embora o curso n√£o esteja diretamente relacionado ao ML ou √† IA, √© definitivamente um bom curso de fundamentos para deseja tornar-se proficiente com a linguagem Python. Bolsas de estudo tamb√©m est√£o dispon√≠veis.

\\
Aqui est√° outra [s√©rie de v√≠deos](https://www.youtube.com/watch?v=fMqL5vckiU0&list=PL-wATfeyAMNrtbkCNsLcpoAyBBRJZVlnf) promissora chamada "Deep Learning (for Audio) with Python" com foco no uso de Tensorflow e Python para constru√ß√£o de aplica√ß√µes relacionadas a √°udio/m√∫sica, e alavancando o uso de deep learning.

\\
![](https://cdn-images-1.medium.com/max/800/1*N5d8-1La8khZ6-XwHL68sg.png)

[*fonte*](https://www.youtube.com/watch?v=fMqL5vckiU0&list=PL-wATfeyAMNrtbkCNsLcpoAyBBRJZVlnf)

\\
Andrew Trask lan√ßou um conjunto de [notebooks com tutoriais passo-a-passo](https://c4ejournal.net/2020/01/16/zack-lipton-fairness-interpretability-and-the-dangers-of-solutionism-ethics-of-ai-in-context2020-c4ej-2/) para alcan√ßar deep learning de forma descentralizada com objetivo de preserva√ß√£o da privacidade. Todos os notebooks cont√™m implementa√ß√µes PyTorch e s√£o destinados a iniciantes.

\\
***Estado da arte em Deep Learning***

\\
Confira esta [palestra em v√≠deo](https://www.youtube.com/watch?v=0VH1Lim8gL8) de Lex Fridman sobre a recente pesquisa e desenvolvimento em Deep Learning. Ele fala sobre os principais avan√ßos em t√≥picos como perceptrons, redes neurais, backpropagation, CNN, deep learning, ImageNet, GANs, AlphaGo, e Transformers. Esta palestra faz parte da S√©rie de Deep Learning do MIT.

\\
***Online learning e pesquisa***

\\
H√° muitas e grandes iniciativas online para colabora√ß√£o tanto em pesquisa quanto em aprendizagem. Os meus favoritos s√£o a sess√£o de leitura de matem√°tica [MLT's](https://twitter.com/__MLT__) e este novo esfor√ßo de colabora√ß√£o distribu√≠da em pesquisa AI iniciado por [nightai](https://www.nightai.co/). Recentemente, tem havido muitos grupos de estudo como este online e estes grupos de estudos s√£o √≥timas maneiras de mergulhar no mundo do ML.

\\
***Perspectivas em aprendizagem por refor√ßo***

\\
Aprenda com a Dra. Katja Hofmann os principais conceitos e m√©todos de aprendizagem por refor√ßo nesta [s√©rie de webinars](https://note.microsoft.com/MSR-Webinar-RL-Algorithm-to-Adoption-Registration-Live.html?wt.mc_id=twitter_MSR-WBNR_post_v3).

# Men√ß√µes honrosas ‚≠êÔ∏è
Confira [esta implementa√ß√£o limpa e auto-contida em PyTorch](https://gist.github.com/y0ast/d91d09565462125a1eb75acc65da1469) da ResNet-18 aplicada ao CIFAR-10, que atinge ~94% de precis√£o.

\\
PyTorch 1.4 √© lan√ßado! Confira as notas de lan√ßamento [aqui](https://github.com/pytorch/pytorch/releases/tag/v1.4.0).

\\
Elona Shatri escreveu este excelente [resumo](_COPY11@e.shatri1/what-is-optical-music-recognition-6515d8a53e01) sobre como ela pretende abordar o reconhecimento da m√∫sica √≥ptica usando deep learning.

\\
O t√≠tulo para este post no blog √© auto-explicativo: ["The Case for Bayesian Deep Learning"](https://cims.nyu.edu/~andrewgw/caseforbdl/)".

\\
Chris Said compartilha sua [experi√™ncia](https://chris-said.io/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/) na otimiza√ß√£o de tamanhos de amostras para testes A/B, uma parte importante em se tratando de Data Science de forma aplicada. Os t√≥picos incluem os custos e benef√≠cios de grandes tamanhos de amostras e melhores pr√°ticas para os profissionais.

\\
Neural Data Server (NDS) is a dedicated search engine for obtaining transfer learning data. Read about the method [here](https://arxiv.org/abs/2001.02799) and the service [here](http://aidemos.cs.toronto.edu/nds/).

O Neural Data Server (NDS) √© um mecanismo de busca dedicado √† obten√ß√£o de dados via _transfer learning_. Leia sobre o m√©todo [aqui](https://arxiv.org/abs/2001.02799) e o servi√ßo [aqui](http://aidemos.cs.toronto.edu/nds/).
