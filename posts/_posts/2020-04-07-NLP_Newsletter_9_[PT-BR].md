---
layout: post
title: "NLP Newsletter #9 [PT-BR]: Guia Ilustrado de GNN, TextVQA e TextCaps, KeraStroke, SyferText, torchlayers,‚Ä¶"
author: haneybarg
excerpt: "A edi√ß√£o inclui t√≥picos que variam de uma ferramenta de NLP que preserva a privacidade a ferramentas interativas para pesquisar artigos relacionados ao COVID-19 at√© um guia ilustrado para representar graficamente redes neurais."
modified:
comments: true
tags: [nlp_newsletter]
image:
  thumb: nlp_newsletter_9.png
---


![](https://cdn-images-1.medium.com/max/1200/1*Vq-bFSTjqYjDGAR4Ja1abw.png)

\\
Bem-vindo √† 9¬™ edi√ß√£o da *NLP Newsletter*. Esperamos que voc√™ e seus entes queridos
estejam bem e se mantenham seguros. Esta edi√ß√£o inclui t√≥picos que variam de uma
ferramenta de *NLP* que preserva a privacidade a ferramentas interativas para pesquisar
artigos relacionados ao COVID-19 at√© um guia ilustrado para representar graficamente redes
neurais.


# Pesquisas e Publica√ß√µes üìô

***Neuroevolution of Self-Interpretable Agents***

\\
Tang et al. (2020) apresentam um [trabalho](https://attentionagent.github.io/)
interessante e criativo que visa desenvolver um agente para absorver uma fra√ß√£o de sua
entrada visual com o objetivo de sobreviver a uma tarefa (por exemplo, evitar bater em um
curva e esquivar de bolas de fogo, como pode ser visto na figura abaixo). Usando
[neuroevolution](https://en.wikipedia.org/wiki/Neuroevolution) para treinar
*self-attention architectures* , os autores foram capazes de treinar agentes de
aprendizado por refor√ßo para executar tarefas diferentes, permitindo apenas uma fra√ß√£o da
entrada. Os benef√≠cios do modelo incluem uma redu√ß√£o substancial no tamanho dos
par√¢metros, interpretabilidade e permitir que o modelo atenda apenas √†s *task-critical
visual hints.*

\\
![](https://cdn-images-1.medium.com/max/800/1*Gm8jlCUvP_JECEPspDypWg.png)

\\
***Introducing RONEC‚Ää‚Äî‚Ääthe Romanian Named Entity Corpus***

\\
[RONEC](https://arxiv.org/abs/1909.01247) √© um corpus *named entity* para o idioma
romeno que cont√©m mais de 26.000 entidades em ~ 5.000 frases rotuladas, pertencentes a 16
classes distintas. As frases foram extra√≠das de um jornal sem direitos autorais, cobrindo
v√°rios estilos. Esse corpus representa a primeira iniciativa de l√≠ngua romena
especificamente direcionada ao reconhecimento de *named entity*. Est√° dispon√≠vel nos
formatos BIO e CoNLL-U Plus e √© gratuito para usar e estender
[aqui](https://github.com/dumitrescustefan/ronec).

\\
***Scaling Laws for Neural Language Models***

\\
Pesquisadores de John Hopkins e da OpenAI conduziram um
[estudo](https://arxiv.org/abs/2001.08361) emp√≠rico para entender as leis de escala para o
desempenho de modelos de linguagem. Esse tipo de estudo pode ser usado como um guia para
tomar melhores decis√µes sobre como usar os recursos de maneira mais eficaz. No geral,
verificou-se que modelos maiores s√£o significativamente mais eficientes em termos de
amostra; se houver computa√ß√£o e dados limitados, √© melhor treinar um modelo grande com
algumas etapas de treinamento, em vez de treinar um modelo menor at√© convergir (consulte
os resultados sumarizados na figura abaixo). Os autores fornecem mais descobertas e
recomenda√ß√µes ao treinar modelos de linguagem grandes (por exemplo, *Transformers*) nos
aspectos de *overfitting*, escolhendo o tamanho ideal do *batch size*, o *fine-tuning*, as
decis√µes de arquitetura, etc.

\\
![](https://cdn-images-1.medium.com/max/800/1*plbjqswVe4Cq8UVggqPH1w.png)

[*Kaplan et al. (2020)*](https://arxiv.org/abs/2001.08361)

\\
***Calibration of Pre-trained Transformers***

\\
Com *Transformers* pr√©-treinados sendo cada vez mais usados em aplica√ß√µes do mundo
real, √© importante entender qu√£o *confi√°veis* s√£o suas sa√≠das. Recente
[trabalho](https://arxiv.org/abs/2003.07892) de UT Austin mostra que as probabilidades
posteriores do *BERT* e do *RoBERTa* s√£o relativamente calibradas (isto √©, consistentes
com resultados emp√≠ricos) em tr√™s tarefas (infer√™ncia de linguagem natural, detec√ß√£o de
par√°frase e *commonsense reasoning*) com conjuntos de dados *em dom√≠nio* e desafiadores
*fora do dom√≠nio*.  Os resultados mostram que: (1) quando usados fora da caixa, modelos
pr√©-treinados e prontos para uso s√£o calibrados *em dom√≠nio*; e (2) escala de temperatura
√© eficaz na redu√ß√£o adicional de erros de calibra√ß√£o *em dom√≠nio*, enquanto *label
smoothing* para aumentar a incerteza emp√≠rica ajuda a calibrar posteriores *fora do
dom√≠nio*.


\\
![](https://cdn-images-1.medium.com/max/800/1*ose0s-G0WfPFoleZJ1htrQ.png)

[*Desai and Durrett (2020)*](https://arxiv.org/pdf/2003.07892.pdf)

\\
***Statistical Mechanics of Deep Learning***

\\
Um recente
[artigo](https://www.annualreviews.org/doi/abs/10.1146/annurev-conmatphys-031119-050745)
examina mais de perto a conex√£o entre t√≥picos f√≠sicos/matem√°ticos e aprendizado
profundo. Os autores t√™m como objetivo discutir t√≥picos mais profundos que intersectam
mec√¢nica estat√≠stica e aprendizado de m√°quina com o objetivo de responder perguntas que
ajudam a entender o lado te√≥rico das redes neurais profundas e por que elas foram
bem sucedidas.

\\
***Towards an ImageNet Moment for Speech-to-Text***

\\
Em um novo
[artigo](https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/) publicado
no *The Gradient*, Alexander Veysov explica por que eles acreditam que o momento do
ImageNet para *Speech-to-Text (STT)* chegou no contexto da l√≠ngua russa. Nos √∫ltimos dois
anos, os pesquisadores tamb√©m fizeram essa afirma√ß√£o sobre NLP. No entanto, para para
alcan√ßar esse momento em STT, Alexander afirma que muitas pe√ßas precisam se unir, como
disponibilizar amplamente os modelos, minimizar os requisitos computacionais e melhorar a
acessibilidade de modelos grandes pr√©-treinados.

# Criatividade, √âtica, e Sociedade üåé

***Browsing and searching COVID-19 related articles***

\\
Na semana passada, apresentamos um conjunto de dados p√∫blicos chamado
[CORD-19](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) que
cont√©m documentos relacionados ao COVID. Gabriele Sarti escreveu [uma ferramenta
interativa](https://github.com/gsarti/covid-papers-browser), que permite que voc√™ pesquise
e navegue com mais efici√™ncia tais documentos, utilizando um [modelo SciBERT com
*fine-tuning*](https://huggingface.co/gsarti/scibert-nli).

\\
![](https://cdn-images-1.medium.com/max/800/0*tu8nzUIEuSizuW5-.gif)

\\
A reciTAL tamb√©m lan√ßou um projeto chamado [COVID-19 *Smart Search
Engine*](https://covidsmartsearch.recital.ai/) para ajudar a melhorar na pesquisa e
navega√ß√£o de artigos relacionados ao COVID-19, com o objetivo de ajudar pesquisadores e
profissionais de sa√∫de a encontrar e descobrir informa√ß√µes com rapidez e efici√™ncia
relacionadas ao COVID-19.

\\
![](https://cdn-images-1.medium.com/max/800/1*Y5W3sib4y6UxSr3YkQSu5Q.png)

\\
***SyferText***

\\
A OpenMined liberou a [SyferText](https://github.com/OpenMined/SyferText), uma nova
biblioteca de NLP que visa preservar a privacidade e seguran√ßa em NLP, e o processamente
de texto para conjuntos de dados privados. Ainda est√° em estados iniciais, mas acreditamos
que isso √© um esfor√ßo muito importante para sistemas de IA mais seguros e √©ticos. Aqui
est√£o alguns [tutoriais](https://github.com/OpenMined/SyferText/tree/master/tutorials)
para come√ßar.

\\
***David over Goliath: towards smaller models for cheaper, faster, and greener NLP***

\\
Maior √© sempre melhor? Ao analisar a evolu√ß√£o do tamanho de modelos de linguagem nos
√∫ltimos anos, pode-se pensar que a resposta √© sim. No entanto, o custo financeiro e
ambiental de treinar tais monstros √© muito alto. Al√©m disso, maior nesse caso geralmente
significa mais lento, mas a velocidade √© cr√≠tica na maioria das aplica√ß√µes. Isso motiva a
tend√™ncia atual de NLP a optar por modelos menores, mais r√°pidos e mais ecol√≥gicos, com o
desempenho preservado. Neste *post* de blog, [Manuel
Tonneau](https://www.linkedin.com/in/ACoAABs605YB_jMmGA0iLFongpVm1iKjFmKLSts/) apresenta
essa nova tend√™ncia que favorece modelos menores, com foco em tr√™s modelos recentes e
populares, *DistilBERT* da [Hugging Face](https://www.linkedin.com/company/huggingface/),
*PD-BERT* da [Google](https://www.linkedin.com/company/google/) e *BERT-of-Theseus* da
[Microsoft](https://www.linkedin.com/company/microsoft/).

\\
***A Survey of Deep Learning for Scientific Discovery***

\\
Muitas das grandes empresas hoje que concentraram esfor√ßos na pesquisa de IA acreditam que
aprendizado profundo pode ser usado como uma ferramenta para a descoberta cient√≠fica. Este
[artigo cient√≠fico](https://arxiv.org/abs/2003.11755) fornece uma vis√£o abrangente de
modelos de aprendizado profundo comummente usados para diferentes casos de uso
cient√≠fico. O artigo tamb√©m compartilha dicas de implementa√ß√£o, tutoriais, outros sum√°rios
de pesquisa e ferramentas.


# Ferramentas e Conjunto de Dados ‚öôÔ∏è

***TextVQA and TextCaps***

\\
Em um esfor√ßo para encorajar a constru√ß√£o de modelos que possam detectar e ler melhor
textos em imagens e depois argumentar para responder perguntas e gerar legendas, a
*Facebook AI* est√° hospedando duas competi√ß√µes separadas. As competi√ß√µes s√£o chamadas
[TextVQA](https://textvqa.org/challenge) *Challenge* e
[TextCaps](https://textvqa.org/textcaps/challenge) *Challenge* para abordar a tarefa de
*visual question answering* e gera√ß√£o de legendas, respectivamente.

\\
***KeraStroke***

\\
Um dos maiores obst√°culos a superar ao projetar redes neurais √© o *overfitting*. T√©cnicas
atuais de aprimoramento da generaliza√ß√£o, como *Dropout*, regulariza√ß√£o e *Early Stopping*
s√£o muito eficazes para a maioria dos casos de uso; no entanto, elas tendem a ficar aqu√©m
ao usar modelos grandes ou conjuntos de dados menores. Em resposta a isso, Charles Averill
desenvolveu a [KeraStroke](https://pypi.org/project/kerastroke/#description), um novo
conjunto de t√©cnicas de melhoria de generaliza√ß√£o √∫teis para modelos grandes ou pequenos
conjuntos de dados. Ao alterar os valores de peso em certos casos durante o treino, os
modelos se adaptam dinamicamente aos dados de treino a que eles est√£o sendo alimentados.

\\
***torchlayers***

\\
[torchlayers](https://github.com/szymonmaszke/torchlayers) √© uma nova ferramenta criada
em cima do PyTorch, que permite a infer√™ncia autom√°tica de forma e dimensionalidade das
camadas dispon√≠vel no m√≥dulo *torch.nn*, como convolucional, recorrente, *transformer*
etc.  Isso significa que voc√™ n√£o precisa definir explicitamente a forma das *features* de
entrada que devem ser especificadas manualmente nas camadas. Isso simplifica a defini√ß√£o
de um modelo em PyTorch. Veja um exemplo de um classificador b√°sico implementado com a
*torchlayers* abaixo:

\\
![](https://cdn-images-1.medium.com/max/800/1*tHOme2pZ39sNziqdUCsn4g.png)

*Podemos ver no snippet de c√≥digo que a camada Linear requer apenas o tamanho das features
de entrada, e n√£o ambos tamanhos de sa√≠da e de entrada. Isto √© inferido pela torchlayers com base
no tamanho da entrada.*


\\
***Haystack: Open-Source Framework for Question Answering at Scale***

\\
[Haystack](https://github.com/deepset-ai/haystack/) permite que voc√™ use modelos de
*transformers* em escala para *question-answering*. Ele usa um
*Retriever-Reader-Pipeline*, onde o *Retriever* √© um algoritmo r√°pido para encontrar
documentos candidatos e o *Reader* √© um *Transformer* que extrai a resposta
granular. Baseia-se nos *Transformers* da Hugging Face e da Elasticsearch. √â
*open-source*, altamente modular e f√°cil de estender.

\\
***Teaching an AI to summarise news articles: A new dataset for abstractive summarisation***

\\
A Curation Corp est√° fornecendo 40.000 resumos de not√≠cias escritos por profissionais.
Este
[artigo](https://medium.com/curation-corporation/teaching-an-ai-to-abstract-a-new-dataset-for-abstractive-auto-summarisation-5227f546caa8)
fornece uma boa introdu√ß√£o √† sumariza√ß√£o de texto e aos desafios que existem com esta
tarefa particular. Al√©m disso, apresenta o conjunto de dados, os problemas que podem ser
abordados, incluindo uma discuss√£o sobre m√©todos de *fine-tuning* e de m√©tricas de
avalia√ß√£o para resumir o texto, e encerra com uma discuss√£o para trabalhos
futuros. Instru√ß√µes para acessar o conjunto de dados podem ser encontradas neste
[reposit√≥rio do Github](https://github.com/CurationCorp/curation-corpus), juntamente com
[exemplos](https://github.com/CurationCorp/curation-corpus/tree/master/examples) do uso
do conjunto de dados para o *fine-tuning*.

\\
No t√≥pico de *text summarization*, a equipe da HuggingFace adicionou ambos
[BART](https://github.com/pytorch/fairseq/blob/master/examples/bart/README.md) e
[T5](https://github.com/dair-ai/nlp_paper_summaries/blob/master/Language%20Modeling/t5-text-to-text-transformer.md)
como parte de sua biblioteca de
[Transformers](https://github.com/huggingface/transformers/releases) . Essas adi√ß√µes
permitem todos os tipos de tarefas de NLP, como *abstractive summarization*, tradu√ß√£o e
*question answering*, entre outras.


# Artigos e Postagens ‚úçÔ∏è

***An Illustrated Guide to Graph Neural Networks***

\\
As redes neurais em grafo (*GNN*) recentemente tiveram mais ado√ß√£o em tarefas como
melhorar modelos de vis√£o computacional e previs√£o de efeitos colaterais devido a
intera√ß√µes medicamentosas. Nessa [vis√£o
geral](https://dair.ai/An_Illustrated_Guide_to_Graph_Neural_Networks/), Rish apresenta um
guia intuitivo e ilustrado para GNNs. (*Apresentado na* [*dair.ai*](https://dair.ai/))

\\
![](https://cdn-images-1.medium.com/max/800/1*Ru3CizrB14hvpZQ7ZtgIag.png)

\\
***Finetuning Transformers with JAX + Haiku***

\\
No m√™s passado, a DeepMind deixou o c√≥digo de Haiku livre, a vers√£o JAX da Sonnet, sua
bliblioteca de redes neurais em TensorFlow. Esta [postagem](https://www.pragmatic.ml/finetuning-transformers-with-jax-and-haiku/) percorre a fonte
completa de uma porta do modelo pr√©-treinado *RoBERTa* para JAX + Haiku, depois demonstra
o *fine-tuning* do modelo para resolver uma tarefa especpifica (*downstream*). Ele
pretende ser um guia pr√°tico de uso das utilidades expostas pelo Haiku para permitir o uso
"m√≥dulos" de luz orientados a objetos, dentro do contexto das restri√ß√µes de programa√ß√£o
funcional do JAX.

\\
***A small journey in the valley of Natural Language Processing and Text Pre-Processing for German language***

\\
Fl√°vio Cl√©sio escreveu um
[artigo](https://flavioclesio.com/2020/02/17/a-small-journey-in-the-valley-of-natural-language-processing-and-text-pre-processing-for-german-language/)
sobre os desafios de lidar com os problemas de NLP no idioma alem√£o. Ele compartilha
muitas li√ß√µes aprendidas, o que funcionou e o que n√£o funcionou, discute v√°rios m√©todos
estado da arte, problemas comuns a serem evitados e uma tonelada de recursos de
aprendizado, documentos e postagens de blog.

\\
***French language keeping pace with AI: FlauBERT, CamemBERT, PIAF***

\\
Nos √∫ltimos meses, foram desenvolvidos recursos interessantes de NLP em franc√™s. Estamos
falando sobre o *CamemBERT*, *FlauBERT* e *PIAF* (Pour une IA Francophone, para uma IA
francesa falante). Os dois primeiros s√£o modelos de linguagem pr√©-treinados e o √∫ltimo √©
conjunto de dados native de *question-answering*(*QA*) em franc√™s. Esta
[postagem](https://piaf.etalab.studio/francophonie-ia-english/) discute todos esses tr√™s
projetos e alguns dos desafios apresentados ao longo do caminho. Essa √© uma boa leitura e
guia para aquelas pessoas que trabalham em modelos diferentes no seu pr√≥prio idioma.

\\
***Custom classifier on top of BERT-like language model***

\\
Marcin escreveu outro excelente
[guia](https://zablo.net/blog/post/custom-classifier-on-bert-model-guide-polemo2-sentiment-analysis/)
mostrando como criar seu pr√≥prio classificador (por exemplo, um classificador de
sentimentos) em cima de modelos de linguagem como o BERT. √â um √≥timo tutorial, porque
tamb√©m mostra como usar outras bibliotecas para as diferentes partes do modelo, como o
*HuggingFace Tokenizer* e *PyTorchLightning*. Encontre o notebbok do Google Colab
[aqui](https://colab.research.google.com/drive/1sajgpLTrTJDzRSlxycy8aE6ysxGqaT18).

\\
![](https://cdn-images-1.medium.com/max/800/0*66IKvwYU2Lq1kjyn.png)

[*Fonte*](https://zablo.net/blog/post/custom-classifier-on-bert-model-guide-polemo2-sentiment-analysis/)


# Educa√ß√£o üéì

***Exploratory Data Analysis for Text Data***

\\
Neste [passo a passo do c√≥digo](https://dair.ai/Exploratory_Data_Analysis_for_Text_Data/),
Yonathan Hadar passa por alguns m√©todos para an√°lise explorat√≥ria de dados textuais com
v√°rios exemplos de c√≥digo. Apresentamos este tutorial na dair.ai porque √© um tutorial
muito abrangente que usa m√©todos padr√£o para analisar dados que qualquer *data scientist*
achar√° √∫til. √â um bom ponto de partida para quem come√ßa a brincar
dados textuais.

\\
***Embeddings in Natural Language Processing***

\\
Mohammad Taher Pilehvar e Jose Camacho-Collados publicaram seu primeiro rascunho de seu
pr√≥ximo
[livro](https://medium.com/r/?url=http%3A%2F%2Fjosecamachocollados.com%2Fbook_embNLP_draft.pdf)
chamado "*Embeddings in Natural Language Processing*". A ideia deste livro √© discutir o
conceito de *embeddings*, que representam algumas das t√©cnicas mais amplamente usadas em
NLP. Como
[declarado](https://medium.com/r/?url=https%3A%2F%2Ftwitter.com%2FCamachoCollados%2Fstatus%2F1246013768074747906%3Fs%3D20)
pelos autores, o livro inclui "no√ß√µes b√°sicas de modelos de espa√ßo vetorial e *word
embeddings* a t√©cnicas mais recentes de *embbedings* contextualizados com base em modelos
de linguagem pr√©-treinados."

\\
***A Brief Guide to Artificial Intelligence***

\\
O Dr. James V Stone publicou seu novo [livro](https://jim-stone.staff.shef.ac.uk/AIGuide/)
sobre Um Guia Breve para a Intelig√™ncia Artificial (*‚ÄúA Brief Guide to Artificial
Intelligence‚Äù*) com o objetivo de prover uma an√°lise geral de sistemas de IA atuais e suas
conquistas para atuar em uma s√©re de tarefas. Como dito no sum√°rio, o livro √© escrito em
um estilo informal, com um gloss√°rio amplo e uma lista de leituras posteriores, o que faz
dele uma introdu√ß√£o ideal para o campo de evolu√ß√£o r√°pida que √© a IA.

\\
![](https://cdn-images-1.medium.com/max/800/0*I2YYXTCQTBmk_YiF.jpg)

\\
***Cursos de ML e Aprendizado Profundo***

\\
Sebastian Raschka lan√ßou dois
[epis√≥dios](https://www.youtube.com/watch?time_continue=1&v=QQD9Y2FiotQ&feature=emb_logo)
gravados para seu curso sobre "Introdu√ß√£o ao Aprendizado Profundo e Modelos
Generativos". Voc√™ pode encontrar notas de aula e outros materiais neste
[reposit√≥rio](https://github.com/rasbt/stat453-deep-learning-ss20).

\\
Aqui est√° outro excelente conjunto de
[palestras](https://www.youtube.com/watch?v=e-erMrqBd1w&feature=youtu.be) no t√≥pico de
"Geometria Discreta Diferencial".

\\
Peter Bloem liberou o [cat√°logo](https://mlvu.github.io/) inteiro, incluindo v√≠deos e
slides de palestras, de seu curso introdut√≥rio de Aprendizado de M√°quina, ministrado na
*VU University Amsterdam*. Os t√≥picos v√£o desde modelos lineares, busca e modelos
probabil√≠sticos at√© modelos para dados sequenciais.

\\
***CNN Architecture‚Ää‚Äî‚ÄäImplementations***

\\
Dimitris Katsios fornece um conjunto de excelentes
[tutoriais](https://github.com/Machine-Learning-Tokyo/CNN-Architectures/tree/master/Implementations)
que fornecem orienta√ß√µes sobre como implementar arquiteturas de redes neurais
convolucionais (CNN) de seus artigos originais. Ele prop√µe uma receita sobre como proceder para
implement√°-las enquanto compartilha o passo a passo que inclui diagramas e c√≥digo com a
capacidade de inferir o estrutura do modelo. H√° muito a aprender com esses guias em termos
de orienta√ß√£o sobre a implementa√ß√£o de artigos de forma mais eficiente.


# Men√ß√µes Honrosas ‚≠êÔ∏è

Voc√™ pode encontrar as edi√ß√µes atenriores da *newsletter*
[aqui](https://dair.ai/NLP_Newsletter_8/). Voc√™ tamb√©m pode encontrar as vers√µes
traduzidas das edi√ß√µes anteriores da *NLP Newsletter*
[aqui](https://github.com/dair-ai/nlp_newsletter).

\\
H√° alguns meses, apresentamos o excelente livro do Luis Serrano sobre *Grokking Machine
Learning*. [Ou√ßa](https://content.alegion.com/podcast/grokking-machine-learning-with-luis-serrano)
Luis discutindo um pouco mais sobre seu livro e sua jornada para se tornar um educador na
√°rea de *ML*. pode valer sua aten√ß√£o

\\
Aqui est√£o muitas *newsletters* que podem valer sua aten√ß√£o: [Sebastian Ruder‚Äôs NLP
News](http://newsletter.ruder.io/), [Made With
ML](https://madewithml.com/blog/newsletter/2020-03-25/), [SIGTYP‚Äôs
newsletter](https://sigtyp.github.io/sigtyp-newsletter-Mar-2020.html), [MLT
Newsletter](https://mailchi.mp/c70ebcf424b2/mlt-newsletter-6), [Nathan‚Äôs AI
newsletter](http://newsletter.airstreet.com/issues/your-guide-to-ai-in-q1-2020-part-1-2-212335),
etc‚Ä¶

\\
Jupyter agora vem com um [*debugger*
visual.](https://blog.jupyter.org/a-visual-debugger-for-jupyter-914e61716559) Isso ir√°
permitir que essa popular plataforma de ci√™ncia de dados seja usada mais facilmente para
fins gerais.

\\
![](https://cdn-images-1.medium.com/max/800/0*FE5Fj5DFb0U9565a.gif)

\\
Abhishek Thakur tem um √≥timo
[canal](https://www.youtube.com/channel/UCBPRJjIWfyNG4X-CRbnv78A) no youtube por onde ele
percorre c√≥digo mostrando como usar m√©todos modernos em aprendizado de m√°quina e
NLP. Alguns v√≠deos dele v√£o desde o *fine-tuning* dos modelos *BERT* at√© a classifica√ß√£o de
grafemas manuscritos at√© a constru√ß√£o um *framework* de aprendizado de m√°quina.

\\
David Silver, renomado professor e pesquisador de aprendizado por refor√ßo, foi
[premiado](https://awards.acm.org/about/2019-acm-prize) com o Pr√™mio ACM em computa√ß√£o
para avan√ßos inovadores em jogos de computador. Prata lidera a equipe Alpha Go que venceu
Lee Sedol em Go.

\\
Para aqueles interessados em aprender as diferen√ßas e o trabalho interno por tr√°s de
m√©todos populares de NLP , como *BERT* e *word2vec*, Mohd
[fornece](https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/)
uma vis√£o geral excelente, acess√≠vel e detalhada dessas abordagens.

\\
O TensorFlow 2.2.0-rc-1 foi
[liberado](https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0-rc1?linkId=85073218). Ele
inclui recursos como um *Profiler* que ajuda a identificar gargalos em modelos de ML e
orientar a otimiza√ß√£o dos mesmos. Al√©m disso, o Colab agora usa o TensorFlow 2 por
[padr√£o](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb?linkId=85251566).

\\
Gabriel Peyr√© fornece um bom conjunto de
[notas](https://mathematical-tours.github.io/book-sources/optim-ml/OptimML.pdf) do seu
curso sobre otimiza√ß√£o de ML. As notas incluem an√°lise convexa, *SGD*, *autodiff*, *MLP*,
entre outros t√≥picos.


----------

## dair.ai updates
- ***Call for Contributions to Open Science:*** Abrimos uma solicita√ß√£o
   contribui√ß√µes para a ci√™ncia livre (*open science*). Existem muitas colabora√ß√µes
   interessantes no *pipeline*. Queremos abrir o convite para qualquer pessoa interessada
   em contribuir para a ci√™ncia livre. Estamos procurando por volunt√°rios, escritores,
   revisores, editores, desenvolvedores, palestrantes, pesquisadores, e interessados em
   manter projetos,‚Ä¶  [Junte-se a n√≥s](https://github.com/dair-ai/dair-ai.github.io/issues/54)!

- ***NLP Research Highlights Issue #1:*** Caso voc√™ n√£o tenha visto, nesse
  [artigo](https://dair.ai/NLP_Research_Highlights_-_Issue_-1/) n√≥s destacamos algumas
  tend√™ncias e t√≥picos de NLP com um foco em sumarizar o que, o como e o porqu√™ de uma
  sele√ß√£o de artigos de NLP interessantes e importantes, publicados nos √∫ltimos meses.

----------

Se voc√™ conhece alguma base de dados, projetos, postagens, tutoriais ou artigos que
gostaria de ver na pr√≥xima edi√ß√£o da *Newsletter de NLP*, por favor submeta-os diretamente
aqui usando esse [formul√°rio](https://forms.gle/3b7Q2w2bzsXE6uYo9).

\\
[*Inscreva-se*](https://dair.ai/newsletter/) *üîñ para receber as pr√≥ximas edi√ß√µes na sua caixa de entrada.*
