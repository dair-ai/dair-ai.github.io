---
layout: post
title: "NLP Newsletter #3 [FR]: Flax, Thinc, Language-specific BERT models, Meena, Flyte, LaserTagger,‚Ä¶"
author: lbourdois
modified:
comments: true
excerpt: ""
tags: [nlp_newsletter]
image:
  thumb: nlp_newsletter_3.png
---

![](https://cdn-images-1.medium.com/max/2400/1*qaOM0D2tfy3chvnWRdycGA.png)


# Avant-propos d'Elvis
Bienvenue √† la Newsletter sur le NLP ! Le troisi√®me num√©ro traite de sujets tels que l'am√©lioration des agents conversationnels, les versions de BERT sp√©cifiques √† chaque langue, les ensembles de donn√©es gratuits, les versions des biblioth√®ques d'apprentissage approfondi, et bien plus encore.


# Publications üìô

***Versions de BERT sp√©cifiques √† chaque langue***

\\
J'ai perdu le compte du nombre de mod√®les BERT sp√©cifiques √† chaque langue, mais voici quelques-unes des versions r√©centes :
 - BERT n√©erlandais ([RobBERT](https://arxiv.org/abs/2001.06286), [BERTje](https://arxiv.org/abs/1912.09582))
 - [BERT allemand](https://deepset.ai/german-bert)
 - [BERT portugais](https://github.com/neuralmind-ai/portuguese-bert)
 - BERT fran√ßais ([CamemBERT](https://arxiv.org/abs/1911.03894), [FlauBERT](https://arxiv.org/abs/1912.05372))
 - BERT italien ([AlBERTo](http://ceur-ws.org/Vol-2481/paper57.pdf), [UmBERTo](https://github.com/musixmatchresearch/umberto))
 - BERT espagnol ([BETO](https://github.com/dccuchile/beto))
 - BERT arabe ([araBERT](https://colab.research.google.com/drive/1KSy89fAkWt6EGfnFQElDjXrBror9lIZh))

\\
Notez que la plupart de ces mod√®les sont √©galement disponibles par le biais de la librairie de Transformers d‚ÄôHugging Face, qui a r√©cemment √©t√© mise √† jour avec la version [2.4.1](https://github.com/huggingface/transformers/releases).


\\
***R√©sultats trop optimistes de pr√©dictions sur des donn√©es d√©s√©quilibr√©es : d√©fauts et avantages de l'application du sur-√©chantillonnage***

\\
Cette [publication](https://arxiv.org/abs/2001.06296) r√©v√®le et examine en d√©tail certains des d√©fauts et des avantages de l'application du sur-√©chantillonnage pour traiter les jeux de donn√©es d√©s√©quilibr√©s avant de les partitionner. En outre, le travail reproduit des √©tudes ant√©rieures et identifie cette faille m√©thodologique qui produit des r√©sultats trop optimistes.

\\
***Encoder, √©tiqueter et r√©aliser : une approche contr√¥lable et efficace pour la g√©n√©ration de texte***

\\
Afin de r√©duire l'effet d'[hallucination](https://arxiv.org/abs/1910.08684) (production de sorties non support√©es par le texte d'entr√©e) commun aux m√©thodes de g√©n√©ration de texte bas√©es sur seq2seq, un groupe d'ing√©nieurs de Google a ouvert une m√©thode de g√©n√©ration de texte appel√©e [LaserTagger](https://ai.googleblog.com/2020/01/encode-tag-and-realize-controllable-and.html).
L'id√©e principale de cette m√©thode est de produire des sorties en marquant des mots avec des op√©rations d'√©dition pr√©dites (par exemple, KEEP, DELETE-ADD,, etc.) et en les appliquant aux mots d'entr√©e dans une √©tape dite de r√©alisation. Cette m√©thode remplace celle de g√©n√©ration de texte courante qui ne fait que g√©n√©rer des sorties √† partir de z√©ro, ce qui est g√©n√©ralement lent et sujet √† des erreurs. Le mod√®le offre d'autres avantages en plus de g√©n√©rer moins d'erreurs, tels que la possibilit√© de pr√©voir en parall√®le les op√©rations d'√©dition tout en conservant une bonne pr√©cision et en surpassant une base de r√©f√©rence BERT dans des sc√©narios avec un nombre r√©duit d'exemples d‚Äôentra√Ænement.


\\
![](https://cdn-images-1.medium.com/max/1600/0*OJN4pNgrQoS2STAX.png)

‚Ää[*source*](https://ai.googleblog.com/2020/01/encode-tag-and-realize-controllable-and.html)

\\
***Les r√©seaux neuronaux convolutifs comme mod√®le du syst√®me visuel : pass√©, pr√©sent et futur***

\\
Grace Lindsay a publi√© ce [rapport](https://arxiv.org/abs/2001.07092) sur l'histoire des CNN et sur la mani√®re dont ils sont √©valu√©s en tant que mod√®les de vision biologique, c'est-√†-dire comment les repr√©sentations des CNN se comparent √† celles du cerveau ? La discussion sur les nouvelles possibilit√©s d'utilisation des CNN pour la recherche sur la vision est vivement recommand√©e aux lecteurs.

\\
![](https://cdn-images-1.medium.com/max/1600/1*SngMqzPQJigR5A3AzeJGDQ.png)

[*source*](https://arxiv.org/abs/2001.07092)

\\
***Multilingual Denoising Pre-training for Neural Machine Translation***

\\
Facebook AI a publi√© [mBART](https://arxiv.org/pdf/2001.08210.pdf), une m√©thode bas√©e sur un auto-encodeur de d√©bruitage multilingue seq2seq pr√©-entrain√© sur des corpus monolingues √† grande √©chelle pour la traduction automatique dans 25 langues.
Le texte d'entr√©e implique le masquage des phrases et la permutation des phrases (bruits). Un mod√®le bas√© sur un Transformer est appris pour reconstruire le texte dans plusieurs langues. Le mod√®le autor√©gressif complet n'est entra√Æn√© qu'une seule fois et peut √™tre ajust√© sur n'importe quelle paire de langues sans impliquer de modifications sp√©cifiques √† la t√¢che ou √† la langue. Les probl√®mes de traduction au niveau du document et de la phrase sont abord√©s. En plus de montrer des gains de performance, les auteurs affirment que la m√©thode fonctionne bien sur la traduction automatique √† faible ressource.

\\
![](https://cdn-images-1.medium.com/max/1600/1*aigX70Om2rEaI7OoTcpyGA.png)

[*source*](https://arxiv.org/pdf/2001.08210.pdf)

\\
***Sur l'am√©lioration des agents conversationnels***


\\
[Meena](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html) est un agent conversationnel qui vise √† mener des conversations plus sensibles et plus sp√©cifiques ;  des mesures d√©finies pour saisir les attributs importants d'une conversation humaine (par exemple, la fluidit√©). Le mod√®le apprend le contexte de la conversation via un encodeur et formule une r√©ponse sensible via le d√©codeur. Il est signal√© que l'am√©lioration de la qualit√© des conversations a √©t√© possible en consid√©rant des d√©codeurs plus puissants.


\\
Vous pouvez √©galement prendre connaissance des [r√©flexions](https://venturebeat.com/2020/01/31/with-googles-meena-are-ai-assistants-about-to-get-a-lot-smarter/) d'Alan Nichol (co-fondateur du si√®ge de Rasa) sur ce travail.

# Cr√©ativit√© et soci√©t√© üé®

***Test de compr√©hension de la lecture et analyseur de sentiments***

\\
Ming Cheuk a con√ßu cette [application](https://littlealbert.now.sh/#/) qui permet de tester les capacit√©s de compr√©hension de lecture d‚Äô[ALBERT](https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html). ALBERT est une version plus petite de BERT pour l'apprentissage des repr√©sentations des langues. L'auteur explique plus en d√©tail le projet et les approches utilis√©es dans ce [blog](https://www.spark64.com/post/machine-comprehension).

\\
Hendrik Strobelt a √©galement publi√© un petit [projet](https://github.com/HendrikStrobelt/sentimenter_minimal_hai) dans lequel il montre comment r√©aliser un prototype d'un analyseur de sentiments interactif.

\\
![](https://cdn-images-1.medium.com/max/1600/1*kgKeL3svHqScr0Wjnfe0Cg.png)

[*source*](https://littlealbert.now.sh/#/)

\\
***Le parcours d'un chercheur autodidacte sur l'IA chez Google***

\\
Dans cet [entretien](https://blog.floydhub.com/emils-story-as-a-self-taught-ai-researcher/) Emil, un chercheur de ML √† Google Art & Culture, parle de son parcours en tant que chercheur autodidacte.


# Outils et jeux de donn√©es ‚öôÔ∏è

***Jeux de donn√©es en libre acc√®s***

\\
[Google Dataset Search](https://blog.google/products/search/discovering-millions-datasets-web/) fournit d√©sormais jusqu'√† 25 millions de jeux de donn√©es. Il s'agit essentiellement d'un moteur de recherche pour les ensembles de donn√©es.

\\
La base de donn√©es [Big Bad NLP](https://quantumstat.com/dataset/dataset.html) est un site web o√π vous pouvez rechercher une base de donn√©es d√©di√©e de plus de 200 jeux de donn√©es de NLP de tous types pour des t√¢ches telles que le raisonnement, l'analyse des sentiments, la r√©ponse aux questions, l'inf√©rence d'implication, etc‚Ä¶

\\
![](https://cdn-images-1.medium.com/max/1600/1*uYwA0snqOdKYyTJ56edtyA.png)

\\
***Librairie d‚Äôapprentissage par renforcement***

\\
Chris Nota a d√©velopp√© et publi√© une [librairie PyTorch](https://github.com/cpnota/autonomous-learning-library) pour la construction d'agents d'apprentissage par renforcement bas√©s sur des algorithmes populaires tels que DQN, PPO et DDPG. L'accent de la librairie est mis sur la conception orient√©e objet et sur la mise en ≈ìuvre et l'√©valuation rapides de nouveaux agents d'apprentissage par renforcement.

\\
***Explicabilit√© et interpr√©tabilit√© du ML***

\\
Si vous travaillez actuellement avec des mod√®les linguistiques bas√©s sur des textes et que vous souhaitez comprendre comment les interpr√©ter plus facilement lorsqu'ils sont appliqu√©s √† diff√©rentes t√¢ches linguistiques, alors vous pourriez √™tre int√©ress√© par [Captum](https://captum.ai/). Captum est une librairie d'interpr√©tabilit√© qui peut √™tre utilis√©e pour analyser l'importance des caract√©ristiques, interpr√©ter des mod√®les de texte et de vision, interpr√©ter des mod√®les multimodaux, et d'autres mod√®les tels que BERT utilis√© pour la r√©ponse aux questions.

\\
Si vous vous int√©ressez √† l'explicabilit√© des mod√®les, cet [tutoriels](https://www.kaggle.com/learn/machine-learning-explainability) peuvent √©galement vous int√©resser.

\\
***Libraries de ML et DL***

\\
L'√©quipe de Google Research a publi√© [Flax](https://github.com/google-research/flax/tree/prerelease), une libririe de r√©seaux neuronaux flexible et puissante bas√©e sur [JAX](https://github.com/google/jax) qui fournit un framework pour le calcul rapide et l‚Äôentra√Ænement de mod√®les de ML en utilisant les API typiques de Numpy.

\\
![](https://cdn-images-1.medium.com/max/1600/1*LSWFZM-xMV-GnvGl_lC-sg.png)

*Flax syntax*

\\
[Thinc](https://thinc.ai/) est une librairie l√©g√®re de deep learning d√©velopp√©e par les cr√©ateurs de spaCy. Elle offre des API de programmation fonctionnelle pour composer, configurer et d√©ployer des mod√®les personnalis√©s construits avec des librairies comme PyTorch et TensorFlow.

\\
Lyft lance [Flyte](https://eng.lyft.com/introducing-flyte-cloud-native-machine-learning-and-data-processing-platform-fb2bb3046a59), une plateforme, pr√™te pour la production et sans serveur, pour le d√©ploiement dde travail de traitement de donn√©es et de ML.

\\
***Un outil pour l'IA conversationnelle***

\\
[DeepPavlov](https://github.com/deepmipt/DeepPavlov) offre une solution gratuite et facile √† utiliser pour la construction de syst√®mes de dialogue et de syst√®mes conversationnels complexes. DeepPavlov est livr√© avec plusieurs composants pr√©d√©finis pour r√©soudre les probl√®mes li√©s au NLP. Il int√®gre BERT (y compris le BERT conversationnel) dans trois t√¢ches : la classification de textes, la reconnaissance d‚Äôentit√©s nomm√©es (et le marquage des s√©quences en g√©n√©ral) et la r√©ponse aux questions. En cons√©quence, il a permis d'am√©liorer consid√©rablement toutes ces t√¢ches. ([Google Colab](https://colab.research.google.com/github/deepmipt/dp_notebooks/blob/master/DP_tf.ipynb) | [Blog](https://medium.com/tensorflow/deeppavlov-an-open-source-library-for-end-to-end-dialog-systems-and-chatbots-31cf26849e37) | [D√©mo](https://demo.deeppavlov.ai/#/en/textqa)).

# Ethique en IA üö®
***Reconnaissance faciale et vie priv√©e***

\\
Le New York Times a r√©dig√© un article sur les diff√©rentes perspectives concernant la vie priv√©e impliquant la technologie de reconnaissance faciale. Ce reportage porte sur une "soci√©t√© secr√®te" appel√©e Clearview qui utiliserait la technologie d'IA pour construire une reconnaissance faciale universelle √† partir d'images r√©cup√©r√©es sur les r√©seaux sociaux tels que Twitter, Facebook et YouTube, etc‚Ä¶ Cette technologie soul√®ve des inqui√©tudes quant au respect de la vie priv√©e, mais elle serait √©galement utilis√©e principalement pour l'application de la loi. Pour en savoir plus, cliquez [ici](https://www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.html).

\\
***Progr√®s de l'IA au niveau humain***

\\
Dans cet [article](https://fortune.com/longform/ai-artificial-intelligence-big-tech-microsoft-alphabet-openai/), Jeremy Kahn discute en d√©tail de la diff√©rence entre ¬´ l'IA √©troite ¬ª et ¬´ l'IA g√©n√©rale ¬ª dans le contexte des progr√®s actuels de la technologie. Outre les nombreux sujets abord√©s, de nombreuses questions se posent sur les b√©n√©fices de la r√©alisation de l‚ÄôIA g√©n√©rale. L‚Äôarticle mentionne √©galement l'int√©r√™t r√©cent des grandes entreprises technologiques qui investissent dans ces efforts. L‚Äôarticle inclut plusieurs pr√©occupations soulev√©es par des chercheurs respect√©s qui d√©noncent le comportement "irresponsable et contraire √† l'√©thique" de certains organismes de recherche qui tentent de manipuler les r√©cits sur l'IA √† leur profit.

\\
***Technologies d'IA pr√©servant la vie priv√©e***

\\
L'un des efforts men√© afin de promouvoir une IA ethique, responsable et respectant la vie priv√©e est celui de la communaut√© [OpenMined] https://twitter.com/OpenMinedOrg). Si vous voulez en savoir plus, vous pouvez √©couter Andrew Trask, parler de cette initiative dans cette [vid√©o](https://www.youtube.com/watch?v=4zrU54VIK6k) faisant partie de la s√©rie de conf√©rences du MIT sur l'apprentissage profond.

\\
***Comprendre l'√©thique et la s√©curit√©***

\\
Le Dr David Leslie a publi√© ce [rapport](https://www.turing.ac.uk/sites/default/files/2019-06/understanding_artificial_intelligence_ethics_and_safety.pdf) tr√®s d√©taill√© sur des sujets qui aident √† mieux comprendre l'IA dans le contexte de l'√©thique et de la s√©curit√©. Il vise √† aider les d√©veloppeurs et les chercheurs √† mieux concevoir et mettre en ≈ìuvre des syst√®mes d'IA pour le secteur public.
![](https://cdn-images-1.medium.com/max/1600/1*Ye09aVDP93RKsLc12PXqNQ.png)

[*source*](https://www.turing.ac.uk/sites/default/files/2019-06/understanding_artificial_intelligence_ethics_and_safety.pdf)


# Articles et Blog ‚úçÔ∏è
***Tutoriel sur l'acc√©l√©ration de la tokenisation***

\\
Steven van de Graaf a √©crit cet [article](https://towardsdatascience.com/a-small-timing-experiment-on-the-new-tokenizers-library-a-write-up-7caab6f80ea6) √† propos des performances de la librairie [Tokenizers](https://github.com/huggingface/tokenizers) d‚ÄôHugging Face par rapport au tokenizer standard int√©gr√© utilis√© dans la librairie [Transformers](https://github.com/huggingface/transformers). Steven constate qu‚Äôune impl√©mentation prend 10,6 secondes pour tokenizer 1 million de phrases et que le temps d‚Äôex√©cution est divis√© par 9.

\\
***Les mod√®les linguistiques peuvent-ils vraiment comprendre ?***

\\
The Gradient a r√©cemment publi√© ce [post](https://thegradient.pub/gpt2-and-the-nature-of-intelligence/) de Gary Marcus o√π il discute de ce qu'il croit √™tre des d√©fauts fondamentaux derri√®re des mod√®les de langage comme GPT-2. L'argument principal de Gary Marcus est qu'un mod√®le entra√Æn√© pour pouvoir pr√©dire le mot suivant n'est pas n√©cessairement un mod√®le capable de comprendre ou de raisonner. C'est-√†-dire que "la pr√©diction est une composante de la compr√©hension, pas l'ensemble".

\\
***Curriculum for Reinforcement Learning***

\\
Lillian Weng [r√©sume](https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html) plusieurs approches bas√©es sur les curriculum et la fa√ßon dont cela peut √™tre utilis√© pour entra√Æner de fa√ßon efficace des agents d'apprentissage par renforcement. Weng aborde les d√©fis de la conception d'une telle approche, qui n√©cessite g√©n√©ralement de trier la complexit√© des t√¢ches et de fournir au mod√®le une s√©quence de t√¢ches dont le niveau de difficult√© augmente au cours de l‚Äôentra√Ænement.

\\
![](https://cdn-images-1.medium.com/max/1600/0*B-t_sNMjKiOb_Y3Z.png)

[*source*](https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html)

\\
***Introduction √† NumPy***

\\
Anne Bonner a r√©cemment publi√© ce tutoriel d√©taill√© pr√©sentant les bases de [NumPy](https://numpy.org/devdocs/user/absolute_beginners.html). Il est constitue une base solide pour les personnes souhaitant s‚Äôinitier √† cette librairie.

\\
![](https://cdn-images-1.medium.com/max/1600/0*FmUSU_dh-_cqGUk_.png)

[*source*](https://numpy.org/devdocs/user/absolute_beginners.html)

# Education üéì
***Fondements de l'apprentissage machine et de l'inf√©rence statistique***

\\
Anima Anandkumar, du Caltech, a publi√© un cours intitul√© "Fondements de l'apprentissage machine et de l'inf√©rence statistique". Le cours se concentre sur les concepts de ML tels que les matrices, les tenseurs, l'optimisation, les mod√®les probabilistes, les r√©seaux de neurones et bien plus encore. Ce cours aborde les aspects th√©oriques du ML. ([la playlist vid√©o](https://www.youtube.com/playlist?list=PLVNifWxslHCDlbyitaLLYBOAEPbmF1AHg) | [le programme du cours](http://tensorlab.cms.caltech.edu/users/anima/cms165-2020.html))

\\
***S√©rie de conf√©rences sur le DL***

\\
DeepMind s'est associ√© √† l'UCL pour lancer une [s√©rie de 12 conf√©rences](https://www.eventbrite.co.uk/o/ucl-x-deepmind-deep-learning-lecture-series-general-29078980901) sur l'apprentissage profond donn√©es par des chercheurs de DeepMind.

\\
***~7 million de programmes d‚Äô√©tudes***

\\
[Open Syllabus](https://opensyllabus.org/) est une organisation √† but non lucratif qui utilise le crowdsourcing pour mettre en place un programme d'enseignement sup√©rieur dans une base de donn√©es en ligne. Elle contient actuellement environ sept millions de programmes.

\\
![](https://cdn-images-1.medium.com/max/1600/1*fwQIhfb2VWuwQJM_LaLehg.png)

[*source*](https://opensyllabus.org/results-list/titles?size=50&fields=Computer%20Science)

\\
***Discuter, partager et apprendre sur le ML***

\\
[r/ResearchML](https://www.reddit.com/r/ResearchML/) est une nouvelle sous-rubrique de reddit consacr√© au ML. Celui-ci est davantage ax√© sur la recherche et encourage des discussions plus approfondies.

# Mentions sp√©ciales ‚≠êÔ∏è

D√©couvrez comment [GitHub](https://github.blog/2020-01-22-how-we-built-good-first-issues/) exploite l'apprentissage machine pour rep√©rer les probl√®mes faciles et personnalis√©s des d√©veloppeurs afin qu'ils puissent s'attaquer aux questions qui correspondent √† leurs int√©r√™ts. Cela encourage des contributions plus rapides et plus nombreuses de la part des contributeurs de logiciels libres.

\\
Sebastian Ruder a publi√© une nouvelle [newsletter](http://newsletter.ruder.io/issues/nlp-progress-restrospectives-and-look-ahead-new-nlp-courses-independent-research-initiatives-interviews-lots-of-resources-217744). On y trouve une mise √† jour des progr√®s du NLP, des r√©trospectives sur la derni√®re d√©cennie, de nouveaux cours de NLP, et d‚Äôautres sujets.

\\
Jetez un coup d'≈ìil √† [ces notebooks TensorFlow 2.0](https://github.com/NERSC/dl4sci-tf-tutorials) qui vont de CycleGAN √† Transformers en passant par les t√¢ches de sous-titrage d'images. Ils ont √©t√© rendus publics par l'√©cole d'apprentissage profond pour la science du LBNL.

\\
Un [blog](https://engineering.papercup.com/posts/bayesian-neural-nets/) pour s‚Äôinitier aux r√©seaux neuronaux bay√©siens.

\\
John Schulman [partage](http://joschu.net/blog/opinionated-guide-ml-research.html) quelques conseils pour les futurs chercheurs du LBNL sur la fa√ßon de mieux choisir les probl√®mes de recherche et d'√™tre plus strat√©gique dans la r√©alisation des t√¢ches de recherche. John partage √©galement des conseils pour le d√©veloppement personnel et le progr√®s continu.


----------

Vous pouvez retrouver la pr√©c√©dente newsletter [ici](https://dair.ai/NLP_Newsletter_-2_-FR/)

\\
Si vous avez des jeux de donn√©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine √©dition de la newletter, vous pouvez utiliser ce [formulaire](https://forms.gle/3b7Q2w2bzsXE6uYo9).

\\
[Abonnez-vous](https://dair.ai/newsletter/) pour recevoir les prochains num√©ros dans votre bo√Æte mail.
