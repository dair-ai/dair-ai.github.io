---
layout: post
title: "NLP Newsletter #11 [FR]: Jukebox, HybridQA, TLDR generation, Blender: the SOTA Chatbot, TorchServe, AI Economist, WT5,..."
author: lbourdois
excerpt: ""
modified:
comments: true
tags: [nlp_newsletter]
image:
  thumb: nlp_newsletter_11.png
---


![](https://cdn-images-1.medium.com/max/1200/1*X_c0mVECV9rtl6ozuE-oPA.png)

# Avant-propos d‚ÄôElvis
Bienvenue au onzi√®me num√©ro de la lettre d‚Äôinformation consacr√©e au NLP.
\\

***Quelques mises √† jour sur la lettre d‚Äôinformation sur le NLP et sur dair.ai :***
\\
- Nous avons publi√© un [jeu de donn√©es](https://github.com/dair-ai/emotion_dataset) qui peuvent √™tre utilis√©es pour la recherche d'√©motions bas√©e sur des textes. Le r√©pertoire comprend un [notebook]( https://colab.research.google.com/drive/1nwCE6b9PXIKhv2hvbqf1oZKIGkXMTi1X#scrollTo=t23zHggkEpc-)) qui montre comment fine-tuner les mod√®les BERT pr√©-entra√Æn√©s pour la t√¢che de classification des √©motions. Plus r√©cemment, un mod√®le a √©t√© fine-tune√© sur notre jeu de donn√©es et [h√©berg√©]( https://huggingface.co/mrm8488/distilroberta-base-finetuned-sentiment) sur HuggingFace, permettant une int√©gration simple √† une pipeline de NLP.

- Nous avons r√©cemment tenu notre toute premi√®re s√©ance de lecture de d‚Äôarticles. Plus de 124 personnes se sont inscrites et une grande partie de ce groupe a particip√© √† l'√©v√©nement √† distance. La premi√®re discussion a port√© sur le document du [T5](https://arxiv.org/abs/1910.10683%27). Nous organisons une deuxi√®me session o√π nous aurons une discussion approfondie sur le document. Pour en savoir plus sur les prochains √©v√©nements, rejoignez notre groupe [Meetup](https://www.meetup.com/dair-ai), ou participez √† la discussion dans notre [groupe Slack](https://join.slack.com/t/dairai/shared_invite/zt-dv2dwzj7-F9HT047jIGkunNKv88lQ~g).


# Publications üìô

***OpenAI‚Äôs Jukebox***

\\
Le dernier travail d√©voil√© par OpenAI s'appelle Jukebox et est essentiellement une architecture de r√©seau neuronal entra√Æn√© pour g√©n√©rer de la musique (√† partir de z√©ro) dans divers genres et styles artistiques. Le mod√®le, bas√© sur une approche de quantification appel√©e VQ-VAE, est aliment√© par le genre, l'artiste et les paroles et produit un nouvel √©chantillon audio. L'id√©e est de traiter et de compresser de longues entr√©es audio brutes via un auto-encodeur √† plusieurs niveaux et de r√©duire la dimensionnalit√© tout en pr√©servant les informations musicales essentielles. Par la suite, des transformers sont utilis√©s pour g√©n√©rer des codes qui sont ensuite reconstruits en audio brut via le d√©codeur VQ-VAE. Plus de d√©tails sur ce travail est disponible sur le [blog d‚ÄôOpenAI](https://openai.com/blog/jukebox/) ou dans [l‚Äôarticle complet](https://cdn.openai.com/papers/jukebox.pdf).


\\
![](https://cdn-images-1.medium.com/max/800/1*JvZuEnM8O4B2PmYSi3RivA.png)

\\
***HybridQA : A Dataset of Multi-Hop Question Answering over***

\\
Jusqu'√† pr√©sent, la plupart des jeux de donn√©es r√©pondant aux questions portent sur des informations homog√®nes. [HybridQA](https://github.com/wenhuchen/HybridQA) est un jeu de donn√©es de r√©ponse aux questions √† grande √©chelle destin√© √† encourager la recherche et les m√©thodes qui n√©cessitent un raisonnement sur des informations h√©t√©rog√®nes. L'ensemble de donn√©es se compose d'un tableau Wikip√©dia structur√© et d'informations non structur√©es sous la forme d'entit√©s se liant √† des corpus de forme libre. Les auteurs introduisent √©galement deux baselines permettant de souligner les avantages de travailler avec des informations h√©t√©rog√®nes par rapport √† l'utilisation d‚Äôinformations homog√®nes. Cependant, ils soulignent que les r√©sultats sont loin derri√®re la performance humaine et que cela n√©cessite des syst√®mes d'assurance qualit√© qui peuvent mieux raisonner sur des informations h√©t√©rog√®nes.

\\
![](https://cdn-images-1.medium.com/max/800/0*ojEoPUGxzskGUc1F.png)

*[Source](https://github.com/wenhuchen/HybridQA)*


\\
***Un chatbot open-source de pointe***

\\
Facebook AI a [construit](https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot) et a mis en open source Blender, un mod√®le bas√© sur l'IA qu'ils appellent le *plus grand chatbot √† domaine ouvert*. Suite au succ√®s de [Meena](https://arxiv.org/abs/2001.09977) (un r√©cent syst√®me d'IA conversationnelle propos√© par Google), ils ont propos√© un mod√®le qui m√©lange les comp√©tences conversationnelles comme l'empathie et la personnalit√© afin d‚Äôam√©liorer la qualit√© de la conversation g√©n√©r√©e. Le mod√®le a √©t√© entra√Æn√© √† l'aide d'un mod√®le bas√© sur un transformer (jusqu'√† 9,4 milliards de param√®tres) sur environ 1,5 milliard d'√©chantillons d‚Äôentra√Ænement. Il a ensuite √©t√© fine-tun√© √† l'aide d'un jeu de donn√©es ([Blended Skill Talk](https://arxiv.org/abs/2004.08449)) qui vise √† fournir les traits souhaitables identifi√©s qui pourraient am√©liorer les capacit√©s conversationnelles du mod√®le. Les auteurs affirment que le mod√®le est capable de g√©n√©rer des r√©ponses que les √©valuateurs humains ont jug√©es plus humaines que celles g√©n√©r√©es par Meena.


\\
***TLDR : r√©sum√© extr√™me d‚Äôarticles scientifiques***

\\
Ce [document](https://arxiv.org/abs/2004.15011) propose une approche, y compris un jeu de donn√©es (SCITLDR), pour la nouvelle t√¢che de *g√©n√©ration de TLDR* d'articles scientifiques.
TLDR √©tant les initiales de ¬´ too long ; didn‚Äôt read ¬ª en anglais. Ce sigle est utilis√© pour indiquer que ce qui suit est un r√©sum√© du texte trop long.
Dans ce travail, les TLDR sont d√©finis comme une alternative et un r√©sum√© compact de l'article scientifique. Les TLDR, comme le sugg√®rent les auteurs, peuvent servir de moyen de comprendre rapidement le sujet d'un article et √©ventuellement aider le lecteur √† d√©cider s'il veut continuer √† lire l'article. Pour la t√¢che finale, un mod√®le bas√© sur BART avec un fine-tuning multit√¢che (incluant la g√©n√©ration de titres et la g√©n√©ration de TLDR) a √©t√© utilis√©.

\\
![](https://cdn-images-1.medium.com/max/800/1*eeRZ4T1CG0lhgHr_ia-8MA.png)

*[Cachola et al. (2020)](https://arxiv.org/abs/2004.15011)*

\\
***WT5 ?! Entra√Ænement des mod√®les de texte √† texte pour expliquer leurs pr√©visions***

\\
Un nouveau travail appel√© [WT5](https://arxiv.org/abs/2004.14546) (abr√©viation de "Why, T5 ?") fine-tune un mod√®le T5 de Google pour produire des explications aux pr√©visions qu'il fait. Cela peut aider √† mieux comprendre pourquoi un mod√®le fait certaines pr√©dictions. Le mod√®le est aliment√© par des exemples avec des explications cibles et des labels cibles. Le texte d'entr√©e, qui comprend un pr√©fixe de t√¢che (par exemple, sentiment) et le texte r√©el peuvent √©galement √™tre pr√©c√©d√©s d'une √©tiquette "explain" (voir l'exemple dans la figure ci-dessous). Cela permet un apprentissage semi-supervis√© o√π des donn√©es enti√®rement √©tiquet√©es sont fournies au mod√®le et o√π seuls des exemples limit√©s ont les balises d'explication. Les auteurs font √©tat de r√©sultats quantitatifs et qualitatifs d√©montrant que leur approche permet d'obtenir des r√©sultats de pointe sur les ensembles de donn√©es d'explicabilit√©, y compris la capacit√© √† obtenir de bons r√©sultats dans les donn√©es hors domaine. Ce travail pr√©sente un mod√®le de base int√©ressant qui peut √™tre utilis√© pour mieux comprendre les pr√©dictions des mod√®les bas√©s sur le texte mais, comme le soulignent les auteurs, l'approche n'est qu'une am√©lioration superficielle de l'interpr√©tabilit√© et qu'il est possible de l'am√©liorer.


\\
![](https://cdn-images-1.medium.com/max/800/1*5TFSiu_G6ofmwM9FhpokHQ.png)

*Narang et al. (2020)*


# Outils et jeux de donn√©es ‚öôÔ∏è

***NVIDIA's Medical Imaging Framework***

\\
[MONAI](https://blogs.nvidia.com/blog/2020/04/21/monai-open-source-framework-ai-healthcare/?ncid=so-twit-79443#cid=ix11_so-twit_en-us) est un framework d'IA en imagerie m√©dicale destin√© √† soutenir le d√©veloppement scientifique dans le domaine des soins de sant√©. Comme indiqu√© dans les notes de publication, MONAI vise √† fournir une biblioth√®que conviviale et optimis√©e pour le traitement des donn√©es relatives aux soins de sant√©. Comme d'autres biblioth√®ques, elle fournit √©galement des outils de traitement et de transformation des donn√©es sp√©cifiques √† un domaine, des mod√®les de r√©seaux neuronaux couramment utilis√©s dans l'espace, y compris l'acc√®s √† des m√©thodes d'√©valuation et la possibilit√© de reproduire les r√©sultats.

\\
***Un √©mulateur Python pour Game Boy***

\\
[PyBoy](https://github.com/Baekalfen/PyBoy) est un outil construit en Python capable de g√©rer une interface Game Boy. Il comprend aussi une enveloppe exp√©rimentale pour entra√Æner un agent bas√© sur l'IA qui interagit avec le jeu.

\\
![](https://cdn-images-1.medium.com/max/800/1*WDJdaEyRjK660-0b6KvVjQ.png)

\\
***Jupyter Notebooks en PDF***

\\
Avez-vous d√©j√† voulu convertir vos notebooks en format PDF ? Cette [extension de Jupyter](https://github.com/betatim/notebook-as-pdf) √©crite par Tim Head vous permet de produire des PDF √† partir de vos ordinateurs portables avec le moins d'exigences possible en termes de plugins et permet de joindre les ordinateurs portables au PDF pour la reproductibilit√©.


\\
***Sur la mise en place de syst√®mes d'IA conversationnelle plus r√©alistes***

\\
La librairie Transformers comprend maintenant [DialoGPT](https://huggingface.co/transformers/model_doc/dialogpt.html). [DialoGPT](https://www.microsoft.com/en-us/research/publication/dialogpt-large-scale-generative-pre-training-for-conversational-response-generation/) est un mod√®le de g√©n√©ration de r√©ponse conversationnelle neuronale √† grande √©chelle propos√© par Microsoft. Il diff√®re des mod√®les pr√©c√©dents qui d√©pendent de donn√©es textuelles g√©n√©rales telles que Wikip√©dia et les articles de presse, car il utilise des quantit√©s massives de conversations extraites des commentaires de Reddit. DialoGPT est bas√© sur le mod√®le de langage autor√©gressif bas√© sur le GPT et vise √† fournir un pr√©-entra√Ænement √† grande √©chelle pour la g√©n√©ration de r√©ponses et de permettre ainsi une IA conversationnelle plus repr√©sentative de l'interaction humaine.

\\
![](https://cdn-images-1.medium.com/max/800/1*HTtADQcR20iRxvPh3DhJ2Q.png)

\\
***TorchServe et [TorchElastic for Kubernetes], nouvelles librairies PyTorch pour servir et entra√Æner des mod√®les √† l'√©chelle***

\\
[TorchServe](https://medium.com/pytorch/torchserve-and-torchelastic-for-kubernetes-new-pytorch-libraries-for-serving-and-training-models-2efd12e09adc) est une librairie open-source qui permet aux d√©veloppeurs d‚Äôentra√Æner leurs mod√®les tout en visant √† r√©duire les frictions dans le processus. L'outil est construit sur PyTorch et permet aux d√©veloppeurs de d√©ployer leurs mod√®les en tant que travaux en utilisant AWS. Torchserve est con√ßu comme la mani√®re canonique de servir les mod√®les entra√Æn√©s en fournissant des fonctionnalit√©s telles que le d√©ploiement s√©curis√©, des API d'inf√©rence, les mesures en temps r√©el du service d'inf√©rence, et une gestion facile des mod√®les.


\\
***MLSUM : Le corpus multilingue de r√©sum√©s***

\\
Pour encourager et renforcer la recherche multilingue en NLP, des chercheurs de ReciTAL et du CNRS ont r√©cemment [propos√©](https://arxiv.org/abs/2004.14900) un corpus de r√©sum√©s multilingues. L'ensemble de donn√©es a √©t√© obtenu √† partir de journaux et contient environ 1,5 million d'articles en fran√ßais, allemand, espagnol, russe et turc.

\\
***Made with ML***

\\
Au cas o√π vous l'auriez manqu√©, Goku Mohandas a construit un site web appel√© ¬´ Made with ML ¬ª qui vise √† fournir un outil pour d√©couvrir des projets ML pertinents et int√©ressants. Il s'agit d'une plateforme qui permet aux cr√©ateurs de partager leurs projets avec la communaut√©. Une r√©cente mise √† jour du site web comprend une section qui fournit des [sujets](https://madewithml.com/topics/) soigneusement s√©lectionn√©s qui peuvent aider les utilisateurs √† trouver rapidement des projets pertinents.

\\
![](https://cdn-images-1.medium.com/max/800/1*eoyqzd6XYVBOqOnU_jqjNA.png)



# Articles et Blog ‚úçÔ∏è

***Quelles sont les nouveaut√©s pour les Transformers lors de la conf√©rence ICLR 2020 ?***

\\
L'une des plus importantes conf√©rences sur l'apprentissage automatique, l'ICLR, a d√ª se tenir virtuellement cette ann√©e en raison des restrictions de voyage impos√©es par les pays du monde entier.
Voici quelques articles pr√©sent√©s lors de cette conf√©rence.

\\
Cet [article] (https://towardsdatascience.com/whats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792) r√©sume certains des travaux relatifs aux Transformers qui comprennent des r√©visions architecturales (par exemple [ALBERT](https://openreview.net/pdf?id=H1eA7AEtvS), [Reformer](https://openreview.net/pdf?id=rkgNKkHtvB) et [Transformer-XH](https://openreview.net/pdf?id=r1eIiCNYwS)), de nouvelles proc√©dures d'apprentissage (par exemple [ELECTRA](https://openreview.net/pdf?id=r1xMH1BtvB) et [Pretrained Encyclopedia](https://openreview.net/pdf?id=BJlzm64tDH)) et l'am√©lioration d'autres domaines tels que la recherche √† grande √©chelle, la g√©n√©ration de texte et les repr√©sentations visuelles et linguistiques. Un [document](https://openreview.net/pdf?id=HJlnC1rKPB) fournit une analyse d√©taill√©e d√©crivant les aspects communs des couches d'auto-attention et convolutionnelles, avec des r√©sultats int√©ressants sugg√©rant que les architectures de Transformers sont une g√©n√©ralisation potentielle des CNN.


\\
Si vous souhaitez en savoir plus sur d'autres travaux publi√©s dans le cadre de la CIRL cette ann√©e, vous pouvez consulter le site [Papers with Code website](https://paperswithcode.com/conference/iclr-2020-1/official).

\\
Enfin, l'ICLR vient de mettre en libre acc√®s toutes les [conf√©rences](https://iclr.cc/virtual_2020/papers.html?filter=keywords).


\\
***AI Economist : Am√©liorer l'√©galit√© et la productivit√© gr√¢ce √† des politiques fiscales ax√©es sur l'IA***

\\
Un groupe de chercheurs a propos√© un framework d'apprentissage par renforcement ([AI Economist](https://blog.einstein.ai/the-ai-economist/)) qui vise √† apprendre les *politiques fiscales dynamiques* uniquement par la simulation et les solutions bas√©es sur des donn√©es. Certaines des am√©liorations obtenues par l'AI Economist montrent des r√©sultats et des calendriers prometteurs qui pourraient d√©boucher sur un cadre susceptible d'am√©liorer les r√©sultats sociaux et l'√©tat des in√©galit√©s √©conomiques.

\\
![](https://cdn-images-1.medium.com/max/800/1*erIkiJKxa6jJgJEyNAnvbA.png)


\\
***Sur l'apport de capacit√©s de raisonnement de bon sens aux syst√®mes d'IA***

\\
L‚Äôune des capacit√©s qui font d√©faut dans de nombreux syst√®mes d'IA actuels est le raisonnement de bon sens. Cet [article](https://www.quantamagazine.org/common-sense-comes-to-computers-20200430/) pr√©sente un bref historique de ce probl√®me et explique comment les chercheurs commencent √† progresser dans ce domaine. Un bon nombre des efforts r√©cents comprennent la cr√©ation de bases de connaissances pour entra√Æner un r√©seau neuronal (en particulier des mod√®les de langage) afin d'apprendre plus rapidement et plus efficacement sur le monde. Cela peut √™tre consid√©r√© comme un effort pour combiner le raisonnement symbolique avec les r√©seaux de neurones afin de traiter les probl√®mes de *couverture* et de *bruitage* des mod√®les.

\\
![](https://cdn-images-1.medium.com/max/800/1*X6kfr8dyhvhjhQAPD-oh2A.png)

*[COMET](https://arxiv.org/abs/1906.05317)‚Ää‚Äî‚ÄäBosselut et al. (2019)*

\\
***Un examen des principaux crit√®res de r√©f√©rence du NLP***

\\
Qu'est-ce que le NLP peut faire de mieux que les humains et o√π y a-t-il encore des possibilit√©s d'am√©lioration ? Dans un [r√©cent billet de blog](https://creatext.ai/blog-posts/nlp-benchmarking-superglue-xtreme), Manuel Tonneau examine les performances du mod√®le par rapport au benchmark GLUE, en identifiant les t√¢ches o√π les syst√®mes de NLP excellent d√©j√† et celles o√π les humains ont encore une longueur d'avance. Les r√©f√©rences SuperGLUE et XTREME sont √©galement pr√©sent√©es comme une initiative visant √† placer la barre plus haut et √† motiver davantage la recherche sur de nouvelles t√¢ches et de nouveaux langages.

\\
![](https://cdn-images-1.medium.com/max/800/1*7tD3nxDuzazUFGSWNZOXWQ.png)

*[SuperGLUE benchmark](https://super.gluebenchmark.com/leaderboard/)*

\\
***Serveur d'inf√©rence Triton (TensorRT) pour les mod√®les de Transformers***

\\
Dans ce [billet de blog](https://blog.einstein.ai/benchmarking-tensorrt-inference-server/) les auteurs utilisent [le serveur d'inf√©rence Triton (TensorRT) de NVIDIA](https://docs.nvidia.com/deeplearning/sdk/triton-inference-server-guide/docs/index.html) pour h√©berger les mod√®les et exp√©rimenter avec diff√©rentes configurations afin de fournir des r√©sultats comparables entre les mod√®les desservis par TensorFlow et PyTorch. Le rapport comprend les r√©sultats obtenus sur les diff√©rents aspects du mod√®le de service, tels que la latence avec concurrence, le d√©bit avec concurrence, et d'autres configurations impliquant la taille des batchs et la longueur de la s√©quence. De nombreux aspects du service de mod√®le sont manquants dans le rapport mais les auteurs sont int√©ress√©s par des tests avec le versionnement de mod√®le et diff√©rentes t√¢ches telles que la d√©tection d'objets. Ces guides fournissent les meilleures pratiques et techniques d'√©valuation des mod√®les qui sont utiles aux personnes qui mettent leurs mod√®les en production.

\\
![](https://cdn-images-1.medium.com/max/800/1*zTIAB4-Q4TdEX32RpEC-hg.png)

*Latence et d√©bit pour diff√©rents mod√®les ‚Ää‚Äî‚Ää[source](https://blog.einstein.ai/benchmarking-tensorrt-inference-server/)*

\\
***Un guide en Keras pour les couches r√©currentes***

\\
Cet [article](https://amitness.com/2020/04/recurrent-layers-keras/) d'Amit Chaudhary fournit une explication visuelle des couches r√©currentes disponibles dans Keras et de l'effet de divers arguments sur l'entr√©e et la sortie. Cela vise √† fournir une meilleure compr√©hension de la fa√ßon d'interagir avec les couches RNN de Keras lors de la pr√©paration et du traitement des donn√©es. Un tutoriel utile pour les d√©butants int√©ress√©s par le langage de mod√©lisation avec les mod√®les RNN.

\\
![](https://cdn-images-1.medium.com/max/800/1*XvJRVz993m5TaOTilodcmg.png)

# Education üéì

***Livre d'apprentissage approfondi pour le cloud, le mobile et les p√©riph√©riques***

\\
Si vous √™tes int√©ress√© par l'utilisation de vos mod√®les d'apprentissage approfondi dans le cloud, les t√©l√©phones mobiles et les p√©riph√©riques, voici un [livre](https://www.practicaldeeplearning.ai/) √©crit par Anirudh Koul, Siddha Ganju et Meher Kasam. Le livre s'intitule "Practical Deep Learning Book for Cloud, Mobile & Edge" et traite de sujets allant du fine-tuning et du d√©ploiement de vos mod√®les de vision par ordinateur √† une introduction de plus de 40 √©tudes de cas de l'industrie, en passant par l'utilisation de l'apprentissage par transfert pour entra√Æner rapidement les mod√®les.

\\
![](https://cdn-images-1.medium.com/max/800/0*XE_--TTGI9fQCTij.jpg)

\\
***Cours de ML***
- Stanford a mis √† disposition un ensemble de [vid√©os](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) du cours de ML enseign√© par Andrew Ng. Ce cours fournit un contenu qui pourrait √™tre utile aux √©tudiants qui se lancent dans le monde de l'apprentissage automatique.
- Alors que nous mettons en production des syst√®mes de ML et de NLP pour une utilisation dans le monde r√©el, il devient crucial de construire des syst√®mes plus fiables et pr√©servant la vie priv√©e. Ce [cours](https://cseweb.ucsd.edu/classes/sp20/cse291-b/index.html) couvre les sujets de l'apprentissage machine fiable.
- Thomas Wolf a enregistr√© une [vid√©o](https://www.youtube.com/watch?v=G5lmya6eKtc) qui explique les tendances r√©centes et les sujets futurs de l'apprentissage par transfert pour le NLP.

\\
***Cours sur les GAN***

\\
Cette [conf√©rence vid√©o](https://www.youtube.com/watch?v=1CT-kxjYbFU&feature=youtu.be) de Pieter Abbeel donne un aper√ßu complet des GAN qui sont utilis√©s aujourd'hui pour toutes sortes d'applications cr√©atives, de la production d'images r√©alistes √† la peinture num√©rique. Cette conf√©rence fait partie du cours [Deep Unsupervised Learning](https://sites.google.com/view/berkeley-cs294-158-sp20/home) actuellement dispens√© √† l'universit√© de Berkley. Voir le plan de la conf√©rence ci-dessous.

\\
![](https://cdn-images-1.medium.com/max/800/1*1BKlAYDwyUheMMRl59TFFQ.png)

\\
***Calcul diff√©rentiel pour l'apprentissage approfondi***

\\
Aur√©lien Geron partage un [notebook](https://colab.research.google.com/github/ageron/handson-ml2/blob/master/math_differential_calculus.ipynb#scrollTo=mnywx0pgMCLA) qui vise √† introduire les concepts de base du calcul diff√©rentiel tels que les d√©riv√©es, les d√©riv√©es partielles et les gradients. Ces sujets sont tous importants dans le domaine de l'apprentissage profond et Aur√©lien G√©ron r√©sume les concepts ainsi que les mises en ≈ìuvre, y compris des visualisations faciles √† comprendre pour guider l'apprenant. Il recommande √©galement de consulter un autre [notebook](https://github.com/ageron/handson-ml2/blob/master/extra_autodiff.ipynb) sur l'autodiff√©renciation.

\\
![](https://cdn-images-1.medium.com/max/800/1*NU6s4j-GE5PjaDsCsgcuNw.png)


# Mentions sp√©ciales ‚≠êÔ∏è
- Andrej Karpathy [partage](https://www.youtube.com/watch?v=hx7BXih7zx8&feature=youtu.be) certains des d√©veloppements r√©cents chez Tesla afin d‚Äôaboutir √† une conduite autonome compl√®te. Les sujets abord√©s comprennent la mod√©lisation des HydraNets, les moteurs de donn√©es, les mesures d'√©valuation et la mani√®re d'effectuer efficacement des inf√©rences sur ces mod√®les de r√©seaux neuronaux √† grande √©chelle.
- Il s'agit d'un [d√©p√¥t](https://github.com/Machine-Learning-Tokyo/Interactive_Tools) pr√©par√© par MLT contenant une liste d'outils interactifs pour l'apprentissage automatique, l'apprentissage approfondi et les math√©matiques.
- Un r√©cent [document](https://arxiv.org/abs/2004.08900) vise √† fournir un aper√ßu concis des co√ªts associ√©s √† l‚Äôentra√Ænement de grands mod√®les de NLP et la mani√®re de calculer ces co√ªts.
- Springer a mis √† disposition gratuitement des centaines de livres dont les titres vont des math√©matiques √† l'apprentissage approfondi. Cet [article](https://towardsdatascience.com/springer-has-released-65-machine-learning-and-data-books-for-free-961f8181f189) r√©sume certains des livres relatifs √† l'apprentissage machine qui peuvent √™tre t√©l√©charg√©s gratuitement.
- Kra-Mania est une application simple de question-r√©ponse construite avec [Haystack](https://github.com/deepset-ai/haystack) en utilisant un jeu de donn√©es d'assurance qualit√© ouvert construit √† partir de l'exposition Seinfeld. Ce [tutoriel](https://colab.research.google.com/drive/17kZqK2i0CYzR6ZDjL6ULEtpDUOYwQAbK) montre √† quel point il est facile de construire des pipelines d'assurance qualit√© avec la librairie. Et ce [lien](https://kra-mania.firebaseapp.com/) vous emm√®ne √† l'application de d√©monstration.
- L'explicabilit√© est le processus par lequel les chercheurs visent √† mieux comprendre les r√©seaux neuronaux profonds. Ce [document](https://arxiv.org/abs/2004.14545) fournit un *"guide de terrain" sur l'explicabilit√© de l'apprentissage profond pour les non-initi√©s*.
- Voici une petite [enqu√™te](https://ai.stanford.edu/blog/data-augmentation/) d√©crivant les travaux r√©cents sur l'augmentation des donn√©es, qui est devenue un domaine d'√©tude populaire en ML et en NLP.
- Dans la newsletter pr√©c√©dente, nous avons pr√©sent√© Longformer, une variante du Transformer qui am√©liore les performances de diverses t√¢ches de NLP, en particulier pour les documents longs. Dans cette [vid√©o](https://www.youtube.com/watch?v=_8KNb5iqblE&t=463s), Yannic Kilcher explique la nouveaut√© propos√©e dans ce travail.

----------

Vous pouvez retrouver la pr√©c√©dente newsletter [ici](https://dair.ai/NLP_Newsletter_-10_-FR/)

\\
Si vous avez des jeux de donn√©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine √©dition de la newletter, vous pouvez utiliser ce [formulaire](https://forms.gle/3b7Q2w2bzsXE6uYo9).

\\
[Abonnez-vous](https://dair.ai/newsletter/) pour recevoir les prochains num√©ros dans votre bo√Æte mail.
