---
layout: post
title: "NLP Newsletter #7 [FR]: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶"
author: lbourdois
modified:
comments: true
tags: [nlp_newsletter]
image:
  thumb: nlp_newsletter_7.png
---

![](https://cdn-images-1.medium.com/max/1200/1*9gNslKwKiRaffDt2RSOgoQ.png)

# Avant-propos d‚ÄôElvis
\\
Bienvenue au 7e num√©ro de la lettre d'information consacr√©e au NLP. J'esp√®re que vous passez une merveilleuse journ√©e et que vous et vos proches √™tes en s√©curit√© en ces temps difficiles. Nous avons d√©cid√© de publier ce bulletin pour apporter un peu de joie √† nos lecteurs, alors n'h√©sitez pas √† le lire quand vous aurez du temps libre. Pour l'instant, concentrons-nous sur les choses qui sont de la plus haute priorit√© : nos familles et nos amis. ‚ù§Ô∏è üíõ üíö

\\
***Quelques mises √† jour sur la lettre d'information sur le NLP et sur dair.ai.***

\\
Les traductions fran√ßaises et chinoises de tous les num√©ros pr√©c√©dents de la newsletter sont d√©sormais [disponibles](https://github.com/dair-ai/nlp_newsletter). D√©couvrez comment vous pouvez contribuer √† la traduction des num√©ros pr√©c√©dents et √† venir en cliquant sur ce [lien](https://github.com/dair-ai/dair-ai.github.io/issues/11).


\\
Nous avons r√©cemment cr√©√© deux nouveaux d√©p√¥ts GitHub qui contiennent des [r√©sum√©s de publications sur le NLP](https://github.com/dair-ai/nlp_paper_summaries) et des [notebooks PyTorch](https://github.com/dair-ai/pytorch_notebooks) pour vous aider √† d√©marrer avec les r√©seaux de neurones.



# Publications üìô

***Mesure de la g√©n√©ralisation de la composition***

\\
Dans le contexte de l'apprentissage machine, la g√©n√©ralisation de la composition est la capacit√© d'apprendre √† repr√©senter le sens et des s√©quences (combinaisons in√©dites) √† partir de ce qui est appris dans le jeu d‚Äôentra√Ænement. √Ä ce jour, la mani√®re de mesurer correctement la composition dans les r√©seaux neuronaux n'est pas claire. Une √©quipe de Google AI [propose](https://ai.googleblog.com/2020/03/measuring-compositional-generalization.html) l'un des plus grands benchmarks pour la g√©n√©ralisation de la composition en utilisant des t√¢ches telles que le question/ answering et l'analyse s√©mantique. L'image ci-dessous montre un exemple du mod√®le propos√© utilisant des atomes (produire, diriger, etc.) pour produire de nouveaux compos√©s, c'est-√†-dire des combinaisons d'atomes. L'id√©e de ce travail est de produire des √©chantillons entra√Ænement/test qui contiennent des exemples qui partagent des atomes similaires (blocs de construction pour g√©n√©rer des exemples) de distribution mais une distribution de compos√©s diff√©rente (la composition des atomes). Les auteurs affirment que c'est une fa√ßon plus fiable de tester la g√©n√©ralisation de la composition.

\\
![](https://cdn-images-1.medium.com/max/800/0*lXmUWOY8HJL7YVn1.gif)

*Credit: Google AI Blog*

\\
***Fine-Tuning de mod√®les linguistiques pr√©-entra√Æn√©s : Initialisation des poids, ordonnancement des donn√©es et arr√™t anticip√©***

\\
Des chercheurs ont men√© une [s√©rie d'essais de fine-tuning](https://arxiv.org/abs/2002.06305) pour mieux comprendre l'effet de l'initialisation du poids et de l'arr√™t pr√©coce dans la performance des mod√®les. Au cours de diverses exp√©riences qui ont n√©cessit√© des centaines de tunage de BERT, il a √©t√© constat√© que des graines al√©atoires distinctes donnent des r√©sultats tr√®s diff√©rents. En particulier, l'√©tude indique qu'une certaine initialisation des poids donne de bons r√©sultats pour un ensemble de t√¢ches. Toutes les donn√©es exp√©rimentales et les essais ont √©t√© rendus publics pour les autres chercheurs qui souhaitent mieux comprendre les diff√©rentes dynamiques lors de la mise au point.


\\
***Une introduction aux circuits***

\\
Les chercheurs d'OpenAI ont publi√© un [article](https://distill.pub/2020/circuits/zoom-in/) sur l'interpr√©tabilit√© des r√©seaux de neurones et proposent une nouvelle approche pour les interpr√©ter. Inspir√©s par la biologie cellulaire, les auteurs approfondissent la compr√©hension des mod√®les de vision et de ce qu'ils apprennent en inspectant le poids des r√©seaux neuronaux. Essentiellement, l'√©tude pr√©sente quelques affirmations ainsi que des preuves recueillies qui, selon eux, pourraient ouvrir la voie √† une meilleure interpr√©tation des r√©seaux neuronaux.

\\
![](https://cdn-images-1.medium.com/max/800/1*i0c-qpiire6dD4IqJVKlYg.png)

\\
***NLP Research Highlights‚Ää‚Äî‚ÄäIssue #1***

\\
Dans une nouvelle s√©rie de dair.ai intitul√©e [NLP Research Highlights](https://medium.com/dair-ai/nlp-newsletter-bertology-primer-fastpages-t5-data-science-education-pytorch-notebooks-slow-8ae5d499e040), nous fournissons des descriptions d√©taill√©es des recherches actuelles int√©ressantes et importantes sur le NLP. Dans le premier num√©ro trimestriel, les sujets vont de l'am√©lioration des mod√®les de langage √† l'am√©lioration des agents conversationnels en passant par les syst√®mes de reconnaissance vocale de pointe. Ces r√©sum√©s seront √©galement mis √† jour [ici](https://github.com/dair-ai/nlp_paper_summaries).



\\
***Apprendre √† simuler la physique complexe avec les r√©seaux de graphes***

\\
Ces derniers mois, nous avons beaucoup parl√© des r√©seaux de neurones de graphes (GNN) en raison de leur efficacit√© non seulement en NLP mais aussi dans d'autres domaines tels que la g√©nomique et les mat√©riaux. Dans un r√©cent [article](https://arxiv.org/abs/2002.09405), des chercheurs proposent un cadre g√©n√©ral bas√© sur les r√©seaux de graphes qui est capable d'apprendre des simulations dans diff√©rents domaines tels que les fluides et les mat√©riaux d√©formables. Les auteurs affirment qu'ils obtiennent des performances de pointe dans diff√©rents domaines et que leur approche g√©n√©rale est potentiellement le simulateur de physique le mieux appris √† ce jour. Les exp√©riences comprennent la simulation de mat√©riaux tels que le ¬´ goop over water ¬ª (je ne sais pas comment cela se traduit en fran√ßais) et d'autres interactions avec des obstacles rigides. Ils ont √©galement test√© un mod√®le pr√©-entra√Æn√© sur des t√¢ches hors distribution et ont trouv√© des r√©sultats prometteurs qui montrent la g√©n√©ralisation du framework √† des domaines plus vastes.


\\
![](https://cdn-images-1.medium.com/max/800/1*48EolUDJoHpYRCTZxgn_qg.png)

[*(Sanchez-Gonzalez et al., 2020)*](https://arxiv.org/pdf/2002.09405.pdf)

\\
***Mod√®les BERT sp√©cifiques √† chaque langue***

\\
Le BERT en arabe (AraBERT) est maintenant disponible dans la librairie Transformer. Vous pouvez acc√©der au mod√®le [ici](https://huggingface.co/aubmindlab/bert-base-arabert) et au document [ici](https://arxiv.org/abs/2003.00104). R√©cemment, une version japonaise de BERT a √©galement √©t√© [publi√©e](https://github.com/akirakubo/bert-japanese-aozora). Il existe √©galement une version polonaise de BERT appel√©e [Polbert](https://github.com/kldarek/polbert).


# Cr√©ativit√©, √©thique et soci√©t√© üåé

***Pr√©visions des structures prot√©iques associ√©es au COVID-19***

\\
DeepMind d√©voile [des structures pr√©dites par calcul](https://deepmind.com/research/open-source/computational-predictions-of-protein-structures-associated-with-COVID-19) pour les prot√©ines li√©es au COVID-19. Les pr√©dictions sont directement obtenues √† partir des syst√®mes AlphaFold mais n'ont pas √©t√© v√©rifi√©es exp√©rimentalement. L'id√©e de cette publication est d'encourager les contributions qui visent √† mieux comprendre le virus et son fonctionnement.


\\
***Utilisation du GPT2 pour une exp√©rience sur des cas judiciaires***

\\
Janelle Shane partage les [r√©sultats](https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights) d'une exp√©rience amusante o√π un mod√®le GPT-2 est fine-tun√© pour g√©n√©rer des cas contre des objets inanim√©s. Le mod√®le a √©t√© aliment√© par une liste de cas o√π le gouvernement saisissait des marchandises de contrebande ou dangereuses. Cela a g√©n√©r√© des cas comme ceux pr√©sent√©s dans l'image ci-dessous.

\\
![](https://cdn-images-1.medium.com/max/800/0*E5mHmkm1h4VQJ2Ni.png)

[*Source*](https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights)

\\
***Vers une conception centr√©e sur l'homme des frameworks de ML***

\\
Google AI [a publi√©](https://ai.googleblog.com/2020/03/toward-human-centered-design-for-ml.html) les r√©sultats d'une enqu√™te √† grande √©chelle men√©e aupr√®s de 645 personnes ayant utilis√© TensorFlow.js. L'objectif √©tait de conna√Ætre les caract√©ristiques les plus importantes ainsi que l'exp√©rience g√©n√©rale des d√©veloppeurs de logiciels non ML testant des frameworks de ML actuels.
Les r√©sultats montrent notamment que le "manque de compr√©hension conceptuelle du ML" entrave l'utilisation des frameworks de ML pour cet ensemble d'utilisateurs. Les participants √† l'√©tude ont √©galement signal√© le besoin de meilleures instructions sur la fa√ßon d'appliquer les mod√®les de ML √† diff√©rents probl√®mes et d'un soutien plus explicite pour les modifications.


\\
***Tracking du visage et de la main avec MediaPipe et TensorFlow.js***

\\
Cet [article sur TensorFlow](https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html?linkId=83996111) explique comment activer le suivi des visages et des mains en temps r√©el √† l'aide de TensorFlow.js et de MediaPipe.

\\
![](https://cdn-images-1.medium.com/max/800/0*XsRsB-tSOZo9yWOc.gif)

*Credit: TensorFlow Blog*


# Outils et jeux de donn√©es ‚öôÔ∏è

***NLP Paper Summaries***

\\
Nous avons r√©cemment cr√©√© un [d√©p√¥t](https://github.com/dair-ai/nlp_paper_summaries) contenant des r√©sum√©s des publications de NLP les int√©ressantes et les plus importantes de ces derni√®res ann√©es. L'objectif est d'am√©liorer l'accessibilit√© de ces recherches et sujets.

\\
***Une librairie de vision par ordinateur pour PyTorch***
\\
[Kornia](https://github.com/kornia/kornia) est une librairie open-source construite sur PyTorch qui permet aux chercheurs d'utiliser un ensemble d'op√©rateurs pour r√©aliser une vision informatique diff√©renci√©e en utilisant PyTorch. Parmi les fonctionnalit√©s, on trouve les transformations d'images, l'estimation de la profondeur et le traitement d'images de bas niveau, pour n'en citer que quelques-unes. Elle est fortement inspir√©e d'OpenCV, mais la diff√©rence est qu'elle est destin√©e √† la recherche plut√¥t qu'√† la cr√©ation d'applications pr√™tes √† la production.

\\
![](https://cdn-images-1.medium.com/max/800/0*gN_-llcA4_3lIHYE.gif)

\\
***Pr√©sentation de DIET : une architecture qui surpasse le fine-tuning de BERT et qui est 6X plus rapide √† entra√Æner***

\\
DIET (Dual Intent and Entity Transformer) est une architecture multit√¢che de compr√©hension du langage naturel (NLU) [propos√©e](https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/) par Rasa. Le framework se concentre sur l‚Äôentra√Ænement multit√¢che afin d'am√©liorer les r√©sultats en mati√®re de classification des intentions et de reconnaissance des entit√©s. Un autre avantage de DIET est que l‚Äôon peut utiliser n'importe quel √©l√©ment pr√© entrain√© tels que BERT et GloVe. L‚Äôobjectif principale de cette librairie est de fournir un mod√®le qui am√©liore les performances actuelles de ces t√¢ches et qui est plus rapide √† entra√Æner (acc√©l√©ration de 6X). Le mod√®le est disponible dans la [librairie python Rasa Open Source](https://rasa.com/docs/rasa/1.8.0/nlu/components/#dietclassifier).


\\
![](https://cdn-images-1.medium.com/max/800/0*R_8FOU-CVZabv7hJ.jpg)

[*DIET framework*](https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/?utm_source=twitter)

\\
***Perdu parmi toutes les langues disponibles pour BERT ?***

\\
[BERT Lang Street](https://bertlang.unibocconi.it/) est un site web qui offre la possibilit√© de rechercher plus de 30 mod√®les bas√©s sur le BERT, en 18 langues et 28 t√¢ches, soit un total de 177 entr√©es. Par exemple, si vous souhaitez conna√Ætre les r√©sultats de la classification des sentiments √† l'aide des mod√®les de BERT, il vous suffit de rechercher "sentiment" dans la barre de recherche (exemple illustr√© dans la capture d'√©cran ci-dessous).

\\
![](https://cdn-images-1.medium.com/max/800/1*UuVno2eOAzYb_wlSSfukPA.png)

\\
***Med7***

\\
Andrey Kormilitzin publie [Med7](https://github.com/kormilitzin/med7) qui est un mod√®le pour du NLP √† usage clinique (en particulier les t√¢ches de reconnaissance d'entit√©s nomm√©es (NER)) sur les dossiers m√©dicaux √©lectroniques. Le mod√®le peut identifier jusqu'√† sept cat√©gories et est disponible pour √™tre utilis√© avec la biblioth√®que spaCy.

\\
![](https://cdn-images-1.medium.com/max/800/1*yOMqhvTwYnxB4LYXv2Mgjg.png)

\\
***Une biblioth√®que open source pour le ML quantique***

\\
[TensorFlow Quantum](https://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html) est une biblioth√®que open-source qui fournit une bo√Æte √† outils pour le prototypage rapide de la recherche en ML quantique qui permet l'application de mod√®les de ML pour aborder des probl√®mes allant de la m√©decine aux mat√©riaux.


\\
***Des r√©seaux infiniment larges, rapides et faciles, avec Neural Tangents***

\\
Neural Tangents est une librairie open-source qui permet aux chercheurs de d‚Äôentra√Æner des mod√®les de largeur finie ou infinie en utilisant JAX. Lisez l'article du blog de la version [ici](https://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html) et acc√©dez √† la librairie [ici](https://github.com/google/neural-tangents).

\\
![](https://cdn-images-1.medium.com/max/800/1*CojgKJwB_n_7-j0DJZ0y7g.png)



# Articles et Blog ‚úçÔ∏è

***De PyTorch √† JAX***

\\
Sabrina J. Mielke a publi√© un [article](https://sjmielke.com/jax-purify.htm) qui explique comment construire et entra√Æner des r√©seaux de neurones en utilisant JAX. L'article se concentre sur la comparaison du fonctionnement interne de PyTorch et de JAX lors de la construction de r√©seaux neuronaux, ce qui permet de mieux comprendre certains des avantages et des diff√©rences de JAX.

\\
![](https://cdn-images-1.medium.com/max/800/0*Nrw4UnmnIZ__elHu.png)

[*Source*](https://sjmielke.com/jax-purify.htm)

\\
***Pourquoi utilisons-nous encore des jeux de donn√©es test cr√©√©s il y a plus de 18 ans ?***

\\
Dans cet [article de blog](https://ehudreiter.com/2020/03/02/why-use-18-year-old-bleu/), Ehud Reiter explique pourquoi nous utilisons encore de vieilles techniques d'√©valuation comme BLUE pour √©valuer les mod√®les de NLP pour des t√¢ches comme la traduction automatique






\\
***Introduction √† BART***

\\
[BART](https://arxiv.org/abs/1910.13461) est un mod√®le propos√© par Facebook qui implique un autoencodeur de d√©bruitage pour le pr√©-entra√Ænement de mod√®les seq2seq, ce qui am√©liorent les performances sur les t√¢ches de g√©n√©ration de texte telles que le r√©sum√© abstrait. Sam Shleifer fournit un [r√©sum√©](https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html) de BART et explique comment il l'a int√©gr√© dans la librairie Transformers de Hugging Face.


\\
***Am√©liorations des Transformers***

\\
Madison May a r√©cemment r√©dig√© une [enqu√™te](https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/) d√©crivant les moyens d'am√©liorer les approches bas√©es sur les Transformers. L‚Äôarticle aborde ainsi les Sparse Transformers, les Adaptive Span Transformers, le Transformer-XL, les compressive Transformers, le Reformer, et les routing transformers.

\\
***Contr√¥ler le style et le contenu dans la r√©daction automatique de textes***

\\
Malgr√© l'impressionnante fluidit√© dont a fait preuve l'√©criture automatique des textes l'ann√©e derni√®re, il est toujours difficile de contr√¥ler des attributs comme la structure ou le contenu du texte. Dans un [r√©cent article de blog](https://creatext.ai/blog-posts/controllable-text-generation), Manuel Tonneau √©voque les progr√®s r√©cents et les perspectives dans le domaine de la g√©n√©ration de texte contr√¥lable, du mod√®le GPT-2 de Hugging Face, fine-tun√© sur arXiv, au T5 de Google, en passant par CTRL de Salesforce et PPLM de Uber AI.


# Education üéì

***L‚Äôavenir du NLP en Python***

\\
Dans l'un de nos num√©ros pr√©c√©dents, nous avons pr√©sent√© [THiNC](https://thinc.ai/), qui est une librairie de DL fonctionnelle ax√©e sur la compatibilit√© avec d'autres librairies existantes. Ces [diapositives](https://speakerdeck.com/inesmontani/the-future-of-nlp-in-python-keynote-pycon-colombia-2020?slide=9) pr√©sentent un peu plus cette livrairie qui a √©t√© utilis√©e par Ines Montani pour la conf√©rence PyCon Colombia.


\\
***Les notebooks de Transformers***

\\
HuggingFace a publi√© un ensemble de [notebooks](https://github.com/huggingface/transformers/tree/master/notebooks) qui aident √† d√©marrer avec leur librairie Transformers. Certains notebooks comprennent l'utilisation de la tokenisation, la mise en place de pipelines et l‚Äôentra√Ænement d'un mod√®le sur des donn√©es personnalis√©es.

\\
![](https://cdn-images-1.medium.com/max/800/1*0AYHYUsHbaqV2vqN2zCzLQ.png)

\\
***TensorFlow 2.0 en 7 heures***

\\
Jetez un oeil √† ce [cours gratuity d‚Äôenviron 7h](https://www.freecodecamp.org/news/massive-tensorflow-2-0-free-course/) consacr√© √† TensorFlow 2.0 abordant des sujets qui vont des r√©seaux neuronaux de base au NLP avec les RNN, en passant par une introduction √† l'apprentissage par renforcement.

\\
***DeepMind: le podcast***

\\
DeepMind a publi√© tous les √©pisodes (sous la forme d'une [playlist YouTube](https://www.youtube.com/playlist?list=PLqYmG7hTraZBiUr6_Qf8YTS2Oqy3OGZEj)) de son podcast qui pr√©sente des scientifiques, des chercheurs et des ing√©nieurs discutant de sujets allant de l'IAG aux neurosciences en passant par la robotique.

\\
***Cours de Machine Learning et de Deep Learning***

\\
Berkeley rend public le [programme complet](https://sites.google.com/view/berkeley-cs294-158-sp20/home) de son cours sur "l'apprentissage profond non supervis√©", principalement ax√© sur les aspects th√©oriques de l'apprentissage autosupervis√© ainsi que sur les mod√®les g√©n√©rateurs. Parmi les sujets abord√©s, citons les mod√®les de variables latentes, les mod√®les autor√©gressifs et les mod√®les de flux pour n'en citer que quelques-uns. Des vid√©os et des diapositives sont disponibles sur Youtube.


\\
Nous avons √©galement trouv√© cette importante [liste](https://www.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/) de cours en ligne sur l'apprentissage machine, le NLP et l'apprentissage approfondi.


\\
Et voici un autre cours intitul√© ["Introduction to Machine Learning"](https://compstat-lmu.github.io/lecture_i2ml/index.html) qui comprend des sujets tels que la r√©gression supervis√©e, l'√©valuation des performances, les for√™ts al√©atoires, le r√©glage des param√®tres, des conseils pratiques, et bien plus encore.







# Mentions sp√©ciales ‚≠êÔ∏è

Connon Shorten a publi√© une [vid√©o](https://www.youtube.com/watch?v=QWu7j1nb_jI&feature=emb_logo) expliquant le mod√®le ELECTRA qui propose une technique appel√©e "remplacement de la d√©tection de jeton" pour pr√©traiter les Transformers plus efficacement. Si vous √™tes int√©ress√©, nous avons √©galement r√©dig√© un bref r√©sum√© du mod√®le [ici](https://medium.com/dair-ai/nlp-research-highlights-cd522b21b01a) (en anglais).

\\
Rachael Tatman travaille sur une nouvelle s√©rie intitul√©e [NLP for Developers](https://www.youtube.com/watch?v=-G36q8_cYsc&feature=emb_logo) o√π l'id√©e est de parler plus en profondeur des diff√©rentes m√©thodes de NLP du moment et d'expliquer les probl√®mes communs que vous pourriez rencontrer.

\\
DeepMind diffuse [AlphaGo - The Movie](https://youtu.be/WXuK6gekU1Y) sur YouTube pour c√©l√©brer le 4√®me anniversaire de la victoire d'AlphaGo sur Lee Sedol au jeu de Go.

\\
OpenMined [√©voque](https://blog.openmined.org/introducing-openmined-research/) les r√¥les d'ing√©nieur de recherche et de chercheur scientifique, ce qui est une bonne occasion de s'impliquer dans la pr√©servation de la vie priv√©e en mati√®re d'IA.


----------

Vous pouvez retrouver la pr√©c√©dente newsletter [ici](https://dair.ai/NLP_Newsletter_-6_-FR/)

\\
Si vous avez des jeux de donn√©es, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine √©dition de la newletter, vous pouvez utiliser ce [formulaire](https://forms.gle/3b7Q2w2bzsXE6uYo9).

\\
[Abonnez-vous](https://dair.ai/newsletter/) pour recevoir les prochains num√©ros dans votre bo√Æte mail.
